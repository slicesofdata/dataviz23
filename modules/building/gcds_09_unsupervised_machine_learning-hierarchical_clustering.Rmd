---
title: "GCDS: Unsupervised ML - Hierarchical Clustering"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
rm(list = ls(all.names = TRUE))      # remove objects in R

source("https://pastebin.com/raw/8mXH02yg")   # run and comment out before knitting
source("https://pastebin.com/raw/97NNTTzu")   # run to include in function definitions

# set the paths for project, script, and data dirs
proj_dir <- gsub("GCDS.*.Rmd", "GCDS", get_ActivePath())
proj_name = ""
r_dir    <- paste(proj_dir, "r", sep = "/")    # r subdir
data_dir <- paste(proj_dir, "data", sep = "/") # data subdir
if ( proj_name != "" & !dir.exists(paste(proj_dir, proj_name, sep = "/")) ) {
  # create project dir
  suppressWarnings(dir.create(paste(proj_dir, proj_name, sep = "/")))
  r_dir <- gsub("/r", paste0("/", proj_name, "/r"), r_dir)
  data_dir <- gsub("/data", paste0("/", proj_name, "/data"), data_dir)
  # create sub directories
  suppressWarnings(dir.create(r_dir))
  suppressWarnings(dir.create(data_dir)) }
```


# **Libraries**

```{r}
library(magrittr)
library(factoextra)
library(dendextend)
#library(gplots)
#library(colorspace) # get nicer colors
library(ggplot2)
library(MASS)
library(dplyr)
```

# **Data Frame Objects**

```{r message=FALSE, warning=FALSE}
example1 <- data.frame(x = c(1,2,3,4,5), y = c(1,2,3,4,5))
example2 <- data.frame(x = c(1,2,3,4,5), y = c(1,2,4,4,3))

# Wine quality data
WINE <- readr::read_csv(paste(data_dir, "winequality.csv", sep = "/"))

# the classic iris data set
IRIS <- datasets::iris

MR_agg <- readr::read_csv(
  paste(data_dir, "mental_rotation_agg.csv", sep = "/"))

MR_agg <- MR_agg %>%
  mutate(., trialtype = factor(trialtype, 
                               levels = c("Same", "Mirror"), 
                               ordered = T))
```


Convert the long data to wide in order to obtain multivariate wide data for clustering.

```{r}
MR_wide <- MR_agg %>%
  group_by(., id, trialtype, rotation) %>%
  summarize(., 
            rt  = mean(na.omit(rt_med)),
            ) %>%
  tidyr::pivot_wider(., 
                     names_from  = c(trialtype, rotation),
                     values_from = rt
  ) %>%
  mutate(., id = as.character(id))

MR_wide <- readr::read_csv(
  paste(data_dir, "mental_rotation_wide.csv", sep = "/")) %>%
  mutate(., id = as.character(id))

# scale measurement variables
MR_wide_z <- MR_wide %>%
  mutate(., across(
    .cols = starts_with("Mirror"), 
    .fns = ~as.numeric(scale(.x)), 
    .names = "{.col}_z")) %>%
  mutate(., across(
    .cols = starts_with("Same"), 
    .fns = ~as.numeric(scale(.x)), 
    .names = "{.col}_z")) %>%
  select(., c(id, contains("_z"))) # keep z

view(MR_wide_z)

# There is an NA id, so remove.
MR_wide_z <- MR_wide_z %>%
  filter(., !is.na(id)) %>%
  na.omit(.)

view(MR_wide_z)
```


# **Mental Rotation Data**

```{r}
MR_same <- MR_wide_z %>%
  dplyr::select(., contains("Same"))

(MR_same_opt_wss <- MR_same %>%
  factoextra::fviz_nbclust(., kmeans, method = "wss"))

(MR_same_opt_gap <- MR_same %>%
  factoextra::fviz_nbclust(., kmeans, method = "gap_stat"))

MR_same_opt_wss

MR_same_opt_gap
```

## *Examine Relationships*


```{r}
MR_same %>%
  cor(.) %>%
  corrplot::corrplot(.,    # a correlation matrix
         method = "ellipse", # a method for producing the correlation plot 
         #method = "number",
         type = "full",    # a plot style (also "upper" and "lower")
         diag = TRUE,      # if TRUE (default), adds the diagonal
         tl.col = "black", # label color (default is red)
         bg = "white",     # background color
         title = "",       # a main title
         col = NULL)       # a color palette
```


## *Create the k-means*

```{r}
MR_clusters3 <- kmeans(x = MR_same,
         centers = 3, 
         algorithm = "Hartigan-Wong",
         nstart = 50
         )

MR_clusters2 <- kmeans(x = MR_same,
         centers = 2, 
         algorithm = "Hartigan-Wong",
         nstart = 50
         )

#MR_same$id <- MR_wide_z$id
#MR_same$clusters5 <- MR_clusters5$cluster
#MR_same$clusters4 <- MR_clusters4$cluster
#MR_same$clusters3 <- MR_clusters3$cluster

#MR_same$clusters2 <- MR_clusters2$cluster

#MR_same
```


## *Visualize Clusters*

```{r}
factoextra::fviz_cluster(object = MR_clusters5, 
                         data = MR_same, 
                         ellipse.type = "norm"
                         ) + see::theme_modern()

factoextra::fviz_cluster(object = MR_clusters4, 
                         data = MR_same, 
                         ellipse.type = "norm"
                         ) 

factoextra::fviz_cluster(object = MR_clusters3, 
                         data = MR_same, 
                         ellipse.type = "norm"
                         ) 

factoextra::fviz_cluster(object = MR_clusters2, 
                         data = MR_same, 
                         ellipse.type = "norm"
                         ) 
```


Assign cluster assignment to the data frame.



```{r}
MR_wide_z <- MR_wide_z %>%
  mutate(., 
         clusters2 = MR_clusters2$cluster,
         clusters3 = MR_clusters2$cluster
         ) %>%
  mutate(., across(where(is.numeric), round, digits = 2))


view(MR_wide_z, filter = "none")
```


Maybe there are no distinct groups in the data based on these measurement variables. 

```{r}
(STROOP_opt_gap <- STROOP_WIDE %>%
  select(congruent_z, incongruent_z) %>% 
  factoextra::fviz_nbclust(., kmeans, method = "gap_stat"))


STROOP_WIDE$id <- as.character(STROOP_WIDE$id)


STROOP_MR <- dplyr::left_join(x = MR_wide_z,
                 y = STROOP_WIDE[, c("id", "congruent_z",
                                     "incongruent_z")],
                 by = "id"
                 )

# combining Stroop and MR
STROOP_MR %>%
  select(., -id) %>%
  na.omit() %>%
  factoextra::fviz_nbclust(., kmeans, method = "gap_stat")
```








## **K Nearest Neighbors**

Classification for categorical variables. 

Predict category belongingness for cases based on k nearest neighbors. Involves calculating the distance of a point to other points in categories of a variable. Class prediction is determined based on the lowest average

```{r}

```


# What's actually going on with `dist()`, `hclust()`, and `as.dendogram()`?

Consider a vector of numeric values from 1:5 inclusive. For each element, the distance (e.g., euclidean, etc. ) from it to all non-reduncant) other elements can be computed. 

For example, 1 is:

- *one* unit from 2, 
- *two* units from 3, 
- *three* units from 4, and 
- *four* units from 5. 

2 is:

- *one* unit from 3,
- *two* units from 4, and
- *three* units from 5
- Note: 2 from 1 is already assumed by 1 from 2

etc.

An Example of the Distance Matrix:

   1: 2: 3: 4: 5:
1:    1  2  3  4
2: 1     1  2  3
3: 2  1     1  2
4: 3  2  1     1
5: 4  3  2  1



```{r}
dat <- data.frame(a = c(1:5), b = c(1:5))

dat %>% 
  dist(., method = "euclidean", upper = T)

dat %>% 
  dist(., method = "manhattan", upper = T)

(hclust_dat <- dat %>% 
  dist(., method = "euclidean") %>% 
  hclust(.))

hclust_dat$method
?dist

(dend_dat <- dat %>% 
  dist() %>% 
  hclust() %>% 
  as.dendrogram())

corfunc <- function(data, stat = "p") {
  #dt = data[vars, ]
  return(
    c(
    cor(data[,1], data[,2], method = stat)#,
#    median(data[,1]),
 #   median(data[,2])
  ))
}

corfunc <- function(pair, indices, method){
 dat <- pair[indices,]
 return(
   c(cor(dat[,1], dat[,2], method = method))
   )
}






set.seed(12345)
?sample
corr_boot <- ''

iris
data = iris[,c("Sepal.Width", "Sepal.Length")]
m <- iris %>%
  dplyr::arrange(., Petal.Width) %>%
  #dplyr::arrange(., Petal.Length) %>%
  #dplyr::arrange(., Species) %>%
  lm(Sepal.Length ~ Sepal.Width + Petal.Width + Petal.Length, 
     data = .)
#car::durbinWatsonTest(m)
car::vif(m)
dim(iris)
data %>% 
  
  dplyr::sample_n(., 50) %>%
  boot::boot(data = ., 
             statistic = corfunc, 
             R = 10, 
             method = 'p') 



sample(x = .data, size = 10, replace = T)

  

plot(corr_boot, index = 1)

boot::boot.ci(corr_boot, index = 1)
 
 
i = iris[, c("Sepal.Length", "Sepal.Width")] 
#%>%
  #corfunc(.)
  boot::boot(data = i, 
             statistic = corfunc,
             R = 10)
?boot::boot

dend_dat %>%
  plot()

dend_elements %>% # the tree labels
  labels()

dend_elements %>% # the number of tree leaves 
  nleaves()

dend_elements %>% # the number of tree nodes (including the leaves)
  nnodes()


dend_elements %>% # head provides the str and the head
  head() 

```




# **Clustering**

Cluster analysis is a class of methods for grouping population members into homogeneous subsets (e.g., classes or clusters) when the groups are not known in advance. Clustering is an unsupervised machine learning approach insofar as the clustering is unknown to the scientist and will be created. 

Computes the distance between cases (e.g., rows) according to a determined distance metric in order to build a similarity or dissimilarity matrix of those distances. Cases that are similar, or in proximity based on the distance, will be grouped in the same cluster. Members of a given cluster should therefore be similar to other members of the same cluster and different from members in other clusters. 







# **Hierarchical Clustering**

Depending on the distance measure and linking method, hierarchical clustering will determine the best number of clusters for the data. Variables can be continuous, ordinal, or nominal. 

Clusters are created using a *bottom-up clustering* method. Groups are built from individual cases and combined into clusters until the full data set forms a single cluster. The similarity and dissimilarity is represented in a dendogram, or a tree representation of the cluster(s). 

Hierarchical cluster and k-means cluster.

```{r}
## -----------------------------------------------------------------------------
#iris  <- datasets::iris
#iris2 <- iris[,-5]
#species_labels <- iris[,5]
#iris2$labels <- iris[,5]
#iris2$color <- rev(rainbow_hcl(3))[as.numeric(species_labels)]
```

## *Working with Iris Data*
### *Visualize using a Scatterplot Matrix (SPLOM) or Pairs Plot*

```{r, fig.width = 9, fig.height = 9, fig.show = 'hold'}
GGally::ggpairs(
  data = IRIS, 
  mapping = aes(color = Species)
) 
```

Notice how the *Setosa* species tend to have smaller petal length and width compared with both *Versicolor* and *Virginica* iris species. By contrast, differentiating *Versicolor* from *Virginica* cannot easily be done based on these variable measurements. 

The same conclusion can be made by looking at the parallel coordinates plot of the data:

Plots will use `rownames()` when rendering the rows. If you want to be able to identify rows in the plots, change the `rownames()`. Remember that `rownames` have to be unique so you cannot set them to the species names. For all rows to be unique, we can paste together the row numver and the species name but clean up the name to an abbreviation.  

```{r}
rownames(IRIS) <- rownames(IRIS) %>% 
  paste0(IRIS$Species, "-", .) %>%
  gsub("setosa", "S", .) %>%
  gsub("versicolor", "VC", .) %>%
  gsub("virginica", "VG", .)
```

### *Computing Distance*

The `base R` `stats::dist` can be used for calculating a distance matrix for distances between rows of data. The method argument needs to be either `"euclidean"`, `"maximum"`, `"manhattan"`, `"canberra"`, `"binary"` or `"minkowski"`. Each distance metric can be reviewed by looking at the `?stats` help documentation. By default is the `"euclidean"` distance.

```{r}
dist_iris <- IRIS %>%
  dplyr::select(., where(is.numeric)) %>%
  dist(.) # method="man" # is a bit better

#dist_iris
```


### *Creating a Hierarchical Cluster*

Using the computed distances, we can create a hierarchical cluster of the data. Such clustering is based on the dissimilarities 
method

`stats::hclust()` will need to pass the dissimilarity matrix structure returned from `dist()` to the `d` parameter.
In order to compute the structure, we need to specify the  agglomeration method to use, which would be either `"ward.D"`, `"ward.D2"`, `"single"`, `"complete"`, `"average"` (= UPGMA), `"mcquitty"` (= WPGMA), `"median"` (= WPGMC) or `"centroid"` (= UPGMC). More details can be reviewed in the help documentation `?hclust`. We will use `method = "complete"`. 

```{r}
hclust_iris <- hclust(d = dist_iris, 
                      method = "ward.D"  #"complete"
                      )


summary(hclust_iris)
```

### **Visualizing the Distances**

In order to understand the distance relationships between rows, we can use `plot()` to view the tree structure of `hclust_iris`. The tree will group cases that are most similar such that the distance between branches is shortest for similar cases and farthest for dissimilar distances. 

```{r}
plot(hclust_iris)
```

Perhaps a better alternative is to convert the dissimilarity structure to a dendogram using `as.dendogram()`. The `dendextend` library will allow for modifying the dendogram for visualization.

???????????????????????? # order it the closest we can to the order of the observations:
dend <- ''
rotate(dend_iris, 1:150)

Taking all the steps together:

```{r}
hclust_iris <- IRIS %>%
  dplyr::select(., where(is.numeric)) %>%
  dist(., method = "euclidean") %>%
  hclust(., method = "complete") 

dend_iris <- hclust_iris %>%
  as.dendrogram(.)

hclust_iris; summary(hclust_iris)

dend_iris; summary(dend_iris)
```

In order to color the branches, use `dendextend::color_branches()` to color the terminal leaves the dendogram luster as well as the edges leading to those leaves. In 

??The edgePar attribute of nodes will be augmented by a new list item col. ??

Passing a numeric value to `k` will specify how many groups or cluster to visualize. This invokes `cutree` which cuts a tree or hierarchical cluster into data groups. Similarly, specifyin a numeric value for `h` will cut the height of the tree. 

```{r}
groups <- cutree(hclust_iris, k = 3)

# in order to see the clusters easily, use a rectangle
rect.hclust(hclust_iris, k = 3, border = "red")

# Color the branches based on the clusters:
plot(color_branches(dend = dend_iris, k = 2), 
     main = "Clustered Iris Dataset"
     )


plot(color_branches(dend = dend_iris, k = 2), 
     main = "Clustered Iris Dataset"
     )
```


The default view is vertical. To view a horizontal structure, set `hori = TRUE`. Also, if we wish to change the number of clusters to color, assign the returned dendogram also with changes to `k` or other argument.

```{r}
plot(dend_iris <- color_branches(dend = dend_iris, k = 4), 
     main = "Clustered Iris Dataset",
     horiz = TRUE
     )
```


```{r}
groups <- c("A", "B", "C")

plot(dend_iris <- color_branches(
  dend = dend_iris, 
  k = 3,
  groupLabels = groups
  ), 
     main = "Clustered Iris Dataset",
     horiz = TRUE #   nodePar = list(cex = .007)
  )
```

We will need to reverse the labels though as the cases at the top (or right side of `horiz = FALSE`) are `"setosa"`. 

```{r}
groups <- rev(levels(iris$Species))

plot(dend_iris <- color_branches(
  dend = dend_iris, 
  k = 3,
  groupLabels = groups
  ), 
     main = "Clustered Iris Dataset",
     horiz = TRUE #nodePar = list(cex = .007)
  )


```


To hang the labels, use `hang.dendogram()`.

```{r}
plot(color_branches(
  hang.dendrogram(dend_iris), 
  k = 3,
  groupLabels = groups
  ), 
     main = "Clustered Iris Dataset",
     horiz = F #nodePar = list(cex = .007)
  )
```


## *Mental Rotation Data*

First, make sure you scale the data.

```{r}

```


Note: If you want to change `rownames()`, you cannot for a tibble, so first convert to data frame. Then assign the `id` to the rownames of the data frame.

```{r}
MR_wide <- MR_wide %>% as.data.frame(.)

rownames(MR_wide) <- MR_wide$id
```


Let's look at a pairs plot for the `Same` and `Mirror` trials separately. Select only the scaled columns because the unstandardized columns will be the same. Keep in mind that some individuals responded in ways not expected for the `Same` trials so theses differences will be observed in the data as well.

```{r}
MR_wide %>%
  dplyr::select(., contains("Same")) %>%
  dplyr::select(., ends_with("_z")) %>%
  GGally::ggpairs(data = .) 

MR_wide %>%
  dplyr::select(., contains("Mirror")) %>%
  dplyr::select(., ends_with("_z")) %>%
  GGally::ggpairs(data = .) 
```

### *Creating the Dendogram*

Compute distance, the hierarchical cluster, and make a dendogram.

```{r}
dend_MR_same <- MR_wide %>%
  filter(., id < 9999) %>%
  dplyr::select(., contains("Same")) %>%
  dplyr::select(., ends_with("_z")) %>%
  dist(., method = "euclidean") %>%
  hclust(., method = "complete") %>%
  as.dendrogram(.)

dend_MR_mirror <- MR_wide %>%
  filter(., id < 9999) %>%
  dplyr::select(., contains("Mirror")) %>%
  dplyr::select(., ends_with("_z")) %>%
  dist(., method = "euclidean") %>%
  hclust(., method = "complete") %>%
  as.dendrogram(.)

dend_MR_same; summary(dend_MR_same)

dend_MR_mirror; summary(dend_MR_mirror)
```

### **Visualizing**

the Distances
and the dendogram

```{r}
plot(color_branches(dend = dend_MR_same), 
     main = "Clustered Mental Rotation Dataset (Same Trials)",
     horiz = TRUE)

plot(color_branches(dend = dend_MR_mirror), 
     main = "Clustered Mental Rotation Dataset (Mirror Trials)",
     horiz = TRUE)
```




```{r}
iris_species <- ''
rev(levels(iris[,5]))

?dist
d_iris  <-  


library(dendextend)
dend <- as.dendrogram(hc_iris)
# order it the closest we can to the order of the observations:
dend <- rotate(dend, 1:150)

# Color the branches based on the clusters:
dend <- color_branches(dend, k=3) #, groupLabels=iris_species)

# Manually match the labels, as much as possible, to the real classification of the flowers:
labels_colors(dend) <-
   rainbow_hcl(3)[sort_levels_values(
      as.numeric(iris[,5])[order.dendrogram(dend)]
   )]

# We shall add the flower type to the labels:
labels(dend) <- paste(as.character(iris[,5])[order.dendrogram(dend)],
                           "(",labels(dend),")", 
                           sep = "")
# We hang the dendrogram a bit:
dend <- hang.dendrogram(dend,hang_height=0.1)
# reduce the size of the labels:
# dend <- assign_values_to_leaves_nodePar(dend, 0.5, "lab.cex")
dend <- set(dend, "labels_cex", 0.5)
# And plot:
par(mar = c(3,3,3,7))
plot(dend, 
     main = "Clustered Iris data set
     (the labels give the true flower species)", 
     horiz =  TRUE,  nodePar = list(cex = .007))
legend("topleft", legend = iris_species, fill = rainbow_hcl(3))

#### BTW, notice that:
# labels(hc_iris) # no labels, because "iris" has no row names
# is.integer(labels(dend)) # this could cause problems...
# is.character(labels(dend)) # labels are no longer "integer"

## ---- fig.width=7, fig.height=7-----------------------------------------------
# Requires that the circlize package will be installed
par(mar = rep(0,4))
circlize_dendrogram(dend)

## ---- echo=FALSE, eval=FALSE--------------------------------------------------
#  # some_col_func <- function(n, top_color = "red4") {
#  #   seq_cols <- c("#F7FCFD", "#E0ECF4", "#BFD3E6", "#9EBCDA", "#8C96C6", "#8C6BB1",
#  #                 "#88419D", "#810F7C")
#  #   c(colorRampPalette(seq_cols, bias =1)(n-1), top_color)
#  # }
#  

## ---- fig.width=9, fig.height=9-----------------------------------------------

some_col_func <- function(n) rev(colorspace::heat_hcl(n, c = c(80, 30), l = c(30, 90), power = c(1/5, 1.5)))

# scaled_iris2 <- iris2 %>% as.matrix %>% scale
# library(gplots)
gplots::heatmap.2(as.matrix(iris2), 
          main = "Heatmap for the Iris data set",
          srtCol = 20,
          dendrogram = "row",
          Rowv = dend,
          Colv = "NA", # this to make sure the columns are not ordered
          trace="none",          
          margins =c(5,0.1),      
          key.xlab = "Cm",
          denscol = "grey",
          density.info = "density",
          RowSideColors = rev(labels_colors(dend)), # to add nice colored strips		
          col = some_col_func
         )



## ---- cache = FALSE, eval = FALSE---------------------------------------------
#  heatmaply::heatmaply(as.matrix(iris2),
#            dendrogram = "row",
#            Rowv = dend)

## -----------------------------------------------------------------------------

hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", 
        "median", "centroid", "ward.D2")
iris_dendlist <- dendlist()
for(i in seq_along(hclust_methods)) {
   hc_iris <- hclust(d_iris, method = hclust_methods[i])   
   iris_dendlist <- dendlist(iris_dendlist, as.dendrogram(hc_iris))
}
names(iris_dendlist) <- hclust_methods
iris_dendlist

## ---- fig.width=8, fig.height=8-----------------------------------------------
iris_dendlist_cor <- cor.dendlist(iris_dendlist)
iris_dendlist_cor
corrplot::corrplot(iris_dendlist_cor, "pie", "lower")

## ---- fig.width=8, fig.height=8-----------------------------------------------
iris_dendlist_cor_spearman <- cor.dendlist(iris_dendlist, method_coef = "spearman")
corrplot::corrplot(iris_dendlist_cor_spearman, "pie", "lower")

## ---- fig.height=5------------------------------------------------------------
# The `which` parameter allows us to pick the elements in the list to compare
iris_dendlist %>% dendlist(which = c(1,8)) %>% ladderize %>% 
   set("branches_k_color", k=3) %>% 
   # untangle(method = "step1side", k_seq = 3:20) %>%
   # set("clear_branches") %>% #otherwise the single lines are not black, since they retain the previous color from the branches_k_color.
   tanglegram(faster = TRUE) # (common_subtrees_color_branches = TRUE)

## ---- fig.height=5------------------------------------------------------------
# The `which` parameter allows us to pick the elements in the list to compare
iris_dendlist %>% dendlist(which = c(1,4)) %>% ladderize %>% 
   set("branches_k_color", k=2) %>% 
   # untangle(method = "step1side", k_seq = 3:20) %>%
   tanglegram(faster = TRUE) # (common_subtrees_color_branches = TRUE)

## ---- fig.height=5------------------------------------------------------------
# The `which` parameter allows us to pick the elements in the list to compare
iris_dendlist %>% dendlist(which = c(1,4)) %>% ladderize %>% 
   # untangle(method = "step1side", k_seq = 3:20) %>%
   set("rank_branches") %>%
   tanglegram(common_subtrees_color_branches = TRUE)

## -----------------------------------------------------------------------------
length(unique(common_subtrees_clusters(iris_dendlist[[1]], iris_dendlist[[4]]))[-1])
# -1 at the end is because we are ignoring the "0" subtree, which indicates leaves that are singletons.

## ---- fig.height=5------------------------------------------------------------
iris_dendlist %>% dendlist(which = c(3,4)) %>% ladderize %>% 
   untangle(method = "step1side", k_seq = 2:6) %>%
   set("branches_k_color", k=2) %>% 
   tanglegram(faster = TRUE) # (common_subtrees_color_branches = TRUE)

## ---- fig.height=15-----------------------------------------------------------
par(mfrow = c(4,2))
for(i in 1:8) {
   iris_dendlist[[i]] %>% set("branches_k_color", k=2) %>% plot(axes = FALSE, horiz = TRUE)
   title(names(iris_dendlist)[i])
}

## -----------------------------------------------------------------------------
iris_dendlist_cor2 <- cor.dendlist(iris_dendlist, method = "common")
iris_dendlist_cor2

## ---- fig.width=5, fig.height=5-----------------------------------------------
# corrplot::corrplot(iris_dendlist_cor2, "pie", "lower")

## -----------------------------------------------------------------------------

get_ordered_3_clusters <- function(dend) {
   cutree(dend, k = 3)[order.dendrogram(dend)]
}

dend_3_clusters <- lapply(iris_dendlist, get_ordered_3_clusters)

compare_clusters_to_iris <- function(clus) {FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE)}

clusters_performance <- sapply(dend_3_clusters, compare_clusters_to_iris)
dotchart(sort(clusters_performance), xlim = c(0.7,1),
         xlab = "Fowlkes-Mallows Index (from 0 to 1)",
         main = "Perormance of clustering algorithms \n in detecting the 3 species",
         pch = 19)

## -----------------------------------------------------------------------------
train <- dendextend::khan$train
test <- dendextend::khan$test

## -----------------------------------------------------------------------------
d_train <- train %>% dist %>% hclust %>% as.dendrogram
d_test <- test %>% dist %>% hclust %>% as.dendrogram
d_train_test <- dendlist(train = d_train, test = d_test)

## -----------------------------------------------------------------------------
d_train_test %>% cor.dendlist

## -----------------------------------------------------------------------------
d_train_test %>% cor.dendlist(method_coef = "spearman")

## -----------------------------------------------------------------------------
Bk_plot(d_train, d_test, k = 2:30, xlim = c(2,30))

## ---- fig.width=8, fig.height=5-----------------------------------------------
pre_tang_d_train_test <- d_train_test %>% ladderize %>% # untangle %>%
   set("branches_k_color", k = 7)
train_branches_colors <- get_leaves_branches_col(pre_tang_d_train_test$train)
pre_tang_d_train_test %>% tanglegram(fast = TRUE, color_lines = train_branches_colors)

## ---- echo = FALSE------------------------------------------------------------
# dput(d_train_test_common)
d_train_test_common <- structure(list(train = structure(list(structure(list(structure(171L, label = "491565", members = 1L, height = 0, leaf = TRUE), 
    structure(178L, label = "505491", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 7.1369942952198), 
    structure(list(structure(list(structure(8L, label = "283315", members = 1L, height = 0, leaf = TRUE), 
        structure(9L, label = "897177", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 2.55936539399907), 
        structure(list(structure(list(structure(106L, label = "345553", members = 1L, height = 0, leaf = TRUE), 
            structure(112L, label = "307660", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 5.17910461856101), 
            structure(list(structure(list(structure(268L, label = "504791", members = 1L, height = 0, leaf = TRUE), 
                structure(306L, label = "782503", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 4.27052507661529), 
                structure(list(structure(list(structure(246L, label = "81518", members = 1L, height = 0, leaf = TRUE), 
                  structure(290L, label = "280837", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 1.37572388944875), 
                  structure(list(structure(list(structure(266L, label = "866694", members = 1L, height = 0, leaf = TRUE), 
                    structure(277L, label = "811956", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 3.31301518861595), 
                    structure(list(structure(273L, label = "842918", members = 1L, height = 0, leaf = TRUE), 
                      structure(274L, label = "626555", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 2.71864544948399)), members = 4, midpoint = 1.5, height = 6.35097701381449)), members = 6, midpoint = 2, height = 8.7097033164167)), members = 8, midpoint = 2.25, height = 9.23807936424017)), members = 10, midpoint = 2.375, height = 11.6573350998416)), members = 12, midpoint = 2.4375, height = 17.5620766260713)), members = 14, midpoint = 2.46875, height = 30.2363452779928, class = "dendrogram"), 
    test = structure(list(structure(list(structure(list(structure(171L, label = "491565", members = 1L, height = 0, leaf = TRUE), 
        structure(178L, label = "505491", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 3.96666017450449), 
        structure(list(structure(list(structure(list(structure(268L, label = "504791", members = 1L, height = 0, leaf = TRUE), 
            structure(306L, label = "782503", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 2.31497882927685), 
            structure(list(structure(list(structure(266L, label = "866694", members = 1L, height = 0, leaf = TRUE), 
                structure(277L, label = "811956", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 1.75475236429532), 
                structure(list(structure(273L, label = "842918", members = 1L, height = 0, leaf = TRUE), 
                  structure(274L, label = "626555", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 1.34617375921535)), members = 4, midpoint = 1.5, height = 2.76465021476497)), members = 6, midpoint = 2, height = 4.52927251774499), 
            structure(list(structure(list(structure(246L, label = "81518", members = 1L, height = 0, leaf = TRUE), 
                structure(290L, label = "280837", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 0.714433271901582), 
                structure(list(structure(8L, label = "283315", members = 1L, height = 0, leaf = TRUE), 
                  structure(9L, label = "897177", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 1.71895552589356)), members = 4, midpoint = 1.5, height = 6.44143803354499)), members = 10, midpoint = 4.75, height = 7.736516720075)), members = 12, midpoint = 3.625, height = 11.0066972375913), 
        structure(list(structure(106L, label = "345553", members = 1L, height = 0, leaf = TRUE), 
            structure(112L, label = "307660", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 3.6486307417989)), members = 14, midpoint = 8.0625, height = 18.2331742971431, class = "dendrogram")), class = "dendlist", .Names = c("train", 
"test"))

## -----------------------------------------------------------------------------
# This was calculated before
# d_train_test_common <- d_train_test %>% prune_common_subtrees.dendlist
# d_train_test_common
d_train_test_common %>% untangle %>%  tanglegram(common_subtrees_color_branches = TRUE)

## -----------------------------------------------------------------------------
d_train_test %>% nleaves
d_train_test_common %>% nleaves

## -----------------------------------------------------------------------------
votes.repub <- cluster::votes.repub

## ---- fig.height=5------------------------------------------------------------
years <- as.numeric(gsub("X", "", colnames(votes.repub)))

par(las = 2, mar = c(4.5, 3, 3, 2) + 0.1, cex = .8)
# MASS::parcoord(votes.repub, var.label = FALSE, lwd = 1)
matplot(1L:ncol(votes.repub), t(votes.repub), type = "l", col = 1, lty = 1,
        axes = F, xlab = "", ylab = "")
axis(1, at = seq_along(years), labels = years)
axis(2)
# Add Title
title("Votes for Republican Candidate\n in Presidential Elections \n (each line is a country - over the years)")

## ---- fig.width=9, fig.height=9-----------------------------------------------
arcsin_transformation <- function(x) asin(x/100)

dend_NA <- votes.repub %>% is.na %>%
   dist %>% hclust %>% as.dendrogram %>% ladderize

dend <- votes.repub %>% arcsin_transformation %>%
   dist %>% hclust(method = "com") %>% as.dendrogram %>%
   rotate(labels(dend_NA)) %>%
   color_branches(k=3)

# some_col_func <- function(n) rev(colorspace::heat_hcl(n, c = c(80, 30), l = c(30, 90), power = c(1/5, 1.5)))
some_col_func <- colorspace::diverge_hcl


# par(mar = c(3,3,3,3))
# library(gplots)
gplots::heatmap.2(as.matrix(votes.repub), 
          main = "Votes for\n Republican Presidential Candidate\n (clustered using complete)",
          srtCol = 60,
          dendrogram = "row",
          Rowv = dend,
          Colv = "NA", # this to make sure the columns are not ordered
          trace="none",          
          margins =c(3,6),      
          key.xlab = "% Votes for Republican\n Presidential Candidate",
          labCol = years,
          denscol = "grey",
          density.info = "density",
          col = some_col_func
         )
          # RowSideColors = rev(labels_colors(dend)), # to add nice colored strips		

## -----------------------------------------------------------------------------

hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", 
        "median", "centroid", "ward.D2")
votes.repub_dendlist <- dendlist()

for(i in seq_along(hclust_methods)) {
   tmp_dend <- votes.repub %>% arcsin_transformation %>% dist %>% hclust(method = hclust_methods[i]) %>% as.dendrogram 
   votes.repub_dendlist <- dendlist(votes.repub_dendlist, tmp_dend)
}
names(votes.repub_dendlist) <- hclust_methods
# votes.repub_dendlist

## ---- fig.width=8, fig.height=8-----------------------------------------------
corrplot::corrplot(cor.dendlist(votes.repub_dendlist), "pie", "lower")

## ---- echo=FALSE, fig.width=9, fig.height=9-----------------------------------
arcsin_transformation <- function(x) asin(x/100)

dend_NA <- votes.repub %>% is.na %>%
   dist %>% hclust %>% as.dendrogram %>% ladderize

dend <- votes.repub %>% arcsin_transformation %>%
   dist %>% hclust(method = "ave") %>% as.dendrogram %>%
   rotate(labels(dend_NA)) %>%
   color_branches(k=3)

# some_col_func <- function(n) rev(colorspace::heat_hcl(n, c = c(80, 30), l = c(30, 90), power = c(1/5, 1.5)))
some_col_func <- colorspace::diverge_hcl


# par(mar = c(3,3,3,3))
# library(gplots)
gplots::heatmap.2(as.matrix(votes.repub), 
          main = "Votes for\n Republican Presidential Candidate\n (clustered using average)",
          srtCol = 60,
          dendrogram = "row",
          Rowv = dend,
          Colv = "NA", # this to make sure the columns are not ordered
          trace="none",          
          margins =c(3,6),      
          key.xlab = "% Votes for Republican\n Presidential Candidate",
          labCol = years,
          denscol = "grey",
          density.info = "density",
          col = some_col_func
         )
          # RowSideColors = rev(labels_colors(dend)), # to add nice colored strips		

## ---- echo=FALSE--------------------------------------------------------------

ord1 <- c("North Carolina", "Virginia", "Tennessee", "Kentucky", "Maryland", 
"Delaware", "Oklahoma", "Missouri", "New Mexico", "Oregon", "Washington", 
"California", "West Virginia", "Hawaii", "Nevada", "Arizona", 
"Montana", "Idaho", "Wyoming", "Utah", "Colorado", "Alaska", 
"Illinois", "New York", "Indiana", "Ohio", "Connecticut", "New Hampshire", 
"New Jersey", "Pennsylvania", "Iowa", "South Dakota", "North Dakota", 
"Wisconsin", "Minnesota", "Nebraska", "Kansas", "Maine", "Michigan", 
"Massachusetts", "Rhode Island", "Vermont", "Alabama", "Georgia", 
"Louisiana", "Arkansas", "Florida", "Texas", "South Carolina", 
"Mississippi")

ord2 <- c("North Carolina", "Virginia", "Tennessee", "Oklahoma", "Kentucky", 
"Maryland", "Delaware", "Missouri", "New Mexico", "West Virginia", 
"Oregon", "Washington", "California", "Nevada", "Arizona", "Montana", 
"Colorado", "Alaska", "Idaho", "Wyoming", "Utah", "Hawaii", "Maine", 
"Illinois", "New York", "New Jersey", "Indiana", "Ohio", "Connecticut", 
"New Hampshire", "Pennsylvania", "Michigan", "Iowa", "South Dakota", 
"North Dakota", "Wisconsin", "Minnesota", "Massachusetts", "Rhode Island", 
"Nebraska", "Kansas", "Vermont", "Alabama", "Georgia", "Louisiana", 
"Arkansas", "Florida", "Texas", "South Carolina", "Mississippi"
)

# dput(lapply(dends, labels)[[2]])


## -----------------------------------------------------------------------------
dend_com <- votes.repub %>% arcsin_transformation %>%
   dist %>% hclust(method = "com") %>% as.dendrogram %>%
   rotate(labels(dend_NA)) %>%
   color_branches(k=3) # %>% ladderize
dend_ave <- votes.repub %>% arcsin_transformation %>%
   dist %>% hclust(method = "ave") %>% as.dendrogram %>%
   rotate(labels(dend_NA)) %>%
   color_branches(k=3) # %>% ladderize

# The orders were predefined after using untangle("step2side")
# They are omitted here to save running time.
dend_com <- rotate(dend_com, ord1)
dend_ave <- rotate(dend_ave, ord2)

dends <- dendlist(complete = dend_com, average = dend_ave) # %>% untangle("step2side")
dends  %>% tanglegram(margin_inner = 7)


## -----------------------------------------------------------------------------
animals <- cluster::animals

colnames(animals) <- c("warm-blooded", 
                       "can fly",
                       "vertebrate",
                       "endangered",
                       "live in groups",
                       "have hair")

## ---- fig.width=9, fig.height=9-----------------------------------------------

dend_r <- animals %>% dist(method = "man") %>% hclust(method = "ward.D") %>% as.dendrogram %>% ladderize %>%
    color_branches(k=4)

dend_c <- t(animals) %>% dist(method = "man") %>% hclust(method = "com") %>% as.dendrogram %>% ladderize%>%
    color_branches(k=3)


# some_col_func <- function(n) rev(colorspace::heat_hcl(n, c = c(80, 30), l = c(30, 90), power = c(1/5, 1.5)))
# some_col_func <- colorspace::diverge_hcl
# some_col_func <- colorspace::sequential_hcl
some_col_func <- function(n) (colorspace::diverge_hcl(n, h = c(246, 40), c = 96, l = c(65, 90)))



# par(mar = c(3,3,3,3))
# library(gplots)
gplots::heatmap.2(as.matrix(animals-1), 
          main = "Attributes of Animals",
          srtCol = 35,
          Rowv = dend_r,
          Colv = dend_c,
          trace="row", hline = NA, tracecol = "darkgrey",         
          margins =c(6,3),      
          key.xlab = "no / yes",
          denscol = "grey",
          density.info = "density",
          col = some_col_func
         )


## -----------------------------------------------------------------------------

hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", 
        "median", "centroid", "ward.D2")
animals_dendlist <- dendlist()

for(i in seq_along(hclust_methods)) {
   tmp_dend <-  animals %>% dist(method = "man") %>% 
      hclust(method = hclust_methods[i]) %>% as.dendrogram 
   animals_dendlist <- dendlist(animals_dendlist, tmp_dend)
}
names(animals_dendlist) <- hclust_methods
# votes.repub_dendlist

## ---- fig.width=8, fig.height=8-----------------------------------------------
cophenetic_cors <- cor.dendlist(animals_dendlist)
corrplot::corrplot(cophenetic_cors, "pie", "lower")

## -----------------------------------------------------------------------------
remove_median <- dendlist(animals_dendlist, which = c(1:8)[-6] )
FM_cors <- cor.dendlist(remove_median, method = "FM_index", k = 4)
corrplot::corrplot(FM_cors, "pie", "lower")


```

