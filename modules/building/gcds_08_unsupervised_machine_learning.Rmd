---
title: "Unsupervised ML"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list = ls(all.names = TRUE))      # remove objects in R

source("https://pastebin.com/raw/8mXH02yg")   # run and comment out before knitting
source("https://pastebin.com/raw/97NNTTzu")   # run to include in function definitions

# set the paths for project, script, and data dirs
proj_dir <- gsub("GCDS.*.Rmd", "GCDS", get_ActivePath())
proj_name = ""
r_dir    <- paste(proj_dir, "r", sep = "/")    # r subdir
data_dir <- paste(proj_dir, "data", sep = "/") # data subdir
if ( proj_name != "" & !dir.exists(paste(proj_dir, proj_name, sep = "/")) ) {
  # create project dir
  suppressWarnings(dir.create(paste(proj_dir, proj_name, sep = "/")))
  r_dir <- gsub("/r", paste0("/", proj_name, "/r"), r_dir)
  data_dir <- gsub("/data", paste0("/", proj_name, "/data"), data_dir)
  # create sub directories
  suppressWarnings(dir.create(r_dir))
  suppressWarnings(dir.create(data_dir)) }
```


# **Libraries**

```{r}
library(magrittr)
library(factorExtra)
library(dendextend)
#library(gplots)
library(colorspace) # get nicer colors
library()
library(ggplot2)
library(dplyr)

library(MASS)
```

# **Data Frame Objects**

```{r}
example1 <- data.frame(x = c(1,2,3,4,5), y = c(1,2,3,4,5))
example2 <- data.frame(x = c(1,2,3,4,5), y = c(1,2,4,4,3))

# Wine quality data
WINE <- readr::read_csv(paste(data_dir, "winequality.csv", sep = "/"))

# the classic iris data set
IRIS <- datasets::iris

MR_agg <- readr::read_csv(
  paste(data_dir, "mental_rotation_agg.csv", sep = "/"))

MR_agg <- MR_agg %>%
  mutate(., trialtype = factor(trialtype, 
                               levels = c("Same", "Mirror"), 
                               ordered = T))

MR_wide <- readr::read_csv(
  paste(data_dir, "mental_rotation_wide.csv", sep = "/")) %>%
  mutate(., id = as.character(id))

MR_wide <- MR_wide %>%
  mutate(., across(
    .cols = starts_with("Mirror"), 
    .fns = ~as.numeric(scale(.x)), 
    .names = "{.col}_z")) %>%
  mutate(., across(
    .cols = starts_with("Same"), 
    .fns = ~as.numeric(scale(.x)), 
    .names = "{.col}_z")) 

#There is an NA id, so remove.
MR_wide <- MR_wide %>%
  filter(., !is.na(id))

view(MR_wide)
```

## **K Nearest Neighbors**

Classification for categorical variables. 

Predict category belongingness for cases based on k nearest neighbors. Involves calculating the distance of a point to other points in categories of a variable. Class prediction is determined based on the lowest average

```{r}

```


# What's actually going on with `dist()`, `hclust()`, and `as.dendogram()`?

Consider a vector of numeric values from 1:5 inclusive. For each element, the distance (e.g., euclidean, etc. ) from it to all non-reduncant) other elements can be computed. 

For example, 1 is:

- *one* unit from 2, 
- *two* units from 3, 
- *three* units from 4, and 
- *four* units from 5. 

2 is:

- *one* unit from 3,
- *two* units from 4, and
- *three* units from 5
- Note: 2 from 1 is already assumed by 1 from 2

etc.

An Example of the Distance Matrix:

   1: 2: 3: 4: 5:
1:    1  2  3  4
2: 1     1  2  3
3: 2  1     1  2
4: 3  2  1     1
5: 4  3  2  1



```{r}
dat <- data.frame(a = c(1:5), b = c(1:5))

dat %>% 
  dist(., method = "euclidean", upper = T)

dat %>% 
  dist(., method = "manhattan", upper = T)

(hclust_dat <- dat %>% 
  dist(., method = "euclidean") %>% 
  hclust(.))

hclust_dat$method
?dist

(dend_dat <- dat %>% 
  dist() %>% 
  hclust() %>% 
  as.dendrogram())

corfunc <- function(data, stat = "p") {
  #dt = data[vars, ]
  return(
    c(
    cor(data[,1], data[,2], method = stat)#,
#    median(data[,1]),
 #   median(data[,2])
  ))
}

corfunc <- function(pair, indices, method){
 dat <- pair[indices,]
 return(
   c(cor(dat[,1], dat[,2], method = method))
   )
}






set.seed(12345)
?sample
corr_boot <- ''

iris
data = iris[,c("Sepal.Width", "Sepal.Length")]
m <- iris %>%
  dplyr::arrange(., Petal.Width) %>%
  #dplyr::arrange(., Petal.Length) %>%
  #dplyr::arrange(., Species) %>%
  lm(Sepal.Length ~ Sepal.Width + Petal.Width + Petal.Length, 
     data = .)
#car::durbinWatsonTest(m)
car::vif(m)
dim(iris)
data %>% 
  
  dplyr::sample_n(., 50) %>%
  boot::boot(data = ., 
             statistic = corfunc, 
             R = 10, 
             method = 'p') 



sample(x = .data, size = 10, replace = T)

  

plot(corr_boot, index = 1)

boot::boot.ci(corr_boot, index = 1)
 
 
i = iris[, c("Sepal.Length", "Sepal.Width")] 
#%>%
  #corfunc(.)
  boot::boot(data = i, 
             statistic = corfunc,
             R = 10)
?boot::boot

dend_dat %>%
  plot()

dend_elements %>% # the tree labels
  labels()

dend_elements %>% # the number of tree leaves 
  nleaves()

dend_elements %>% # the number of tree nodes (including the leaves)
  nnodes()


dend_elements %>% # head provides the str and the head
  head() 

```




# **Clustering**

Cluster analysis is a class of methods for grouping population members into homogeneous subsets (e.g., classes or clusters) when the groups are not known in advance. Clustering is an unsupervised machine learning approach insofar as the clustering is unknown to the scientist and will be created. 

Computes the distance between cases (e.g., rows) according to a determined distance metric in order to build a similarity or dissimilarity matrix of those distances. Cases that are similar, or in proximity based on the distance, will be grouped in the same cluster. Members of a given cluster should therefore be similar to other members of the same cluster and different from members in other clusters. 

# **K-means Clustering**

A clustering method, sometimes also known as disjoint clustering. Specify the number of clusters in advance and algorithmically assign the cases to the clusters. A problem is with determining the correct number of clusters in order classify correctly. Determining the  optimal number of clusters may be through trial and error. 

change - The basic idea behind k-means clustering consists of defining clusters so that the total intra-cluster variation (known as total within-cluster variation) is minimized. There are several k-means algorithms available. However, the standard algorithm defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid - end change

k-means process

1. Specify *k* - the number of clusters to create from the data

2. Select at random  *k* items from the data to use as center of the initial clusters

3. Assign each item to their closest centroid, based on the  distance (e.g., Euclidean) between the item and the centroid.

4. For each *k* cluster, recompute the cluster centroid by calculating the new mean value of all the data points in the cluster.

5. Iteratively minimize the total within sum of square. Repeat Step 3 and Step 4, until the centroids do not change or the maximum number of iterations is reached (R uses 10 as the default value for the maximum number of iterations).




The biggest disadvantage is that it requires us to pre-specify the number of clusters (k). However, for the Uber dataset, you had some domain knowledge available that told you the number of boroughs in New York City. This might not always be the case with real world datasets. Hierarchical clustering is an alternative approach that does not require a particular choice of clusters. An additional disadvantage of k-means is that it is sensitive to outliers and different results can occur if you change the ordering of the data.

k-means is a lazy learner where generalization of the training data is delayed until a query is made to the system. Which means k-means starts working only when you trigger it to, thus lazy learning methods can construct a different approximation or result to the target function for each encountered query. It is a good method for online learning but it requires a possibly large amount of memory to store the data, and each request involves starting the identification of a local model from scratch.


You can see that the variables in the data are `r names(WINES)`. Each was rated/measured on different dimensions. A question is whether the wines can be classified or categorized based on these properties. For example, wines that are above average on Dimension A but are below the average Dimensions B:Z could be grouped into a category that is different from other wines that are below average on Dimension A and above average on Dimensions B:Z. In other words, the wines are more similar to other wines in that group but different from wines in another group. 

## *Data Preparation*

In order to perform clustering, however, we must first do some preparation. We will `scale()` the data so that variables are on the same scale. Doing so also allows us to easily identify wines that are above and below a mean based on their sign. We will remove any observations with `NA` and any categorical variables in the data set.

Clean up data. Omit any NA and then scale to have a mean of 0 and standard deviation of 1.

```{r}
WINE <- na.omit(WINE)

WINE_z <- WINE %>%
  mutate(., across(.cols = where(is.numeric), 
                   .fns = ~as.numeric(scale(.x)), 
                   .names = "{.col}")) 

view(WINE_z)

str(WINE_z)
```

# *Examining Relationships Among Variables*

There are `r length(names(WINES))-1)` measurement variables in the data set and we can examine the relationship among them using a correlation matrix or pairs plot.

```{r}
WINE_z %>%
  dplyr::select(., -color) %>%
  cor(.) %>%
  corrplot::corrplot(.)
```

The default plot illustrates correlation values by color and circle size. Labels are also in red. You can change some characteristics of the plot by changing various arguments. The correlogram reveals that there are some relationship between variables and these change from pair to pair. Given there are relationships, this means that wines measurements are moving in either the same or opposite directions. Thus, wines may group differently based on their measurements. 

If you want to examine pairs plots using `GGally::ggpairs()`, you may find graphical rendering problems because there are some many variables. If you wish to examine the data in this way, select a subset of variables of interest for separate plots. These too will take a moment to render.

```{r}
WINE_z %>%
  dplyr::select(., -color) %>%
  cor(.) %>%
  corrplot::corrplot(.,    # a correlation matrix
#         method = "ellipse", # a method for producing the correlation plot 
         method = "number",
         type = "full",    # a plot style (also "upper" and "lower")
         diag = TRUE,      # if TRUE (default), adds the diagonal
         tl.col = "black", # label color (default is red)
         bg = "white",     # background color
         title = "",       # a main title
         col = NULL)       # a color palette
```


```{r}
WINE_z %>%
  dplyr::select(., -color) %>%
  dplyr::select(., 1:4) #%>%
  GGally::ggpairs(.)
```

# *Determining the Optimal Number of Clusters*

Although k-means clustering is unsupervised, it still requires the scientist to specify the number of clusters to be created. But an importan question is: How do you determine the corrent number of expected clusters?

There are different methods.

One solution is to compute k-means clustering using different values of clusters k. Next, the `wss` (within sum of square) is drawn according to the number of clusters. The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.

Use `factoextra::fviz_nbclust()` to obtain a convenient solution to estimate the optimal number of clusters.

Using the total within sums of squares (e.g., `wss`) method, the `wss` will be plotted as a function of number of clusters one could extract. This plot will help you narrow the search space for determining the optimal number of clusters as this is only determined by, or suggested by, the machine algorithm. Sometimes the appropriate clusters will be informed by expertise, so clustering in this case is part experience, part math, and part art. The k-means algorithm determines clusters based on mean centers of the the clusters by minimizing the distance of cluster members from cluster centers. Other methods may be appropriate, for example, approaches based on median distances or other distance metrics.

The plot may take a moment to render after running the algorithm. Bigger data, longer run time. 

Note: If you receive an error like `"Error in .Call.graphics(C_palette2, .Call(C_palette2, NULL)) : invalid graphics state"`, execute `dev.off()` to clear your plot device. 

```{r}
(WINES_opt_wss <- WINE_z %>%
  dplyr::select(., -color) %>%
  factoextra::fviz_nbclust(., kmeans, method = "wss"))
```

The plot represents the variance within the clusters. Notice that when k = 1, there is a lot of variability. This indicates that a single cluster may not provide a good representation of the data, at least for grouping based on similarity. From 1 to 2 clusters, variability drops substantially, a slight drop from 2 to 3 clusters, and a drop is present after which the bend (or 'elbow') at k = 4 to 5 suggests that creating another cluster does not reduce the `wss` much at all. This suggests that 3 for sure or 4 clusters may be useful.

Using the `gap_stat` method will involve bootstrapping procedures so this will run for quite some time. Do not execute unless you need to get a coffee. This methods suggests that 4 clusters could be best.

```{r}
#(WINE_opt_gap <- WINE_z %>%
#  dplyr::select(., -color) %>%
#  factoextra::fviz_nbclust(., kmeans, method = "gap_stat"))
```

# **Clustering using the k-means Algorithm**

Using `kmeans()`, we can specify the data frame, the centers, the maximum number of iterations to run, as well as the number of starting centers. 

`x`: numeric matrix, numeric data frame or a numeric vector
`centers`: Possible values are the number of clusters (k) or a set of initial (distinct) cluster centers. If a number, a random set of (distinct) rows in x is chosen as the initial centers.
`iter.max`: The maximum number of iterations allowed. Default value is 10.
`algorithm`: The algorithm used to create the clusters.
`nstart`: The number of random starting partitions when centers is a number. Trying nstart > 1 is often recommended.

OK, so if 3 clusters is appropriate, we can run the `kmeans()` function by passing `centers = 3` and another model using `centers = 4` in order to create 3- and 4- clusters. Each run will assign each row to a cluster number. For now, just assume the default arguments for parameters, though of course clusters may differ based on changing them.

```{r}
WINE_km3 <- WINE_z %>%
  dplyr::select(., -color) %>%
  kmeans(., centers = 3, nstart = 25)

WINE_km4 <- WINE_z %>%
  dplyr::select(., -color) %>%
  kmeans(., centers = 4, nstart = 25)
```

Looking at the `names()` of the object, you see different information. `cluster` will provide the numeric assignment of the cluster based on the algorithm. `centers` will provide means for measures for each cluster. `size` and error measures are also provided.

Some key items returned from `kmeans()` include:

1. `cluster`: a vector of integers (from 1:k) indicating the cluster to which each point is allocated.

2. `centers`: a matrix of cluster centers.
    withinss: vector of within-cluster sum of squares, one component per cluster.

3. `tot.withinss`: total within-cluster sum of squares. That is, sum(withinss).

4. `size`: the number of points in each cluster.


```{r}
names(WINE_km3)
```

Add the clusters extracted to the `WINE_z` data frame.

```{r}
WINE_z$cluster3 <- WINE_km3$cluster
WINE_z$cluster4 <- WINE_km4$cluster
```

# **Describing the Clusters based on Measuremnts**

Looking at the grouping of each cluster, we can see the means (or dispersion) of the measurement variables for each cluster. Note that `kmeans()` will return an object containing the means for all measurements. You could obtain the same information and your own measures if you group by cluster and summarize using `dplyr`.

```{r}
view(as.data.frame(WINE_km3$centers), filter = "none")

view(as.data.frame(WINE_km4$centers), filter = "none")

WINES_z %>%
  group_by(., cluster3) %>%
  arrange(., cluster3) %>%
  summarise(., across(where(is.numeric), ~mean(.x, na.rm = TRUE))) %>%
  view(.)
```


## *Visualizing the Clusters*

A visual representation of the clusters is useful to see. A problem with rendering the clusters based on multivariate data is that the plot would need to be n-dimensional or in order to plot on a scatterplot, we need to specify the variables for both x and y, which is only two. 

A common solution to deal with multivariate data is to reduce the number of dimensions from n to 2 by applying a dimensionality reduction algorithm. A common approach is known as Principal Component Analysis, or PCA, which examines for principle 

PCA can take the n variables and reduce them to the first two principle component variables which represent the original variables. Having those two variables, then a dimension reduced scatterplot can be used to visualize the data using `factoextra:: fviz_cluster()`

In order to create the plot, we need to pair the kmeans object with the data frame but remove any variables in the data frame that are not the measurement variables. 

### *3-cluster plot*

```{r}
WINE_km3 %>%
  factoextra::fviz_cluster(
  object = ., 
  data = dplyr::select(WINE_z, -c(color, cluster3, cluster4)),
  ellipse.type = "norm"
  )

# we can change some arguments as well
WINE_km3 %>%
  factoextra::fviz_cluster(
  object = ., 
  data = dplyr::select(WINE_z, -c(color, cluster3, cluster4)),
  ellipse.type = "convex",
  palette = "jco",
  ggtheme = theme_minimal()
  ) 

```

### *4-cluster plot*

```{r}
WINE_km4 %>%
  factoextra::fviz_cluster(
  object = ., 
  data = dplyr::select(WINES_z, -c(color, cluster3, cluster4)),
  ellipse.type = "convex",
  palette = "jco",
  ggtheme = theme_minimal()
  ) 
```



# **Using Iris Data** 

If you like flowers, `iris` data are for you. Clean up the data, scale, and select measurement variables. Then, examine the plot to help determine the number of clusters that could be good.

```{r}
IRIS <- IRIS %>%
  na.omit(.) %>%
  mutate(., across(.cols = 1:4, 
                   .fns = ~as.numeric(scale(.x)), 
                   .names = "{.col}_z")) %>%
  dplyr::select(., c(Species, contains("_z")))


factoextra::fviz_nbclust(IRIS[2:5], kmeans, method = "wss") 
```


# *Creating the Clusters*

```{r}
iris_clusters <- kmeans(x = IRIS[,2:5], 
                        centers = 3, 
                        algorithm = "Hartigan-Wong"
                        )

iris_clusters
```

Add the cluster number to the Iris data frame as column variable named `Cluster`.

```{r}
IRIS$Cluster <- as.factor(iris_clusters$cluster)
```

Examine the structure of the object returned from the `kmeans()` function.

```{r}
str(iris_clusters)
```

Examine the cluster size for each k cluster.

```{r}
iris_clusters$size
```

## *Visualizing k-means clusters*

```{r}
factoextra::fviz_cluster(object = iris_clusters, 
                         data = IRIS[, 2:5], 
                         ellipse.type = "norm"
                         ) 

# Modify the color palette from the default 
factoextra::fviz_cluster(object = iris_clusters, 
                         data = IRIS[, 2:5], 
                         ellipse.type = "norm",
                         palette = "Set2", 
#                         ggtheme = theme_minimal()
                         ) + see::theme_modern()


# Plot points only
factoextra::fviz_cluster(object = iris_clusters, 
                         data = IRIS[, 2:5], 
                         ellipse.type = "norm",
                         geom = "point"
                         )

# Plot rownames text only
factoextra::fviz_cluster(object = iris_clusters, 
                         data = IRIS[, 2:5], 
                         ellipse.type = "norm",
                         geom = "text"
                         )

```

[ package] can be used to easily visualize k-means clusters. It takes k-means results and the original data as arguments. In the resulting plot, 
Data observations are represented by points after using PCA (if the number of variables is greater than 2). 



```{r}

```

# Obtain a number of optimal clusters

```{r}
IRIS <- IRIS %>%
  mutate(., across(.cols = 1:4, 
                   .fns = ~as.numeric(scale(.x)), 
                   .names = "{.col}_z")) %>%
  dplyr::select(., c(Species, contains("_z")))

MR_wide %>%
  dplyr::select(., contains("_z")) %>%
  dplyr::select(., contains("Same")) %>%
  na.omit() %>% # remove if NA for any row
  factoextra::fviz_nbclust(., kmeans, method = "wss")
```





# **Mental Rotation Data**

Maybe 4 clusters? 

# Create the k-means

```{r}
MR_same <- MR_wide %>%
  dplyr::select(., contains("_z")) %>%
  dplyr::select(., contains("Same")) %>%
  na.omit()

MR_clusters5 <- kmeans(x = MR_same,
         centers = 5, 
         algorithm = "Hartigan-Wong",
         nstart = 25
         )

MR_clusters4 <- kmeans(x = MR_same,
         centers = 4, 
         algorithm = "Hartigan-Wong",
         nstart = 25
         )

MR_clusters3 <- kmeans(x = MR_same,
         centers = 3, 
         algorithm = "Hartigan-Wong",
         nstart = 25
         )

MR_clusters2 <- kmeans(x = MR_same,
         centers = 2, 
         algorithm = "Hartigan-Wong",
         nstart = 25
         )

```




## *Visualize Clusters*

```{r}
factoextra::fviz_cluster(object = MR_clusters5, 
                         data = MR_same, 
                         ) + see::theme_modern()

factoextra::fviz_cluster(object = MR_clusters4, 
                         data = MR_same, 
                         ) #+ see::theme_modern()

factoextra::fviz_cluster(object = MR_clusters3, 
                         data = MR_same, 
                         ) #+ see::theme_modern()

factoextra::fviz_cluster(object = MR_clusters2, 
                         data = MR_same, 
                         ) #+ see::theme_modern()

```





# **Hierarchical Clustering**

Depending on the distance measure and linking method, hierarchical clustering will determine the best number of clusters for the data. Variables can be continuous, ordinal, or nominal. 

Clusters are created using a *bottom-up clustering* method. Groups are built from individual cases and combined into clusters until the full data set forms a single cluster. The similarity and dissimilarity is represented in a dendogram, or a tree representation of the cluster(s). 

Hierarchical cluster and k-means cluster.

```{r}
## -----------------------------------------------------------------------------
#iris  <- datasets::iris
#iris2 <- iris[,-5]
#species_labels <- iris[,5]
#iris2$labels <- iris[,5]
#iris2$color <- rev(rainbow_hcl(3))[as.numeric(species_labels)]
```

## *Working with Iris Data*
### *Visualize using a Scatterplot Matrix (SPLOM) or Pairs Plot*

```{r, fig.width = 9, fig.height = 9, fig.show = 'hold'}
GGally::ggpairs(
  data = IRIS, 
  mapping = aes(color = Species)
) 
```

Notice how the *Setosa* species tend to have smaller petal length and width compared with both *Versicolor* and *Virginica* iris species. By contrast, differentiating *Versicolor* from *Virginica* cannot easily be done based on these variable measurements. 

The same conclusion can be made by looking at the parallel coordinates plot of the data:

Plots will use `rownames()` when rendering the rows. If you want to be able to identify rows in the plots, change the `rownames()`. Remember that `rownames` have to be unique so you cannot set them to the species names. For all rows to be unique, we can paste together the row numver and the species name but clean up the name to an abbreviation.  

```{r}
rownames(IRIS) <- rownames(IRIS) %>% 
  paste0(IRIS$Species, "-", .) %>%
  gsub("setosa", "S", .) %>%
  gsub("versicolor", "VC", .) %>%
  gsub("virginica", "VG", .)
```

### *Computing Distance*

The `base R` `stats::dist` can be used for calculating a distance matrix for distances between rows of data. The method argument needs to be either `"euclidean"`, `"maximum"`, `"manhattan"`, `"canberra"`, `"binary"` or `"minkowski"`. Each distance metric can be reviewed by looking at the `?stats` help documentation. By default is the `"euclidean"` distance.

```{r}
dist_iris <- IRIS %>%
  dplyr::select(., where(is.numeric)) %>%
  dist(.) # method="man" # is a bit better

#dist_iris
```


### *Creating a Hierarchical Cluster*

Using the computed distances, we can create a hierarchical cluster of the data. Such clustering is based on the dissimilarities 
method

`stats::hclust()` will need to pass the dissimilarity matrix structure returned from `dist()` to the `d` parameter.
In order to compute the structure, we need to specify the  agglomeration method to use, which would be either `"ward.D"`, `"ward.D2"`, `"single"`, `"complete"`, `"average"` (= UPGMA), `"mcquitty"` (= WPGMA), `"median"` (= WPGMC) or `"centroid"` (= UPGMC). More details can be reviewed in the help documentation `?hclust`. We will use `method = "complete"`. 

```{r}
hclust_iris <- hclust(d = dist_iris, 
                      method = "ward.D"  #"complete"
                      )


summary(hclust_iris)
```

### **Visualizing the Distances**

In order to understand the distance relationships between rows, we can use `plot()` to view the tree structure of `hclust_iris`. The tree will group cases that are most similar such that the distance between branches is shortest for similar cases and farthest for dissimilar distances. 

```{r}
plot(hclust_iris)
```

Perhaps a better alternative is to convert the dissimilarity structure to a dendogram using `as.dendogram()`. The `dendextend` library will allow for modifying the dendogram for visualization.

???????????????????????? # order it the closest we can to the order of the observations:
dend <- ''
rotate(dend_iris, 1:150)

Taking all the steps together:

```{r}
hclust_iris <- IRIS %>%
  dplyr::select(., where(is.numeric)) %>%
  dist(., method = "euclidean") %>%
  hclust(., method = "complete") 

dend_iris <- hclust_iris %>%
  as.dendrogram(.)

hclust_iris; summary(hclust_iris)

dend_iris; summary(dend_iris)
```

In order to color the branches, use `dendextend::color_branches()` to color the terminal leaves the dendogram luster as well as the edges leading to those leaves. In 

??The edgePar attribute of nodes will be augmented by a new list item col. ??

Passing a numeric value to `k` will specify how many groups or cluster to visualize. This invokes `cutree` which cuts a tree or hierarchical cluster into data groups. Similarly, specifyin a numeric value for `h` will cut the height of the tree. 

```{r}
groups <- cutree(hclust_iris, k = 3)

# in order to see the clusters easily, use a rectangle
rect.hclust(hclust_iris, k = 3, border = "red")

# Color the branches based on the clusters:
plot(color_branches(dend = dend_iris, k = 2), 
     main = "Clustered Iris Dataset"
     )


plot(color_branches(dend = dend_iris, k = 2), 
     main = "Clustered Iris Dataset"
     )
```


The default view is vertical. To view a horizontal structure, set `hori = TRUE`. Also, if we wish to change the number of clusters to color, assign the returned dendogram also with changes to `k` or other argument.

```{r}
plot(dend_iris <- color_branches(dend = dend_iris, k = 4), 
     main = "Clustered Iris Dataset",
     horiz = TRUE
     )
```


```{r}
groups <- c("A", "B", "C")

plot(dend_iris <- color_branches(
  dend = dend_iris, 
  k = 3,
  groupLabels = groups
  ), 
     main = "Clustered Iris Dataset",
     horiz = TRUE #   nodePar = list(cex = .007)
  )
```

We will need to reverse the labels though as the cases at the top (or right side of `horiz = FALSE`) are `"setosa"`. 

```{r}
groups <- rev(levels(iris$Species))

plot(dend_iris <- color_branches(
  dend = dend_iris, 
  k = 3,
  groupLabels = groups
  ), 
     main = "Clustered Iris Dataset",
     horiz = TRUE #nodePar = list(cex = .007)
  )


```


To hang the labels, use `hang.dendogram()`.

```{r}
plot(color_branches(
  hang.dendrogram(dend_iris), 
  k = 3,
  groupLabels = groups
  ), 
     main = "Clustered Iris Dataset",
     horiz = F #nodePar = list(cex = .007)
  )
```


## *Mental Rotation Data*

First, make sure you scale the data.

```{r}

```


Note: If you want to change `rownames()`, you cannot for a tibble, so first convert to data frame. Then assign the `id` to the rownames of the data frame.

```{r}
MR_wide <- MR_wide %>% as.data.frame(.)

rownames(MR_wide) <- MR_wide$id
```


Let's look at a pairs plot for the `Same` and `Mirror` trials separately. Select only the scaled columns because the unstandardized columns will be the same. Keep in mind that some individuals responded in ways not expected for the `Same` trials so theses differences will be observed in the data as well.

```{r}
MR_wide %>%
  dplyr::select(., contains("Same")) %>%
  dplyr::select(., ends_with("_z")) %>%
  GGally::ggpairs(data = .) 

MR_wide %>%
  dplyr::select(., contains("Mirror")) %>%
  dplyr::select(., ends_with("_z")) %>%
  GGally::ggpairs(data = .) 
```

### *Creating the Dendogram*

Compute distance, the hierarchical cluster, and make a dendogram.

```{r}
dend_MR_same <- MR_wide %>%
  filter(., id < 9999) %>%
  dplyr::select(., contains("Same")) %>%
  dplyr::select(., ends_with("_z")) %>%
  dist(., method = "euclidean") %>%
  hclust(., method = "complete") %>%
  as.dendrogram(.)

dend_MR_mirror <- MR_wide %>%
  filter(., id < 9999) %>%
  dplyr::select(., contains("Mirror")) %>%
  dplyr::select(., ends_with("_z")) %>%
  dist(., method = "euclidean") %>%
  hclust(., method = "complete") %>%
  as.dendrogram(.)

dend_MR_same; summary(dend_MR_same)

dend_MR_mirror; summary(dend_MR_mirror)
```

### **Visualizing**

the Distances
and the dendogram

```{r}
plot(color_branches(dend = dend_MR_same), 
     main = "Clustered Mental Rotation Dataset (Same Trials)",
     horiz = TRUE)

plot(color_branches(dend = dend_MR_mirror), 
     main = "Clustered Mental Rotation Dataset (Mirror Trials)",
     horiz = TRUE)
```




```{r}
iris_species <- ''
rev(levels(iris[,5]))

?dist
d_iris  <-  


library(dendextend)
dend <- as.dendrogram(hc_iris)
# order it the closest we can to the order of the observations:
dend <- rotate(dend, 1:150)

# Color the branches based on the clusters:
dend <- color_branches(dend, k=3) #, groupLabels=iris_species)

# Manually match the labels, as much as possible, to the real classification of the flowers:
labels_colors(dend) <-
   rainbow_hcl(3)[sort_levels_values(
      as.numeric(iris[,5])[order.dendrogram(dend)]
   )]

# We shall add the flower type to the labels:
labels(dend) <- paste(as.character(iris[,5])[order.dendrogram(dend)],
                           "(",labels(dend),")", 
                           sep = "")
# We hang the dendrogram a bit:
dend <- hang.dendrogram(dend,hang_height=0.1)
# reduce the size of the labels:
# dend <- assign_values_to_leaves_nodePar(dend, 0.5, "lab.cex")
dend <- set(dend, "labels_cex", 0.5)
# And plot:
par(mar = c(3,3,3,7))
plot(dend, 
     main = "Clustered Iris data set
     (the labels give the true flower species)", 
     horiz =  TRUE,  nodePar = list(cex = .007))
legend("topleft", legend = iris_species, fill = rainbow_hcl(3))

#### BTW, notice that:
# labels(hc_iris) # no labels, because "iris" has no row names
# is.integer(labels(dend)) # this could cause problems...
# is.character(labels(dend)) # labels are no longer "integer"

## ---- fig.width=7, fig.height=7-----------------------------------------------
# Requires that the circlize package will be installed
par(mar = rep(0,4))
circlize_dendrogram(dend)

## ---- echo=FALSE, eval=FALSE--------------------------------------------------
#  # some_col_func <- function(n, top_color = "red4") {
#  #   seq_cols <- c("#F7FCFD", "#E0ECF4", "#BFD3E6", "#9EBCDA", "#8C96C6", "#8C6BB1",
#  #                 "#88419D", "#810F7C")
#  #   c(colorRampPalette(seq_cols, bias =1)(n-1), top_color)
#  # }
#  

## ---- fig.width=9, fig.height=9-----------------------------------------------

some_col_func <- function(n) rev(colorspace::heat_hcl(n, c = c(80, 30), l = c(30, 90), power = c(1/5, 1.5)))

# scaled_iris2 <- iris2 %>% as.matrix %>% scale
# library(gplots)
gplots::heatmap.2(as.matrix(iris2), 
          main = "Heatmap for the Iris data set",
          srtCol = 20,
          dendrogram = "row",
          Rowv = dend,
          Colv = "NA", # this to make sure the columns are not ordered
          trace="none",          
          margins =c(5,0.1),      
          key.xlab = "Cm",
          denscol = "grey",
          density.info = "density",
          RowSideColors = rev(labels_colors(dend)), # to add nice colored strips		
          col = some_col_func
         )



## ---- cache = FALSE, eval = FALSE---------------------------------------------
#  heatmaply::heatmaply(as.matrix(iris2),
#            dendrogram = "row",
#            Rowv = dend)

## -----------------------------------------------------------------------------

hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", 
        "median", "centroid", "ward.D2")
iris_dendlist <- dendlist()
for(i in seq_along(hclust_methods)) {
   hc_iris <- hclust(d_iris, method = hclust_methods[i])   
   iris_dendlist <- dendlist(iris_dendlist, as.dendrogram(hc_iris))
}
names(iris_dendlist) <- hclust_methods
iris_dendlist

## ---- fig.width=8, fig.height=8-----------------------------------------------
iris_dendlist_cor <- cor.dendlist(iris_dendlist)
iris_dendlist_cor
corrplot::corrplot(iris_dendlist_cor, "pie", "lower")

## ---- fig.width=8, fig.height=8-----------------------------------------------
iris_dendlist_cor_spearman <- cor.dendlist(iris_dendlist, method_coef = "spearman")
corrplot::corrplot(iris_dendlist_cor_spearman, "pie", "lower")

## ---- fig.height=5------------------------------------------------------------
# The `which` parameter allows us to pick the elements in the list to compare
iris_dendlist %>% dendlist(which = c(1,8)) %>% ladderize %>% 
   set("branches_k_color", k=3) %>% 
   # untangle(method = "step1side", k_seq = 3:20) %>%
   # set("clear_branches") %>% #otherwise the single lines are not black, since they retain the previous color from the branches_k_color.
   tanglegram(faster = TRUE) # (common_subtrees_color_branches = TRUE)

## ---- fig.height=5------------------------------------------------------------
# The `which` parameter allows us to pick the elements in the list to compare
iris_dendlist %>% dendlist(which = c(1,4)) %>% ladderize %>% 
   set("branches_k_color", k=2) %>% 
   # untangle(method = "step1side", k_seq = 3:20) %>%
   tanglegram(faster = TRUE) # (common_subtrees_color_branches = TRUE)

## ---- fig.height=5------------------------------------------------------------
# The `which` parameter allows us to pick the elements in the list to compare
iris_dendlist %>% dendlist(which = c(1,4)) %>% ladderize %>% 
   # untangle(method = "step1side", k_seq = 3:20) %>%
   set("rank_branches") %>%
   tanglegram(common_subtrees_color_branches = TRUE)

## -----------------------------------------------------------------------------
length(unique(common_subtrees_clusters(iris_dendlist[[1]], iris_dendlist[[4]]))[-1])
# -1 at the end is because we are ignoring the "0" subtree, which indicates leaves that are singletons.

## ---- fig.height=5------------------------------------------------------------
iris_dendlist %>% dendlist(which = c(3,4)) %>% ladderize %>% 
   untangle(method = "step1side", k_seq = 2:6) %>%
   set("branches_k_color", k=2) %>% 
   tanglegram(faster = TRUE) # (common_subtrees_color_branches = TRUE)

## ---- fig.height=15-----------------------------------------------------------
par(mfrow = c(4,2))
for(i in 1:8) {
   iris_dendlist[[i]] %>% set("branches_k_color", k=2) %>% plot(axes = FALSE, horiz = TRUE)
   title(names(iris_dendlist)[i])
}

## -----------------------------------------------------------------------------
iris_dendlist_cor2 <- cor.dendlist(iris_dendlist, method = "common")
iris_dendlist_cor2

## ---- fig.width=5, fig.height=5-----------------------------------------------
# corrplot::corrplot(iris_dendlist_cor2, "pie", "lower")

## -----------------------------------------------------------------------------

get_ordered_3_clusters <- function(dend) {
   cutree(dend, k = 3)[order.dendrogram(dend)]
}

dend_3_clusters <- lapply(iris_dendlist, get_ordered_3_clusters)

compare_clusters_to_iris <- function(clus) {FM_index(clus, rep(1:3, each = 50), assume_sorted_vectors = TRUE)}

clusters_performance <- sapply(dend_3_clusters, compare_clusters_to_iris)
dotchart(sort(clusters_performance), xlim = c(0.7,1),
         xlab = "Fowlkes-Mallows Index (from 0 to 1)",
         main = "Perormance of clustering algorithms \n in detecting the 3 species",
         pch = 19)

## -----------------------------------------------------------------------------
train <- dendextend::khan$train
test <- dendextend::khan$test

## -----------------------------------------------------------------------------
d_train <- train %>% dist %>% hclust %>% as.dendrogram
d_test <- test %>% dist %>% hclust %>% as.dendrogram
d_train_test <- dendlist(train = d_train, test = d_test)

## -----------------------------------------------------------------------------
d_train_test %>% cor.dendlist

## -----------------------------------------------------------------------------
d_train_test %>% cor.dendlist(method_coef = "spearman")

## -----------------------------------------------------------------------------
Bk_plot(d_train, d_test, k = 2:30, xlim = c(2,30))

## ---- fig.width=8, fig.height=5-----------------------------------------------
pre_tang_d_train_test <- d_train_test %>% ladderize %>% # untangle %>%
   set("branches_k_color", k = 7)
train_branches_colors <- get_leaves_branches_col(pre_tang_d_train_test$train)
pre_tang_d_train_test %>% tanglegram(fast = TRUE, color_lines = train_branches_colors)

## ---- echo = FALSE------------------------------------------------------------
# dput(d_train_test_common)
d_train_test_common <- structure(list(train = structure(list(structure(list(structure(171L, label = "491565", members = 1L, height = 0, leaf = TRUE), 
    structure(178L, label = "505491", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 7.1369942952198), 
    structure(list(structure(list(structure(8L, label = "283315", members = 1L, height = 0, leaf = TRUE), 
        structure(9L, label = "897177", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 2.55936539399907), 
        structure(list(structure(list(structure(106L, label = "345553", members = 1L, height = 0, leaf = TRUE), 
            structure(112L, label = "307660", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 5.17910461856101), 
            structure(list(structure(list(structure(268L, label = "504791", members = 1L, height = 0, leaf = TRUE), 
                structure(306L, label = "782503", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 4.27052507661529), 
                structure(list(structure(list(structure(246L, label = "81518", members = 1L, height = 0, leaf = TRUE), 
                  structure(290L, label = "280837", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 1.37572388944875), 
                  structure(list(structure(list(structure(266L, label = "866694", members = 1L, height = 0, leaf = TRUE), 
                    structure(277L, label = "811956", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 3.31301518861595), 
                    structure(list(structure(273L, label = "842918", members = 1L, height = 0, leaf = TRUE), 
                      structure(274L, label = "626555", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 2.71864544948399)), members = 4, midpoint = 1.5, height = 6.35097701381449)), members = 6, midpoint = 2, height = 8.7097033164167)), members = 8, midpoint = 2.25, height = 9.23807936424017)), members = 10, midpoint = 2.375, height = 11.6573350998416)), members = 12, midpoint = 2.4375, height = 17.5620766260713)), members = 14, midpoint = 2.46875, height = 30.2363452779928, class = "dendrogram"), 
    test = structure(list(structure(list(structure(list(structure(171L, label = "491565", members = 1L, height = 0, leaf = TRUE), 
        structure(178L, label = "505491", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 3.96666017450449), 
        structure(list(structure(list(structure(list(structure(268L, label = "504791", members = 1L, height = 0, leaf = TRUE), 
            structure(306L, label = "782503", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 2.31497882927685), 
            structure(list(structure(list(structure(266L, label = "866694", members = 1L, height = 0, leaf = TRUE), 
                structure(277L, label = "811956", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 1.75475236429532), 
                structure(list(structure(273L, label = "842918", members = 1L, height = 0, leaf = TRUE), 
                  structure(274L, label = "626555", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 1.34617375921535)), members = 4, midpoint = 1.5, height = 2.76465021476497)), members = 6, midpoint = 2, height = 4.52927251774499), 
            structure(list(structure(list(structure(246L, label = "81518", members = 1L, height = 0, leaf = TRUE), 
                structure(290L, label = "280837", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 0.714433271901582), 
                structure(list(structure(8L, label = "283315", members = 1L, height = 0, leaf = TRUE), 
                  structure(9L, label = "897177", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 1.71895552589356)), members = 4, midpoint = 1.5, height = 6.44143803354499)), members = 10, midpoint = 4.75, height = 7.736516720075)), members = 12, midpoint = 3.625, height = 11.0066972375913), 
        structure(list(structure(106L, label = "345553", members = 1L, height = 0, leaf = TRUE), 
            structure(112L, label = "307660", members = 1L, height = 0, leaf = TRUE)), members = 2L, midpoint = 0.5, height = 3.6486307417989)), members = 14, midpoint = 8.0625, height = 18.2331742971431, class = "dendrogram")), class = "dendlist", .Names = c("train", 
"test"))

## -----------------------------------------------------------------------------
# This was calculated before
# d_train_test_common <- d_train_test %>% prune_common_subtrees.dendlist
# d_train_test_common
d_train_test_common %>% untangle %>%  tanglegram(common_subtrees_color_branches = TRUE)

## -----------------------------------------------------------------------------
d_train_test %>% nleaves
d_train_test_common %>% nleaves

## -----------------------------------------------------------------------------
votes.repub <- cluster::votes.repub

## ---- fig.height=5------------------------------------------------------------
years <- as.numeric(gsub("X", "", colnames(votes.repub)))

par(las = 2, mar = c(4.5, 3, 3, 2) + 0.1, cex = .8)
# MASS::parcoord(votes.repub, var.label = FALSE, lwd = 1)
matplot(1L:ncol(votes.repub), t(votes.repub), type = "l", col = 1, lty = 1,
        axes = F, xlab = "", ylab = "")
axis(1, at = seq_along(years), labels = years)
axis(2)
# Add Title
title("Votes for Republican Candidate\n in Presidential Elections \n (each line is a country - over the years)")

## ---- fig.width=9, fig.height=9-----------------------------------------------
arcsin_transformation <- function(x) asin(x/100)

dend_NA <- votes.repub %>% is.na %>%
   dist %>% hclust %>% as.dendrogram %>% ladderize

dend <- votes.repub %>% arcsin_transformation %>%
   dist %>% hclust(method = "com") %>% as.dendrogram %>%
   rotate(labels(dend_NA)) %>%
   color_branches(k=3)

# some_col_func <- function(n) rev(colorspace::heat_hcl(n, c = c(80, 30), l = c(30, 90), power = c(1/5, 1.5)))
some_col_func <- colorspace::diverge_hcl


# par(mar = c(3,3,3,3))
# library(gplots)
gplots::heatmap.2(as.matrix(votes.repub), 
          main = "Votes for\n Republican Presidential Candidate\n (clustered using complete)",
          srtCol = 60,
          dendrogram = "row",
          Rowv = dend,
          Colv = "NA", # this to make sure the columns are not ordered
          trace="none",          
          margins =c(3,6),      
          key.xlab = "% Votes for Republican\n Presidential Candidate",
          labCol = years,
          denscol = "grey",
          density.info = "density",
          col = some_col_func
         )
          # RowSideColors = rev(labels_colors(dend)), # to add nice colored strips		

## -----------------------------------------------------------------------------

hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", 
        "median", "centroid", "ward.D2")
votes.repub_dendlist <- dendlist()

for(i in seq_along(hclust_methods)) {
   tmp_dend <- votes.repub %>% arcsin_transformation %>% dist %>% hclust(method = hclust_methods[i]) %>% as.dendrogram 
   votes.repub_dendlist <- dendlist(votes.repub_dendlist, tmp_dend)
}
names(votes.repub_dendlist) <- hclust_methods
# votes.repub_dendlist

## ---- fig.width=8, fig.height=8-----------------------------------------------
corrplot::corrplot(cor.dendlist(votes.repub_dendlist), "pie", "lower")

## ---- echo=FALSE, fig.width=9, fig.height=9-----------------------------------
arcsin_transformation <- function(x) asin(x/100)

dend_NA <- votes.repub %>% is.na %>%
   dist %>% hclust %>% as.dendrogram %>% ladderize

dend <- votes.repub %>% arcsin_transformation %>%
   dist %>% hclust(method = "ave") %>% as.dendrogram %>%
   rotate(labels(dend_NA)) %>%
   color_branches(k=3)

# some_col_func <- function(n) rev(colorspace::heat_hcl(n, c = c(80, 30), l = c(30, 90), power = c(1/5, 1.5)))
some_col_func <- colorspace::diverge_hcl


# par(mar = c(3,3,3,3))
# library(gplots)
gplots::heatmap.2(as.matrix(votes.repub), 
          main = "Votes for\n Republican Presidential Candidate\n (clustered using average)",
          srtCol = 60,
          dendrogram = "row",
          Rowv = dend,
          Colv = "NA", # this to make sure the columns are not ordered
          trace="none",          
          margins =c(3,6),      
          key.xlab = "% Votes for Republican\n Presidential Candidate",
          labCol = years,
          denscol = "grey",
          density.info = "density",
          col = some_col_func
         )
          # RowSideColors = rev(labels_colors(dend)), # to add nice colored strips		

## ---- echo=FALSE--------------------------------------------------------------

ord1 <- c("North Carolina", "Virginia", "Tennessee", "Kentucky", "Maryland", 
"Delaware", "Oklahoma", "Missouri", "New Mexico", "Oregon", "Washington", 
"California", "West Virginia", "Hawaii", "Nevada", "Arizona", 
"Montana", "Idaho", "Wyoming", "Utah", "Colorado", "Alaska", 
"Illinois", "New York", "Indiana", "Ohio", "Connecticut", "New Hampshire", 
"New Jersey", "Pennsylvania", "Iowa", "South Dakota", "North Dakota", 
"Wisconsin", "Minnesota", "Nebraska", "Kansas", "Maine", "Michigan", 
"Massachusetts", "Rhode Island", "Vermont", "Alabama", "Georgia", 
"Louisiana", "Arkansas", "Florida", "Texas", "South Carolina", 
"Mississippi")

ord2 <- c("North Carolina", "Virginia", "Tennessee", "Oklahoma", "Kentucky", 
"Maryland", "Delaware", "Missouri", "New Mexico", "West Virginia", 
"Oregon", "Washington", "California", "Nevada", "Arizona", "Montana", 
"Colorado", "Alaska", "Idaho", "Wyoming", "Utah", "Hawaii", "Maine", 
"Illinois", "New York", "New Jersey", "Indiana", "Ohio", "Connecticut", 
"New Hampshire", "Pennsylvania", "Michigan", "Iowa", "South Dakota", 
"North Dakota", "Wisconsin", "Minnesota", "Massachusetts", "Rhode Island", 
"Nebraska", "Kansas", "Vermont", "Alabama", "Georgia", "Louisiana", 
"Arkansas", "Florida", "Texas", "South Carolina", "Mississippi"
)

# dput(lapply(dends, labels)[[2]])


## -----------------------------------------------------------------------------
dend_com <- votes.repub %>% arcsin_transformation %>%
   dist %>% hclust(method = "com") %>% as.dendrogram %>%
   rotate(labels(dend_NA)) %>%
   color_branches(k=3) # %>% ladderize
dend_ave <- votes.repub %>% arcsin_transformation %>%
   dist %>% hclust(method = "ave") %>% as.dendrogram %>%
   rotate(labels(dend_NA)) %>%
   color_branches(k=3) # %>% ladderize

# The orders were predefined after using untangle("step2side")
# They are omitted here to save running time.
dend_com <- rotate(dend_com, ord1)
dend_ave <- rotate(dend_ave, ord2)

dends <- dendlist(complete = dend_com, average = dend_ave) # %>% untangle("step2side")
dends  %>% tanglegram(margin_inner = 7)


## -----------------------------------------------------------------------------
animals <- cluster::animals

colnames(animals) <- c("warm-blooded", 
                       "can fly",
                       "vertebrate",
                       "endangered",
                       "live in groups",
                       "have hair")

## ---- fig.width=9, fig.height=9-----------------------------------------------

dend_r <- animals %>% dist(method = "man") %>% hclust(method = "ward.D") %>% as.dendrogram %>% ladderize %>%
    color_branches(k=4)

dend_c <- t(animals) %>% dist(method = "man") %>% hclust(method = "com") %>% as.dendrogram %>% ladderize%>%
    color_branches(k=3)


# some_col_func <- function(n) rev(colorspace::heat_hcl(n, c = c(80, 30), l = c(30, 90), power = c(1/5, 1.5)))
# some_col_func <- colorspace::diverge_hcl
# some_col_func <- colorspace::sequential_hcl
some_col_func <- function(n) (colorspace::diverge_hcl(n, h = c(246, 40), c = 96, l = c(65, 90)))



# par(mar = c(3,3,3,3))
# library(gplots)
gplots::heatmap.2(as.matrix(animals-1), 
          main = "Attributes of Animals",
          srtCol = 35,
          Rowv = dend_r,
          Colv = dend_c,
          trace="row", hline = NA, tracecol = "darkgrey",         
          margins =c(6,3),      
          key.xlab = "no / yes",
          denscol = "grey",
          density.info = "density",
          col = some_col_func
         )


## -----------------------------------------------------------------------------

hclust_methods <- c("ward.D", "single", "complete", "average", "mcquitty", 
        "median", "centroid", "ward.D2")
animals_dendlist <- dendlist()

for(i in seq_along(hclust_methods)) {
   tmp_dend <-  animals %>% dist(method = "man") %>% 
      hclust(method = hclust_methods[i]) %>% as.dendrogram 
   animals_dendlist <- dendlist(animals_dendlist, tmp_dend)
}
names(animals_dendlist) <- hclust_methods
# votes.repub_dendlist

## ---- fig.width=8, fig.height=8-----------------------------------------------
cophenetic_cors <- cor.dendlist(animals_dendlist)
corrplot::corrplot(cophenetic_cors, "pie", "lower")

## -----------------------------------------------------------------------------
remove_median <- dendlist(animals_dendlist, which = c(1:8)[-6] )
FM_cors <- cor.dendlist(remove_median, method = "FM_index", k = 4)
corrplot::corrplot(FM_cors, "pie", "lower")


```

