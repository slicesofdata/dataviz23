---
title: "GCDS: Modeling Cognitive Task Data (Applying Supervised Learning Methods)"
#author: ""
#date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true # this will create a table of contents of hyperlinks (change true to omit)
    toc_depth: 3
    number_sections: yes
    code_folding: show # vs. hide    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
########################################################################
# Don't forget to run this
########################################################################
rm(list = ls(all.names = TRUE))      # remove objects in R

#source("https://pastebin.com/raw/8mXH02yg")   # run and comment out before knitting
source("https://pastebin.com/raw/97NNTTzu")   # run to include in function definitions

# set the paths for project, script, and data dirs
proj_dir <- gsub("GCDS.*.Rmd", "GCDS", get_ActivePath())
proj_name = ""
r_dir    <- paste(proj_dir, "r", sep = "/")    # r subdir
data_dir <- paste(proj_dir, "data", sep = "/") # data subdir
if ( proj_name != "" & !dir.exists(paste(proj_dir, proj_name, sep = "/")) ) {
  # create project dir
  suppressWarnings(dir.create(paste(proj_dir, proj_name, sep = "/")))
  r_dir <- gsub("/r", paste0("/", proj_name, "/r"), r_dir)
  data_dir <- gsub("/data", paste0("/", proj_name, "/data"), data_dir)
  # create sub directories
  suppressWarnings(dir.create(r_dir))
  suppressWarnings(dir.create(data_dir)) }
```

# **Loading Libraries**

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(performance) # for parameter estimates and model performance
library(lm.beta)     # for standardized betas
#library(car)         # for ANOVA models; swap before dplyr
library(caret)       # for k-fold cross validation
library(parameters)  # for model parameters
```

# **Obtain Data**

```{r message=FALSE, warning=FALSE}
#suppressMessages(source(paste(r_dir, "aggregate_STROOP.R", sep =  "/")))
#STROOP <- readr::read_csv(paste(data_dir, "STROOP.csv", sep = "/"))

STROOP_agg <- readr::read_csv(paste(data_dir, "STROOP_agg.csv", sep = "/"))

completes <- STROOP_agg %>%
  select(., id) %>%
  filter(., id < 2000) %>% 
  as.vector()
```

Those who completed are: `r sort(unique(completes$id))`. Still no data from Teams Brian and Rebecca.


# **Group Level Data Summary**

```{r message=FALSE, warning=FALSE}
# Add a factor for some modeling
STROOP_agg <- STROOP_agg %>%
  mutate(., trialtype_f = factor(trialtype))

STROOP_agg_summary <- STROOP_agg %>%
  group_by(., trialtype) %>%
  summarise(., 
            mean = mean(na.omit(rt)),
            sd = sd(na.omit(rt)),
            sem = sd(na.omit(rt)) / sqrt(length(na.omit(rt))),
            n = dplyr::n() #length(na.omit(rt))
            ) %>%
  mutate(., across(where(is.numeric), round, digits = 2)) 

#STROOP_agg_summary %>% view(filter = "none")

STROOP_agg_summary 
```

The descriptive statistics from `STROOP_agg_summary` indicate that the response time means and standard deviations (and standard errors of the mean) differ numerically across the trial types. Whether the mean response times differ enough relative to their variance is a matter or inferential statistics.  

Thought Questions:

- What is the difference in means?
- Are those differences representing unsystematic variation in response times or are those differences accounted for by trial type as a predictor?
- How many standard errors (approximately) is one mean from the other mean?

Some of these questions are relevant to determining a model fitting the data.  



# **Fitting Simple Models**

There are two models we can run for comparison purposes. One model will use only numeric values for the predictor and another will use character strings for the predictor. You will see how the model is the same but knowing this is important.

*Model 1*: Because `ingongruent` is used to numerically specify trials as incongruent, the congruent trials are coded as 0s and the incongruent trials are coded as 1s for this variable.  

*Model 2*: Using the character information of `trialtype`, incongruent and congruent trials are referenced by name rather than 0s and 1s in the previous model. 

Before running either model, let's look at a correlation matrix for these `rt` and `incongruent`. We can see that there is a positive relationship between `rt` and `incongruent`. More on this later but we know response times must be larger (slower) for incongruent compared with congruent trials. 

```{r message=FALSE, warning=FALSE}
STROOP_agg %>%
  select(., rt, incongruent) %>% cor()
```

## *Model 1*

- `lm_incongruent <- lm(rt ~ incongruent, data = STROOP_agg)`

```{r}
lm_incongruent <- lm(rt ~ incongruent, data = STROOP_agg)

summary(lm_incongruent)

model_parameters(lm_incongruent)

model_performance(lm_incongruent)
```

### *Interpreting Intercept and Slope*

You can see that the intercept reflects the response times when `incongruent = 0` (congruent trials) and the slope reflects the change in `rt` for a unit change in `incongruent`. Moving along the predictor from 0 to 1, the slope then reflects the unit change in response time from congruent trials to incongruent trials. 
Although we can interpret the parameter estimates here, we will do so after standardizing the model first because doing so will help with interpreting the parameters of a more complex model.


### *Standardized Coefficients of the Model*

```{r}
lm_incongruent_beta <- lm.beta::lm.beta(lm_incongruent)

summary(lm_incongruent_beta)

model_parameters(lm_incongruent_beta)

model_performance(lm_incongruent_beta)
```

Looking at the model parameters is the `summary()` table, notice that both raw *estimates* and *standardize estimates* are included. The `t value` represents whether the estimate differs from 0 (non-zero intercept) and is calculated as `estimate / std.error` and thus provides the *number of standard deviations the intercept (or slope) is from 0.* The corresponding p-value,`Pr(>|t|)` informs us that the intercept is significantly greater than 0 (see asterisks for flagging p values). Similarly, the t-value and p-value for the slope tells us that the slope estimate is significantly positive and non-zero. Thus, according to the model, knowing `incongruent` trial information to predict `rt` is significantly better than predicting using the mean rt when `incongruent = 0` under the mean-based model.


### *Thinking about Regression*

1. What is the mean for the congruent trials?

2. How does the mean for the congruent trials compare with the intercept coefficient?

3. How does the standardized slope compare with the correlation value obtained from the earlier matrix? 


## *Model 2*

Now using `trialtype` to predict `rt`.

- `lm_trial <- lm(rt ~ trialtype, data = STROOP_agg)`

```{r}
lm_trial <- lm(rt ~ trialtype, data = STROOP_agg)

model_parameters(lm_trial)

model_performance(lm_trial)
```

If you compare the model parameters for Model 1 and Model 2, you will see they are exactly the same. If curious, do a logical test, `model_parameters(lm_incongruent) == model_parameters(lm_trial)`. Importantly, the model is the same whether coded numerically or as characters.


### *Standardized Coefficients of the Model*

```{r message=FALSE, warning=FALSE}
lm_trial_beta <- lm.beta::lm.beta(lm_trial)

summary(lm_trial_beta)

model_parameters(lm_trial_beta)

#model_performance(lm_trial)
```

# **Making Model Predictions**

General model: `y ~ b0 + b1x + ϵ`

Where:

- `x` = predictor variable
- `y` = outcome variable
- `b0` = intercept of the regression model
- `b1` = slope of the regression model
- `ϵ or e` = error term

Example: `rt ~ intercept + triatype(x)`

rt = `r as.numeric(coef(lm_trial)[1])` + `r as.numeric(coef(lm_trial)[2])` * (CongruentRt)`

Let's review the descriptive data from before. Then using `coef()`, the model coefficients can be extracted based on their element position in the vector.

```{r message=FALSE, warning=FALSE}
STROOP_agg_summary %>%
  select(., c(trialtype, mean)) 

b0 <- as.numeric(coef(lm_trial)[1])  # the numeric for of the intercept coefficient
  
b1 <- as.numeric(coef(lm_trial)[2])  # the numeric for of the intercept coefficient
```


R is using the levels of `trialtype` in alphabetical order to determine what numeric values x should take on. Because "congruent" is earlier alphabetically, x = 0 for congruent and x = 1 for incongruent. This order makes sense in this example but if your levels don't order alphabetically in meaningful ways, you'll need to reorder them. Knowing whether you are predicting congruent or incongruent trial response times will be plugged into the model.

## *Predicting by Hand*

```{r message=FALSE, warning=FALSE}
x = 0 # congruent 

y = b0 + (b1 * x)
```

Thus, y = `r b0` + (`r b1` * 0) = `r y`.

Taking the `intercept` and adding to it nothing results in predicting `rt` for congruent trials as `r y` ms. This is what we predict for all congruent trials. Importantly, the error in the model (residuals) reflects the difference between the actual rt and the predicted rt (e.g., `actual rt - predicted rt`).


Now for incongruent trials...

```{r message=FALSE, warning=FALSE}
x = 1 # incongruent 
y = b0 + (b1 * x)
```

Thus, y = `r b0` + (`r b1 * 1` * 1) = `r y`.


Taking the `intercept` and adding to it the product of `slope * 1`, the predicted `rt` for congruent trials is `r y` ms. Notice that the intercept is the mean of congruent trials (x = 0) and the slope is the difference between the mean of the congruent and incongruent trials. This is what we predict for all congruent trials. Again, residuals reflect the difference between actual and predicted rts.


## *Predicting y with `predict()`*

Using `predict()`, you can make a prediction of outcome y for a given predictor x. You would pass the model so that `predict()` knows how to predict based on the parameters. You would also need to specify predictor values for which to predict y from. We will create a sample data frame for this. Because there are only two values of `trialtype` which are characters in Model 2, then we specify values as "congruent" and "incongruent" for predicting `rt` from `trialtype`. 

```{r message=FALSE, warning=FALSE}
sample_trial <- data.frame(
  trialtype = c("congruent", "incongruent")
)

predict(lm_trial, newdata = sample_trial)
```

Notice that the model is simply predicting the mean congruent and incongruent rts based on the trial type. When your predictor is categorical, all predicted ys will be the same within a group/level. When your predictor is numeric, all predicted ys will the same within each x value (here 0 and 1). For example, when predicting height from weight, everyone who is 130 lbs will be predicted to be the same height.


If you wanted to make predictions for Model 1, there must be some changes. First, the mode is a different name, so the model passed into `predict()` must be the `lm_incongruent` model. Second, the predictor is `incongruent` with values 0 or 1, which need to be numeric. A sample data frame is:

```{r message=FALSE, warning=FALSE}
sample_incongruent <- data.frame(
  incongruent = c(0, 1)
)

predict(lm_incongruent, newdata = sample_incongruent)
```


## *With Confidence or Prediction Intervals*

In `ggplot` plots, you might see a gray band around the regression line. This is a prediction interval, a range within which you would predict y for any given x. 

We can pass `interval = "confidence"` to create that interval.

```{r message=FALSE, warning=FALSE}
predict(lm_trial, newdata = sample_trial, interval = "confidence")
```

We can also pass `interval = "prediction"` to create an interval for prediction.

```{r}
predict(lm_trial, newdata = sample_trial, interval = "prediction")
```

For the "prediction" interval, according to the model, even though we predict congruent rts to be `r round(predict(lm_trial, newdata = sample_trial, interval = "prediction")[2,1], 2)` ms, the interval tells us that 95% of the incongruent trials have a response time between `r round(predict(lm_trial, newdata = sample_trial, interval = "prediction")[2,2],2)` and `r round(predict(lm_trial, newdata = sample_trial, interval = "prediction")[2,3],2)` ms.


# **Multiple Regression**

One problem with the `rt` data is that `rt` could be influenced by the `error` rate. A classic speed-accuracy tradeoff would be reflected in slower response times in order to keep error rate low. Plotting `error ~ rt` will reveal the relationship, if any. Not much appears going on.

```{r message=FALSE, warning=FALSE}

hist(STROOP_agg$error)

ggplot(data = STROOP_agg) + 
  geom_point(aes(rt, error)) +
  see::theme_modern()
```

We can approximate using Pearson's r but you'll notice that errors are not distributed symmetrically. 

```{r message=FALSE, warning=FALSE}
STROOP_agg %>%
  select(., c(rt, error)) %>%
  cor(.) #, method = "kendall")
```

Any relationship may be owing to outliers serving as leverage points. Let's remove them.

```{r message=FALSE, warning=FALSE}
STROOP_agg %>%
  filter(., error < .20) %>%
  select(., c(rt, error)) %>%
  cor(.) 
```

## *Holding Constant Predictor Influence*

For purposes of illustrating multiple regression, let's just predict `rt` using both `trialtype` and `error`. Specifically, the model will provide information on the influence of `trialtype` on `rt` while holding constant `error` as well as provide information about `errors` predicting `rt` while holding constant `trialtype`. 

You might be thinking that "there are different trial types though". The regression model will hold constant the variations in `trialtype` when using `error` to predict `rt `as if the was only one trialtype. And if you are thinking "error may differ across the trialypes", the model will control for differences in errors by making them mathematically the same in order to examine the influence of `trialtype` on `rt`. 

Let's take a closer look at that plot again first to see if `error` looks related to `trialtype`. Color code the points and add a linear fit by `trialtype` to get a better idea of how the data will be influencing the model. *Note:* Depending on the current data set, the visual linear fits may differ from a written description. 

```{r message=FALSE, warning=FALSE}
ggplot(data = STROOP_agg, aes(rt, error)) + 
  geom_point(aes(color = trialtype)) + 
  geom_smooth(aes(color = trialtype), method = lm, se = F) +
  see::theme_modern()
```

## *Build the Models*

*Additive Model*

```{r message=FALSE, warning=FALSE}
lm_trial_error <- STROOP_agg %>%
  lm(rt ~ trialtype + error, data = .)

model_parameters(lm_trial_error)

model_performance(lm_trial_error)
#plot(check_outliers(lm_trial_error))
```

*Interaction Model*

Technically, to specify an interaction in R, you would use `:`, for example:

- `lm(rt ~ trialtype + error + trialtype:error)`

Using `*` in the formula will expand the model to the above form. As such, you will see many examples online using `*` instead of `:`. Both are fine.

```{r}
lm_trial_error_ixn <- STROOP_agg %>%
  lm(rt ~ trialtype + error + trialtype:error, data = .)

model_parameters(lm_trial_error_ixn)

model_performance(lm_trial_error_ixn)
#plot(check_outliers(lm_trial_error_ixn))
```


## *Obtain the Standardized Coefficients*

```{r message=FALSE, warning=FALSE}
lm_trial_error_beta <- lm.beta::lm.beta(lm_trial_error)

lm_trial_error_ixn_beta <- lm.beta::lm.beta(lm_trial_error_ixn)

model_parameters(lm_trial_error_beta)

model_parameters(lm_trial_error_ixn_beta)
```


Notice how introducing `error` into the model, the slope for `trialtype` (may) change but also that the standard error (e.g., `SE`) may also change and corresponding confidence interval may cover a larger range of values, suggesting less certainty. The slope for `error` may be negative.

For the interaction model, holding constant the predictors can eliminate any influence of the other predictors on response times. If so, the interaction model should be a non contender for comparison. The AIC and BIC weights suggest the two-predictor model may be better. 

```{r message=FALSE, warning=FALSE}
compare_performance(lm_trial_beta, lm_trial_error_beta, rank = T)
```

Notice the ranking. The *adjusted R2* is slightly higher for the simple model even though R2 is lowest for that model. Although the `R2` value is higher, it's not much higher. Similarly, `RMSE` also does not differ much by adding this second predictor. Remember, that the model provides point estimates along with error measures for confidence intervals. There are more advanced ways to evaluate model accuracy that involves some training assessments.



# **Model Accuracy Using k-fold Cross-Validation (kFCV)**

Using `caret::train()`, we can train the model using k-fold cross-validation. The function will return a cross-validated RMSE for both model accuracy and for comparing models. The data will essentially be split into k different samples ("folds") and the model will be run on each sample. The k-models will be averaged and combined for comparisons.

Instead of refitting the model n different times, the model is refit k times. The first time, the first fold (the “test” fold) is withheld (hold-out sample), and the model is fit/trained with the other k−1 folds and the test fold is use to make predictions with the mean square error (MSE) using all data from that fold. The process is repeated for each remaining fold as a "test" fold. In the end, an estimate of the MSE is computed. Another approach is called *leave-one-out cross validation* (LOOCV) for which n-1 cases in the data set are dropped out and the model is run n-1 time. With kFCV, we trade off the estimate's bias (its more biased with kFCV) for less variance (which will be higher in LOOCV). With LOOCV, the model is run n-1 times rather than k times with kFCV so there is a time trade-off as well. With very large samples.

We will use k-fold cross validation for both the simple model and the more complex one. In order to replicate the outcome, let's first set a random seed so the results will be the same.

```{r message=FALSE, warning=FALSE}
set.seed(131)

lm_trial_cv <- caret::train(
  rt ~ trialtype,
  data = STROOP_agg,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
  )

lm_trial_error_cv <- caret::train(
  rt ~ trialtype + error,
  data = STROOP_agg,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
  )

lm_trial_error_ixn_cv <- caret::train(
  rt ~ trialtype + error + trialtype:error,
  data = STROOP_agg,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
  )
```

## *Extract the Model*

Using `caret::resamples()`, we can specify a `list()` object holding the models. We will assign each model a name too just to make output easier to read.

```{r message=FALSE, warning=FALSE}
mod_resamples <- caret::resamples(
  list(
    trial = lm_trial_cv,
    trial_e = lm_trial_error_cv,
    trial_e_ixn = lm_trial_error_ixn_cv
  )
)

```

## *Compare RMSE and R2*

We will now take a look at the cross-validated unadjusted `R2` and `RMSE`'s (root mean square error) for all three models. AIC and BIC weights are not included. MAE is mean absolute error.

```{r message=FALSE, warning=FALSE}
# new estimates using k-fold cross-validation
summary(mod_resamples)
```

Based on the random seed, the mean R2 is (at this writing) higher for the `trialtype` only model after sampling, perhaps able to explain a little more variance in rt. The mean RMSE is, however, slightly lower for the simple model. Which model is best may come down to weighing R2 against RMSE. 

