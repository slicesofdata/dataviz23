---
title: "Homework 10: Qualitative-Nominal-Categorical Data: Chi-Squared Tests"
author: "partner names"
date: "type date"
output: html_document
---

```{r, echo=F}
#http://yatani.jp/teaching/doku.php?id=hcistats:chisquare
#http://www.theanalysisfactor.com/effect-size/
#http://daniellakens.blogspot.com/search/label/Bayesian%20Statistics
```

##Part A##
#General Questions#

1. **QUESTION:** How many independent variables are needed for the $\chi$^2^ goodness-of-fit test?

*ANSWER:* 


2. **QUESTION:** You have a 2 x 2 design with introverts and extroverts who are either young or old and you ask 25 people from each of the treatment combinations how many close friends they have. How would you test whether there is a difference in the number of friends? Tricky: Make sure to read carefully and work through the steps to decide.

*ANSWER*


3. **QUESTION:** You have a 2 x 2 design with introverts and extroverts who are either young or old and you ask 25 people from each of the treatment combinations how many close friends they have. How would you test whether there is a relationship between the treatment combinations and whether people have more than 5 close friends? Tricky: Make sure to read carefully.

*ANSWER*


4. **QUESTION:** What is the difference between the $\chi$^2^ goodness-of-fit test and the $\chi$^2^ test for independence?

*ANSWER*


5. **QUESTION:** What assumption is there for $\chi$^2^ tests and expected frequencies? 

*ANSWER*



6. **QUESTION:** Create your own example of a situation for which you would you use a chi-squared goodness-of-fit test. Do not use an example from the Internet. Be specific about your IV(s), levels, and DV(s). 

*ANSWER:* 




##Part B##

##Before you begin##

This homework exercise involves having you answer some questions, write some code, and create a nice HTML file with your results. When asked different questions, simply either type your coded or written responses after the ANSWER message. When asked to write code to complete sections, type your code in the empty code blocks that follow the ANSWER message (between the back ticks). After adding that code, you must make sure that it will execute. So remember to read the content and run each line of your code as you write it so that you know it executes correctly. If your code does not execute, then R Markdown won't know what you are telling it to do and your HTML file will not be produced. Also, don't create your HTML file until you finish and know that all of your code works correctly.

##1. Installing and using libraries in RStudio##

1.1. Use the R Studio interface to install packages/libraries. Go to the Tools option and select Install Packages. Type the package name(s) correctly using the proper letter casing. Also, make sure that you *check the box to Install Dependencies*. Do not install with code.

- Mac users, most of you should already have XQuartz installed from the first R class, but please make sure that you have it downloaded and installed in your Applications folder. See the original download instruction file if you need help.  


1.2. Key functions used for this assignment: 

- assocstats() for obtaining effect size measures for $\chi$^2^; vcd library
- binom.test() for testing fit of a binomial distribution; built-in stats library
- chisq.test() for testing goodness-of-fit and independence; built-in stats library
- cramersV() for calculating effect size for $\chi$^2^ goodness-of-fit; lsr library
- fisher.test() for testing goodness-of-fit and independence; built-in stats library
- recode() for recoding responses into other values; car library



## 2. Loading libraries ##
If you know what libraries you will use for your code, you can load them now. Use the library() function to load the following libraries: lsr, car, vcd

*ANSWER:* 
```{r, message = FALSE, warning = FALSE}

```

Change from scientific notation if you want. 

```{r}
options(scipen = 999) #options(scipen=0)
```

##1.0. Overview of the *Chi-squared ($\chi$^2^)* test##

1.1. The Pearson Chi-squared ($\chi$^2^) test is a commonly used test for data that are nominal (categorical) in scale and are well suited for analyzing data for which medians or means do not make much sense. For example, belonging or not belonging to a category may not produce values for reporting averages. Asking 100 people whether they own a Macbook, Windows PC, or a Linux PC would tell you how many people belong to each category. You could count the number of people who use a certain computer type as a *frequency or count* who belong to each category. Using the mode as a measure of central tendency, you would know which computer type was most common. In this case, the "average" would be the mode rather than the mean or median. If 50 people in your sample have a Mac, 47 have a Windows PC, and 3 have a Linux PC, the data points are just observation counts of 50, 47, and 3. Moreover, there is also no variability to calculate because all people who have a Mac are the same -- they have a Mac. 

1.2. The $\chi$^2^) test is an approximation test rather than an exact test, which means the *p*-value associated with the test statistic will be biased with small sample sizes. In such cases, use the Fisher Exact test in order to obtain an accurate *p*-value for interpretation. The bias of the $\chi$^2^) test is inversely proportional to the sample size, so larger sample sizes produce less bias. The $\chi$^2^) test is much easier to compute by hand than Fisher's Exact test and for this reason we will discuss the concept of analyzing frequencies from the $\chi$^2^) approach, but will use fisher.test() for data analysis.  

1.3. For many research questions, you might assign people randomly to different categories and being smart by keeping the sample sizes across your groups equal. Or you may want to keep equal numbers of non-randomized variables (e.g., Gender Identity, Race, Personality type, etc.) for purposes of studying a dependent variable that might be influence differently by those IVs. For other research questions, however, belongingness to groups or categories may be the dependent variable. You might simply collect data by asking questions about how people fit into categories (e.g., Do you recycle? Do you own a Mac?, Do you own a home?, etc.). 

Chi-squared tests take on two general forms, which depend on the question asked and the number of independent variables studied. 


#2.0 The *Fisher Exact* and *Chi-squared goodness-of-fit test* or *test-of-homogeneity*#

2.1. One application of the Chi-squared test is for determining whether whether a *single* categorical variable follows a hypothesized population distribution. In other words, you are testing whether the frequencies of events obtained from data fit the expected frequencies associated with a hypothesize population. Returning to the computer data, you have 50 people have Macs, 47 have a Windows PC, and 3 have a Linux PC. The *obtained* frequencies are then compared to the *expected* frequencies for some hypothetical expected, say 1/3, 1/3, 1/3. we can test the data using the chisq.test() function.

- chisq.test(variable of obtained frequencies)

```{r}
# make a vector with the obtained frequency counts
computers <- c(50, 47, 3)

# specify the chi-square model by passin the vector to the function
computer.chi <- chisq.test(computers)

# for curiosity sake, we can check the frequencies for what is expected if the likelihoods were 1/3 each (the default)
computer.chi$expected

# and the observed (same as our vector)
computer.chi$observed

# examine the Pearson chi-square statistic to determine if the observed frequencies fit the expected frequencies, or if they appear not to 
computer.chi
```

The output reveals the $\chi$^2^ value, the *df* (number of categories - 1), and the *p*-value for the likelihood of the data occurring if the expected frequencies were a good fit for the data. What we see is that if alpha was .05, the obtained frequencies do not fit the expected frequencies; observed data are different from expected. This means the model of equal frequencies (in this example) is NOT a good fit of the data because evidence suggests otherwise.

There are various effect size measures for $\chi$^2^ tests. Calculating an effect size can be done using the cramersV() function from the lsr library by passing the $\chi$^2^ object to the function. Cramer's V is a measure of association for nominal variables (think Pearson for nominal data). It ranges from 0 to 1.0, with larger V values representing stronger relationships between the data.  

- cramersV(the chi-squared object)

```{r}
#library(lsr)
computer.v <- cramersV(computers)

# take a look at the effect size
computer.v
```

In this case, Cramer's V is about .46, meaning that the relationship between the frequencies counts and the categories. If the frequencies for the obtained data were equivalent across all levels of the IV, V = 0 because obviously the frequencies would not be related to computer type. 


2.2. By default, the chi-squared test, chisq.test(), assumes equal probabilities of events (expected values) across levels of your IV. In some cases, when you know nothing about various events, a reasonable assumption is that they could be equal. However, you might do some research on market share and find the 8% of people own Macs, 91% own Windows PCs, and 1% own Linux PCs. Based on this information, you may wish to ask if your sample representing your campus sample differs from the entire market share based on the entire population. This could be useful if you were a computer vendor; knowing your target audience is useful when selling products. In this case, you would have to change your expected frequencies to something other than equivalent frequencies (the default setting).

To change the expected probabilities, add the "p" argument to the chist.test() function; p is a vector of probabilities of the same length as your levels of IV, in this case 3 category levels. The sum of the probabilities has to sum to 1.0.

```{r}
# expected probabilities for the 3 events based on data; Mac, PC, Linux
expprob <- c(.08, .91, .01) # must sum to 1.0

# specify the chi-square model
computer.chi <- chisq.test(computers, p = expprob)  
# or if you like everything together, computer.chi <- chisq.test(computers, p = c(.08, .91, .01))

# for curiosity sake, we can check the frequencies for what is expected based on market share data; more on this below when exp < 5
computer.chi$expected

# and the observed (same as our vector)
computer.chi$observed

# examine the Pearson chi-square statistic
computer.chi
```

This comparison also reveals that they observed data do not fit what is expected from the market-share data. In fact, by comparing the observed and expected frequencies, there certainly appears to be more Mac users in our sample and fewer Windows users, but the Linux users seem to be about right.


2.3. Reporting the test statistic values

Based on the R output, you will see the variable for which you calculated, computers, the $\chi$^2^ value based on the calculation, the degrees of freedom, and the *p*-value corresponding to finding frequencies differ from expected under H0. 

When you calculate a Chi-squared test for independence and report the results of the statistic, you will need to specify the ($\chi$^2^ value and the degrees of freedom. The df for the test is the number of categories - 1 (e.g., 3 - 1 = 2).

To report the outcome of the test, the general form is:

  $\chi$^2^ (df, N = sample size) = chi-squared value, p < or > $\alpha$ 

By substitution and making sure to p, we can say:

  $\chi$^2^(2, N = 100) = 245.77, *p* < .05; Cramer's V = .46


If you used the *p*-value to make decisions about H0, you could interpret the data as evidence for the frequency counts from your samples not being the same as expected. In other words, you have evidence that you samples do not come from populations with equivalent ownership (example 1) across ownership of the computer types; your samples likely come from populations with unequal frequencies. Similarly, your campus sample does not appear to follow the market-share distribution either


2.4. There is an assumption of the $\chi$^2^ test, which was caught in the warning message. The warning that appeared when running the test indicated that the "chi-squared approximation might be incorrect". This warning will occur if the expected frequency is smaller than 5. In such cases, you may wish it increase your sample size such that the smallest expected frequency is greater than 5. 



#3.0. The *Fisher Exact test* and *Chi-squared test for independence*)#

You may be interested in determining whether belongingness to one category depends on belongingness to another category. This test is referred to as the Fisher Exact *test for independence* and is based on the joint occurrence of events. Let's use the SURVEY data for this test and ask whether the color of dress people saw differed by gender. 

3.1. Prepping data for the test.
```{r}
# set working dir if not set
setwd("c:/users/gcook/desktop/Psyc109")

# read in data frame. 
SURVEY <- read.csv("SurveyNames.csv")

# recode values of a variable to NA; in this case make 3 = NA but leave 1 and 2 the same (the 3 was a confused respondent). Recode is for the car library
DressRecode <- recode(SURVEY$Dress, "1 = 1; 2 = 2; 3 = NA")

# create separate vectors from the vectors in the SURVEY data frame (there are other ways of doing this, but this approach is easy)
Gender <- SURVEY$Gender 

# then add those vectors to a new data frame with a meaningful name using data.frame()
DRESSG <- data.frame(DressRecode, Gender)

# make a smaller and more manageable data frame to remove anyone with NA 
DRESSG <- subset(DRESSG, 
                    complete.cases(DRESSG), 
                    select = c(DressRecode, Gender))

# look at the joint occurrence of events in a 2 x 2 table/contingency table 
table(DRESSG)
```

Notice that 7 out of 24 men (7 + 17) and 17 out of 29 (17 + 12) women saw the white dress. Let's clean this up a little bit to make the labels easier to read. 

3.2. Making the data more readable if that matters to you.

```{r}
# in order to help understand what the categories are, make factors and label the levels
# factorize the IVs and add to the new data frame
DRESSG$Dress.fact <- factor(DRESSG$DressRecode, 
                              levels = c(1, 2), labels = c("White/Gold", "Blue/Black"))

# create Gender factor and add to the data frame, specify levels and labels
DRESSG$Gender.fact <- factor(DRESSG$Gender, 
                               levels = c(0, 1), labels = c("Male", "Female"))

# make the data set contain only the vector variables relevant for the chi-square test.
DRESSG <- subset(DRESSG, 
                    complete.cases(DRESSG), 
                    select = c(Dress.fact, Gender.fact))

# Or keep all rows, minus columns 1 and 2 of the data frame so that you only have variables relevant for the chi-square test.
# DRESSG <- DRESSG[, c(-1,-2)]

# does it look right?
head(DRESSG)

# does the table make more sense to read?
table(DRESSG)

# assign the table to an object for the 2 x 2 table
DRESSG.2x2 <- table(DRESSG)
```

3.3. Specifying the model. 

Ok, now the goal of the Fisher Exact test of independence is to determine if the color of the dress is associated/related to one's sex. A significant outcome would indicate that the variables are not independent of one another, but rather are dependent; what you see would depend on your sex. Using the fisher.test() function from the built-in stats library., simply pass the table object to the function.

- fisher.test(contingency table)
- chisq.test(contingency table)

```{r}
# pass the table to fisher.test()
DRESSG.2x2.fish <- fisher.test(DRESSG.2x2)

# examine the model
DRESSG.2x2.fish
```

3.4. Interpreting the output.

We can see that the data fall slightly short of evidence for a relationship between sex and dress color when tested against an alpha = .05. Fisher Exact test or $\chi$^2^ test can able be used used for contingency tables that are larger than this 2 x 2.

The $\chi$^2^ test can also be used if you have large sample sizes by simply using the chisq.test() function as before, but by passing the contingency table to the function.

```{r}
# pass the table to the function in order to create the object holding the chi-squared data
DRESSG.2x2.chi <- chisq.test(DRESSG.2x2)

# inspect
DRESSG.2x2.chi
```

The Fisher Exact test or $\chi$^2^ test can able be used used for contingency tables that are larger than this 2 x 2.

WARNING: If you do use chisq.test() a warning message like "Chi-squared approximation may be incorrect" will appear if you have small cell values in the contingency table. In this case, you can use a Fisher exact test. The Yates' correction is simply a correction for the *p*-value not being exact for the $\chi$^2^ test.

3.5. Effect sizes.

When obtaining effect sizes for contingency tables (when you have more than one factor), there are different effect sizes you can use. Some people recommend the $\phi$ coefficient over Cramer's V. You can obtain that using the assocstats() function from the vcd library.

```{r}
# get measures of effect size
# library(vcd)
assocstats(DRESS.2x2)
```

3.6. Changing the expected frequencies.

Just like with the $\chi$^2^ goodness-of-fit test, you can change the expected frequencies for the Fisher Exact test and $$\chi
^2^ test for independence.


3.7. If you didn't have the data in a data frame, but you did have the frequencies, you could create a data frame using the frequencies to create rows and columns containing the frequencies.

```{r}
whitegoldmf <- c(7, 17)
blueblackmf <- c(17, 12)

# put into a data frame
DF <- data.frame(whitegoldmf, blueblackmf) 

# convert the data frame into a matrix 
DF <- data.matrix(DF, rownames.force = NA) 
DF

# then pass the DF to the function, 
chisq.test(DF)
```
##4.0. Assumptions for *$\chi$ squared* tests## 

4.1. Assumptions for the $\chi$-squared goodness-of-fit test

  a) Date must be frequencies
  b) Expected frequencies cannot be smaller than 5; if you have sample sized smaller than 10, use a Fisher's exact test fisher.test()
  c) Data must be independent (if you have repeated measures, use the McNemar test or the Cochran-Mantel-Haenszel test)


##5. Do it yourself!##

1. **QUESTION:** Use the SURVEY data frame to determine if there is a relationship between beliefs about global issues and gender.

*CODED ANSWER:*
```{r}

```

*INTERPRETATION:*


##DONE!##
E-mail your knit HTML file that you completed with your partner. Make sure to include your names.


##COMMENTS##