---
title: "Homework 05: Regression (Bivariate and Multiple) Homework"
author: "replace with partner names"
date: "replace with date"
output: 
 html_document:
   toc: true # this will create a table of contents of hyperlinks (change to false to omit)
   toc_depth: 2
---


#Part A
##General Questions

1. **QUESTION:** Describe the purpose of using bivariate-linear regression. Include in your answer the type of data needed for linear regression analyses.


*ANSWER:*  Bivariate regression is used to understand the linear relationship between two numeric variables (interval or ratio). The relationship allows for using one variable to predict the outcome of another variable especially when information is unknown about the criterion variable.   



2. **QUESTION:** Identify the two regression coefficient values that are needed for a simple regression analysis and then describe what they are.


*ANSWER:*  The y-intercept (b0), the value of the criterion variable (y) when the predictor variable (x) is equal to 0. the regression coefficient (b1), or slope that represents the unit change (increase or decrease) in the criterion for each unit change in the predictor. 




3. **QUESTION:** The regression model is compared to another model, a simpler one. Identify what that model is and describe what the error in that model represents.  


*ANSWER:* A mean-based model is a simple model that describes all of the error in the criterion variable. In absence of knowing information about a second (predictor) variable, the mean is the best guess or estimate of a score in a sample. The error associated with the mean-based model, in terms of SS, is referred to SStotal. 



4. **QUESTION:** If the SStotal is 2000 and the SSresidual is 1000. What is the proportion of variance in the criterion is accounted for by the predictor? 

*ANSWER:*  SSmodel = SStotal - SSresidual;  SSmodel/SStotal = .50


#Part B

##Before you begin

This homework exercise involves having you answer some questions, write some code, and create a nice HTML file with your results. When asked different questions, simply either type your coded or written responses after the ANSWER message. When asked to write code to complete sections, type your code in the empty code blocks that follow the ANSWER message (between the back ticks). After adding that code, you must make sure that it will execute. So remember to read the content and run each line of your code as you write it so that you know it executes correctly. If your code does not execute, then R Markdown won't know what you are telling it to do and your HTML file will not be produced. Also, don't create your HTML file until you finish and know that all of your code works correctly.

Besides lecture notes, some videos help with understanding R output. I recommend watching:

- [Checking Assumptions](https://www.youtube.com/watch?v=eTZ4VUZHzxw)
- [Multiple Linear Regression](https://www.youtube.com/watch?v=q1RD5ECsSB0)

## 1. Downloading the source files 

1.1. Run the code below to download some libraries needed for this assignment. By using `include=FALSE, cache=FALSE` in the `r` code header, we will make sure that any error messages do not appear in the HTML file you create.

```{r include=FALSE, cache=FALSE}
source("https://dl.dropboxusercontent.com/u/6036547/109_2016_f.txt?raw=1")
```

 
1.2. Key functions used for this assignment: 

- `car::ncvTest()` for testing assumptions
- `car::dwt()` for testing assumptions using Durbin-Watson test 
- `lattice::histogram()` for plotting nicer histograms
- `lm()` for evaluating linear models
- `par()` for changing graphing parameters
- `plot()` for examining lm regression plots
- `QuantPsyc::lm.beta()` standardize the values in order to compare them easily
- `qqnorm()` and `qqline()` for examining normality visually 
- `read.csv()` for reading a csv file into a data frame
- `round()` for rounding values
- `summary(`) for obtaining a summary of the model components
- `shapiro.test()` for testing normality
- `sqrt()` for transforming data
- `visreg()` for plotting regression models
- `with()` for selecting the data frame on which to perform other operations 

##2. Loading libraries
Use `library()` to load the following libraries: `car`, `lattice`, `QuantPsyc`, `visreg`

*ANSWER:* 
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(car)
library(lattice)
library(QuantPsyc)
library(visreg)
```

##3. Checking your working directory

Always make sure that your working directory points to Psyc109 on your desktop. 
```{r}
#getwd()
setwd("C:/users/gcook/desktop/Psyc109/")
```

#Part C
##1. General Linear Regression

1.1. Simple-linear regression is an approach for modeling the linear relationship between a criterion y and one or more predictor/explanatory variables (or independent variables). 

1.2. There are two common forms of *linear regression*:

- *Simple-linear regression* is used when the goal is to use one predictor variable in the linear regression equation to predict a criterion variable (Ex: formula: criterion ~ predictor). This will produce a linear model of your criterion as-a-function-of your predictor. 

- *Multiple-linear regression* is used to indicate there is more than one predictor in the linear regression equation (Ex: formula: criterion ~ predictor1 + predictor2 + predictor3).

Both of these forms of regression analysis have assumptions that should be met in order to interpret the data correctly. The assumptions are addressed in sections below.


##2. Simple (bivariate) Linear Regression##

2.1. Understanding Simple Bivariate Linear Regression.

For a simple-linear regression there is:

  - One predictor in the equation
  
  - An unstandardized regression coefficient that represents 
    the marginal relationship between the criterion and the 
    predictor variable
    
  - The standardized regression coefficient equals is the correlation value

2.2. Reading in the data

One of the first steps in completing a simple linear regression is to plot your data on a scatter plot. For that we are going to need to bring some data into our workspace. We will read in a data file named `Album Sales.csv` and assign it to a data frame object named `ALBUM`. Download from Sakai. Examine the structure once you are done so that you know your variables, their scale of measurement, and their format. Using `str()` will help you examing the structure of your data frame.

```{r}
ALBUM <- read.csv("Album Sales.csv") # Read in csv file and assign as data frame object
str(ALBUM) # examine the structure
```

As usual, the first thing to do with any new data set is to view the data frame. You will notice that `str()` indicates that there are 200 observations of data (rows) and 5 variables (columns). The variables/columns are:

- *Id* represents the unique Id for a rock band album 
- *Sales* represents album sales (in thousands of pounds)
- *Ads* represents the amount of money spent on the advertising budget (in thousands of pounds)
- *Airplay* represents the number of times an album is played on the radio during the week prior to release
- *Attract* represents the attractiveness of the band members [on a scale from 0 (not attractive at all to 10 (most attractive as possible)]

```{r}
# may need to comment this out when knitting your file (especially on Macs)
View(ALBUM)
```

##3. Producing a correlation matrix

3.1. As we have done before, you can use `cor()` to specify a pair of variable pairs to correlation. 

- Ex pair:  `with(dataframe, cor(x, y))`

However,  when there are multiple pairs, there is an easier way than specifying multiple pairs for each x-y pair you want to correlate. You can simply use `cor()` on the `ALBUM` data fram object in order to correlate all variables in the data frame. Because correlation is on the dataframe, you do not need to use `with()`, but using it will still be fine.

- Ex dataframe: `cor(dataframe)`

```{r}
cor(ALBUM)
```

3.2. However, the decimal points in the correlation matrix are a little busy. We can use `round()` to round the results by specifying the number of decimals for rounding the correlation values. With `R`, you can nest functions inside functions, so we will nest `cor()` inside `round()` in order to round the correlation values for all the variable pairs. Remember, `cor()` using Pearson's *r* by default.

```{r}
round(cor(ALBUM), 3)  # rounding makes it easier to look at

round(cor(ALBUM[, -1]), 3)  # because ID is meaningless for our correlations, we can remove column 1 from the ALBUM object because ID is the first column in the ALBUM data frame
```


##4. Define a simple linear model

4.1. We will specify a linear model using the `lm()` by specifying arguments of the function. The general form of the regression model is as follows:

Regression model: `Y-hat = b0 + b1*X + error`

Ex:

`mymodel <- with(dataframe, lm(criterion ~ predictor, na.action = some action)`

- *mymodel* is an object that contains information about the linear model; `summary()` will be useful for inspecting the model
- *criterion* represent the predicted/outcome variable 
- *predictor(s)* represent the variable(s) used to predict scores on the criterion
- *data* is the name of the data frame object
- *na.action* (optional) allows you to specify a complete data set if you wish to drop out anyone with missing values; this in an alternative to subsetting when using regression models (or you can use `na.omit()` on the dataframe).


4.2. You can specify the predictor and the criterion, "y as a function of x", as well as the data frame. 

```{r}
ALBUM.model <- with(ALBUM, lm(Sales ~ Ads))

# If there are missing values, we can remove them (good idea to add this)
ALBUM.model <- with(ALBUM, lm(Sales ~ Ads, na.action = na.exclude))

# or because you are familiar with na.omit() to remove ALL NAs from your data frame.
ALBUM.model <- with(na.omit(ALBUM), lm(Sales ~ Ads))
```

##5. Examining and interpreting the simple bivariate linear model

5.1. In order to evaluate the parameters of the model, you can examine the y-intercept (b0) and the regression coefficient (b1), from the model by calling the model object just created.  you can simply ask to see them by calling the model object.

```{r}
ALBUM.model  # Displays only the coefficients from the model
```

Or you can summarize other details of the regression model by using `summary()`. This provides information about significance tests for the coefficients, R-squared, and a test of the model fit.

```{r}
summary(ALBUM.model)  # provides other model elements
```

The best way to visualize a linear regression is through the use of a scatterplot and because you are using a regression line to predict a variable it is helpful to plot the regression line as well.  You can do this with `lattice::xyplot()`. However, using `visreg()` on `ALBUM.model` allows you to see linear relationship between `Sales` and `Ads` as well as the 95% confidence bands that are generated around the regression line.  Remember, that the regression model simply provides estimates of coefficients based on sample data. Samples are not populations and are therefore biased representations of populations.  Thus, the gray shaded region represents the uncertaintly in the estimate of the regression line, the 95% confidence band that the true regression line lies somewhere in that gray zone.

```{r}
visreg(ALBUM.model)
```

5.2. Interpreting Overall Model Fit

- The *Multiple R-squared* represents the proportion of variance in the criterion that is accounted for by the predictor. The square root of this is Pearson's correlation coefficient. Because there is only one predictor, this multiple R-squared is really just r-squared. Because the estimates are based on samples, they are not perfect for making inferences about the populations you really care about. The adjusted R-squared is adjusted for shrinkage, or loss of predictive power that occurs when using sample data to predict population data; the more error in the regression model (e.g., residuals), the more the adjusted R-squared will differ from the unadjusted R-squared. 


- The *F-statistic* represents the ANOVA test value (*F* value) for the ratio test of error for the model. A statistically significant result (according to NHST) indicates that the regression model explains the data better than a model based on the mean of album sales (mean-based model). In NHST, you would compare the *p*-value to alpha to decide if the regression model fits the data better than the mean-based model. This ANOVA test only tells you whether the overall model fits the data; it does not examine the components of the model (e.g., b0, b1, etc.).


5.3. Interpreting Specific Model Parameters (coefficients part of the output table)

The coefficients report is displayed in 2 rows and 4 columns of the output. 

5.3.1. The *Estimate* column displays the two estimated coefficient values y-intercept (b0) and the regression coefficient (b1). The *Std. Error* column displays the error in estimating the coefficients. This error indicates the amount of error in coefficients; small error is good and would indicate that b0 and b1 would not vary as much from sample to sample. The *t-value* column displays the value of the *t*-statistic, which simply tests whether the coefficient differs from 0 (H0: t = 0), which can be inferred also from examining the *Pr(>|t|)* column which provides the *p*-value for the *t*-test.

5.3.1.1 The first row corresponds to the y-intercept (b0). The y-intercept predicts the 
number of sales when NO money (e.g., X = 0) is spent on advertisements. A *p*-value less than 
or equal to alpha may indicate that the y-intercept differs from 0. Mathematically, b0 can be 
less than 0. 

5.3.1.2. The second row corresponds to the regression coefficient/slope (b1). This 
represents the increase or decrease (depending on whether the correlation is positive or 
negative) in the number of album sales for each unit change in money spent on advertising. 
A *p*-value equal to or less than alpha means the slope differs from 0. When there are 
multiple predictor variables, there will be slopes corresponding to each predictor and 
the criterion. 


##6. Examine Model Assumptions

6.1. Model assumptions can and should be inspected both visually and statistically. Passing the model to `plot()` will return a set of plots for inspecting the model assumptions. Other specialized functions from other libraries can also help inspect a model.

*Assumptions and things to look for:*

6.1.1. The predictor(s) must be either quantitative or categorical (2 categories); the criterion must be quantitative and continuous. The criterion should also be unbounded or unconstrained; for example, if a scale that ranges from 1 to 9 is used to measure the criterion and if responses only fall between say 3 and 7, the data are constrained (bounded). An obvious solution is to check the range for the criterion `range()`. 

```{r}
with(ALBUM, range(Sales))
```


6.1.2. Predictors should not be restricted and not have variances that are near 0. Range restriction in general is often problematic with regression (leading to attenuated correlations and reduce predictive ability). We can check variance. It's often good to determine if your data range is similar to previous research. If you have a much smaller range, you should investigate reasons why. Otherwise, your correlations and regression coefficients may not map on well with previous research.

```{r}
with(ALBUM, range(Ads))
with(ALBUM, mean(Ads, na.rm = TRUE))
with(ALBUM, var(Ads, na.rm = TRUE)) # does not look near 0
with(ALBUM, sd(Ads, na.rm = TRUE))  # does not look near 0, so we have variability
```

6.1.3. Using `plot()` on the model object that you created will produce model plots for examining the assumptions of the model. You can view each plot separately or produce a 2x2 chart that displays them together. However, grouping the plots will reduce their size for visual inspection. 

```{r}
par(mfrow = c(2,2))  # adjust the plots in order to plot the 4 graphs on one chart with 2 rows, 2 columns.
plot(ALBUM.model)
par(mfrow = c(1,1))  # return to 1 row, 1 column
```

6.1.4. *The relationship between the criterion and the predictor(s) is linear* rather than curvilinear. Plot #1 is the residual plot for which errors (y-axis)) plotted as a function of predicted/fitted values (x-axis); a straight horizontal line on this plot would indicate that the linearity assumption is met; a curvilinear model is better than a linear model, then the errors would not be distributed normally, but instead be greater in some parts of the plot than in others. 

6.1.5. *Homoscedasticity of the residuals* - the variance in the residuals, or error in prediction, should be the same across the values of the predictors. Homoscedasticity is also referred to as constant variance. Non-constant variance in the residuals is referred to as heteroscedasticity. We can examine homoscedasticity visually and statistically.

**Plot #1** can also provide information about the variance in the residuals across levels of the predictor. Based on this example, you can see that there is more variability of the residuals (y-axis) on the left side of the plot than at the right side; thus the variability in residuals does not seem to be constant. If the variance is not constant, then we may have issues with residuals not being distributed normally (see next assumption).

If we want to test for constant variance statistically, we can use`car::ncvTest()`. The ncv stands for non-constant variance. We want constant variance. The `car::ncvTest()` function provides a Chi-Square test value and a corresponding *p*-value for the test. If *p* is less than or equal to alpha, you have evidence that that you have non-constant variance, or heteroscedasticity. Unfortunately, we have heteroscedasticity. 

```{r}
#library(car) # load car if not already loaded 
car::ncvTest(ALBUM.model) # the p-value is much lower than .05
```

6.1.6. *Model Residuals (prediction errors) are random and therefore distributed normally*. If errors are random and the size of the prediction error does not depend on whether the score on the predictor is low or high, then those errors should be distributed normally. **Plot #2** will provide detail about the normality of the residuals. Plot #2 is a quantile-quantile plot used to determine normality of errors; if points fall along the identity line, the errors are normally distributed. In this example, you can see that points on the ends of the distribution do not quite fall along the identify line. We could also test this assumption by using `with()` and `sharipo.test()` on the model residuals. 

We can take a look at a histogram or because the residuals are actually stored as part of the information returned from running `lm()` and named `ALBUM.model$residuals`, we can extract those residuals from the model object. Luckily, they are named appropriately as "residuals". Remember, the model was `ALBUM.model`; the residuals do not depart from a normal distribution. 

```{r}
with(ALBUM.model, lattice::histogram(~residuals)) # take a look; looks fairly normal to the eye

with(ALBUM.model, shapiro.test(residuals)) # test for normality; p > alpha so the residuals do not depart far enough from normality to say we have violated this assumption.
```


6.1.7. *Independence of errors* - for any two observations in the data, the errors are not related. This is usually not a problem as long as your sample is selected at random and you don't allow people to participate in your study, allow them to talk to other participants, etc. in ways that affect how two people (or objects) respond. This independence assumption can be tested using the Durbin-Watson test using `car::dwt()`. The Durbin-Watson test will produce a D-W statistic value that will be large if independence is violated (and you have dependence). Compare *p* to alpha. In this example, we do not see violations of independence, which is good.

```{r}
car::dwt(ALBUM.model)
```

6.1.8. *No multicollinearity* - predictors should not be strongly correlated with each other. When you have more than one predictor, you have to examine for multicollinearity. collinearity will mask the true relationship among variables. Having multicollinearity makes the beta values (y-intercept and slope) unreliable and untrustworthy when making inferences from samples to populations. Generally speaking, there will be more error in beta values for greater amounts of multicollinearity. There are other influences too, but we won't address them here. You should just know to determine if you meet this assumption. *Because we only have one predictor in this example, we do not have to test for this assumption.* 

If you had multiple predictors, you would want to test for multicollinearity. One easy way to test this is to examine Variance Inflation Factors (VIFs) using `car::vif()`. The function returns a VIF value for each predictor. If the square root of the VIF is greater than 2, this predictor would be eliminated from your model. If you only have one predictor, `R` will give you an error. We will return to this issue in multiple regression.

An example if you have multiple predictors:

- `car::vif(mymodel)`
- `sqrt(car::vif(mymodel))`

```{r}
# variance inflation factors for the model
# car::vif(ALBUM.model) 
# sqrt(car::vif(ALBUM.model)) # the square root of them
```


##8. Multiple Linear Regression

Multiple regression focuses on using more than one predictor to predict a criterion. Because predictors need to be correlated with the criterion to be useful, examining the correlations of the variables is useful. 

```{r}
round(cor(ALBUM[, -1]), 3)  # remove column 1 of ALBUM because ID is the first column in the data frame
```

Note: For partial correlations in a multiple-regression context, check out the [ppcor library](https://cran.r-project.org/web/packages/ppcor/ppcor.pdf)


8.1. **QUESTION:** What is the correlation value between Sales and Airplay? 

*ANSWER:* r = .599 or .60


8.2. **QUESTION:** Use `with()` and `cor.test()` to test whether the correlation between Sales and Airplay is statistically different from 0; use alpha = .05. 

*ANSWER:* 

```{r}
with(ALBUM, cor.test(Sales, Airplay))
```

Sales and Airplay *r* = .599; *p* < .05 


8.3. **QUESTION:** Use `cor.test()` to examine the correlations between the 3 predictors (multicollinearity). Are any pairs of predictors correlated significantly? Use alpha = .05?

*ANSWER:*

```{r}
with(ALBUM, cor.test(Ads, Airplay))
with(ALBUM, cor.test(Ads, Attract))
with(ALBUM, cor.test(Airplay, Attract))
```

- Ads and Airplay = *r* = .101; *p* > .05
- Ads and Attract = *r* = .081; *p* > .05
- Ads and Airplay = *r* = .182; *p* < .01


You might have notice that two predictors are correlated significantly. In order to determine if a predictor is problematic to your regression model, you would want to check for multicollinearity. 



##9. Define a multiple-regression model##

9.1. There are different types of models to create, but the focus will be on simple additive models (e.g., + separates predictors). Taking a *hierarchical regression* approach, create the model by entering predictors in the order of importance or theoretical contribution. 

The general form is:

`mymodel <- lm(formula = criterion ~ predictor1 + predictor3 + predictor3, `
`              na.action = some action)`


9.2. Specify the multiple-regression model using multiple predictors. Use `lm()` to build a linear model to predict album sales from Ads, Airplay, and Attraction of band members and name the regression model `ALBUM.model2` so that you can distinguish this from the bivariate model.

```{r}
ALBUM.model2 <- with(na.omit(ALBUM), lm(Sales ~ Ads + Airplay + Attract))
```

9.3. Obtain a summary of the model using `summary()`.

```{r}
summary(ALBUM.model2) 
```

9.3.1. **QUESTION:** Use the F-statisic and the *p*-value to help determine whether the model is statistically better than a mean-based model. Be careful with reading the scientific notation. Use alpha of .05.

*ANSWER:*  *F*(3, 196) = 129.5, *p* < .05. The multiple-regression model is a better predictor than the mean based model based on no predictor(s). The regression model seems to fit the data.  


9.3.2 **QUESTION:** What proportion of the variance in Sales does the model account for? Is this greater than was accounted for by the simple model?

*ANSWER:*  *R*-squared = .6647. This amount of variance accounted for my predictors is certainly better than not having predictors. Adjusted for bias in the sample, the population adjusted R is slightly lower, but still .6595. Almost two-thirds of the variability in `Sales` is accounted for by the predictors together.  



9.3.4. **QUESTION:** Are the regression coefficients (bs, slopes) for the 3 predictors significantly different from a slope of 0? Use alpha of .05.

*ANSWER:*  Yes. b1(ads) has a large *t*-value; *p* < .05; b2(Airplay) has a large *t*-value; *p* < .05; b3(Attract) does not appear to have as large of a *t*-value; but *p* < .05. All slopes are positive, which makes sense because all predictors are correlated positively with the criterion.


9.3.5. **QUESTION:** Which predictor has the steepest standardized slope value? 

*ANSWER:*  Cannot answer yet. I don't know how to standardize betas.



9.4. Standardized beta coefficients adjust the slope estimate as a function of the standard error of the estimate. This is one way to compare the regression coefficients of different predictors. `lm.beta()` from the `QuantPsyc` library will standardize the values in order to compare them easily. 

Ex: `mymodelbetas <- QuantPsyc::lm.beta(mymodel)`  

```{r}
#library(QuantPsyc) # load if not already loaded
ALBUM.model2.betas <- QuantPsyc::lm.beta(ALBUM.model2)  # assign the standardized betas to an object 
ALBUM.model2.betas # take a look at the object to compare the betas
```

**QUESTION:** Now that the slopes have been standardized, you can examine the change in the criterion for each standard deviation change in each predictor. Which predictor has the steepest standardized slope value (biggest bang for your buck)? 

*ANSWER:*  Ads and Airplay both have larger standardized changes in Sales (criterion) for each standard deviation change in them (predictors) relative to Attraction of the band members. Ads and Airplay both are very close in standardized betas (unique variance), but Airplay seems to provide the biggest influence on Sales. However, convincing radio DJs to play crappy songs might be more challenging than advertising. Only you can determine which approach is most effective to use, given your goals and your resources. 



9.5. Use the `plot()` function to generate the diagnostic plots for examining the model. Step through the plots one at a time might be helpful to see them on a larger scale.

```{r}
par(mfrow = c(2,2)) 
plot(ALBUM.model2)
par(mfrow = c(1,1)) 
```


**QUESTION:** Based on that plot, does the model appear to be a linear model? Explain how you arrived at that decision. 

*ANSWER:*  Plot #1 shows a relatively straight line for the residuals plotted as a function of predicted values. It's not a straight as it could be, but it's certainly not an obvious curve. Perhaps there are outliers to remove if you have a good reason to remove them (e.g., leverage).


9.6. **QUESTION:**  Use the appropriate function to test for the homoscedasticity/constant variance assumption. Is the assumption met? Why or why not? Use alpha of .05.

```{r}
car::ncvTest(ALBUM.model2)
```

*ANSWER:*  The test for non-constant variance has a small Chi-square value with a corresponding *p*-value of .58 ; *p* > .05. Thus, we don't have problems with heteroscedasticity.



9.7. **QUESTION:** Use the appropriate function to test to determine whether the residuals of the model are distributed normally. Use alpha of .05.

```{r}
with(ALBUM.model2, shapiro.test(residuals)) 
```

*ANSWER:*  The W statistic is .994 with a corresponding *p*-value of .72; *p* > .05 so we don't have evidence that residuals deviate from a normal distribution 



9.8. **QUESTION:**  Use the appropriate function to test for independence of errors. Is the assumption met? Why or why not? Use alpha of .05.

```{r}
car::dwt(ALBUM.model2)
```

*ANSWER:*  The D-W statistic value is about 1.95 with a corresponding *p*-value of .74; *p* > .05, so there does not appear to be a violation in independence of errors.



9.9. **QUESTION:**  Use the appropriate function to test for multicollinearity. Use the suggested values to determine if you have multicollinearity.

```{r}
car::vif(ALBUM.model2)
sqrt(car::vif(ALBUM.model2))
```

*ANSWER:*  The square root of the variance-inflation factors do not exceed 2. Although there is a correlation between some of the predictors, that relationship is not large enough to compromise estimates of unique variance for each predictor. 


9.10. **BONUS QUESTION:**  Compare the adjusted R-squared values for the bivariate model and the model with 3 predictors. Does R-squared change? If so, how?

*ANSWER:*  

Model 1: Adjusted R-squared:  0.3313
Model 2: Adjusted R-squared:  0.6595


We could also compare the models statistically using an ANOVA test. This shows that the model with 3 predictors is statistically better than the simple model.

```{r}
anova(ALBUM.model, ALBUM.model2)
```


9.11. **BONUS QUESTION:** Do `Ads` predict `Sales` differently holding constant the influence of `Airplay` and `Attract` on `Sales`?

*ANSWER:*
