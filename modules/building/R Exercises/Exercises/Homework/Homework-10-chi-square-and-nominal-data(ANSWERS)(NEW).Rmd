---
title: "Homework 10: Qualitative-Nominal-Categorical Data: Chi-Squared Tests (ANSWERS)"
author: "partner names"
date: "type date"
output: 
 html_document:
   toc: true # this will create a table of contents of hyperlinks (change to false to omit)
   toc_depth: 2
---

#Part A
##Before you begin

This homework exercise involves having you answer some questions, write some code, and create a nice HTML file with your results. When asked different questions, simply either type your coded or written responses after the ANSWER message. When asked to write code to complete sections, type your code in the empty code blocks that follow the *ANSWER* message (between the back ticks). After adding that code, you must make sure that it will execute. So remember to read the content and run each line of your code as you write it so that you know it executes correctly. If your code does not execute, then RMarkdown won't know what you are telling it to do and your HTML file will not be produced. Also, don't create your HTML file until you finish and know that all your code works correctly.

1.0. Installing and using libraries in RStudio

1.1. Run the code below to download some libraries needed for this assignment. By using `include=FALSE, cache=FALSE` in the `R` code header, we will make sure that any error messages do not appear in the HTML file you create.

```{r include=FALSE, cache=FALSE}
source("https://dl.dropboxusercontent.com/u/6036547/109_2016_f.txt?raw=1")
```

If you know what libraries you will use for your code, you can load them now. Use `library()`to load the following libraries: `lsr`, `car`, `vcd`

*ANSWER:* 
```{r, message = FALSE, warning = FALSE}
library(lsr)
library(car)
library(vcd)
```


1.2. Key functions used for this assignment: 

- `vcd::assocstats()` for obtaining effect size measures for $\chi$^2^
- `binom.test()` for testing fit of a binomial distribution
- `chisq.test()` for testing goodness-of-fit and independence
- `lsr::cramersV()` for calculating effect size for $\chi$^2^ goodness-of-fit
- `fisher.test()` for testing goodness-of-fit and independence
- `car::recode()` for recoding responses into other values

1.3. Set your working directory if necessary.

```{r}
#setwd("c:/users/gcook/desktop/Psyc109")
```

#Part B
##1.0. Overview of the *Chi-squared ($\chi$^2^)* test

1.1. The Pearson Chi-squared ($\chi$^2^) test is a hypothes test used commonly for data that are nominal (categorical) in scale and are well suited for analyzing data for which medians or means do not make much sense. For example, belonging or not belonging to a category may not produce values for reporting means or medians. Asking 100 people whether they own a Macbook, Windows PC, or a Linux PC would tell you how many people belong to each of those categories. You could count the number of people who use a certain computer type as a *frequency or count* who belong to each category. Using the mode as a measure of central tendency, you would know which computer type was most common. In this case, the "average" would be the mode rather than the mean or median. If 50 people in your sample have a Mac, 47 have a Windows PC, and 3 have a Linux PC, the data points are just observation counts of 50, 47, and 3. Moreover, there is also no variability to calculate because all people who have a Mac are the same -- they have a Mac. 

1.2. The $\chi$^2^) test is an approximation test rather than an exact test, which means the *p*-value associated with the test statistic will be biased with small sample sizes. In such cases, use the Fisher Exact test in order to obtain an accurate *p*-value for interpretation. The bias of the $\chi$^2^) test is inversely proportional to the sample size, so larger sample sizes produce less bias. The $\chi$^2^) test is much easier to compute by hand than Fisher's Exact test and for this reason we will discuss the concept of analyzing frequencies from the $\chi$^2^) approach, but will use `fisher.test()` for data analysis.  

1.3. For many research questions, you might assign people randomly to different categories and being smart by keeping the sample sizes across your groups equal. Or you may want to keep equal numbers of non-randomized variables (e.g., Gender Identity, Race, Personality type, etc.) for purposes of studying a dependent variable that might be influence differently by those IVs. For other research questions, however, belongingness to groups or categories may be the dependent variable. You might simply collect data by asking questions about how people fit into categories (e.g., Do you recycle? Do you own a Mac?, Do you own a home?, etc.). 

Chi-squared tests take on two general forms, which depend on the question asked and the number of independent variables studied. 


##2.0 The *Fisher Exact* and *Chi-squared goodness-of-fit test* or *test-of-homogeneity*

2.1. One application of the Chi-squared test is for determining whether a *single* categorical variable follows a hypothesized population distribution. In other words, you are testing whether the frequencies of events obtained from data fit the expected frequencies associated with a hypothesize population. Returning to the computer data, you have 50 people have Macs, 47 have a Windows PC, and 3 have a Linux PC. The *obtained* frequencies are then compared to the *expected* frequencies for some hypothetical expected, say 1/3, 1/3, 1/3. we can test the data using `chisq.test()`.

- `chisq.test(variable of obtained frequencies)`

```{r}
# make a vector with the obtained frequency counts
Computers <- c(50, 47, 3)

# specify the chi-square model by passing the vector to the function
Computer.chi <- chisq.test(Computers)

# for curiosity sake, we can check the frequencies for what is expected if the likelihoods were 1/3 each (the default)
Computer.chi$expected

# and the observed (same as our vector)
Computer.chi$observed

# examine the Pearson chi-square statistic to determine if the observed frequencies fit the expected frequencies, or if they appear not to 
Computer.chi
```

The output reveals the $\chi$^2^ value, the *df* (number of categories - 1), and the *p*-value for the likelihood of the data occurring if the expected frequencies were a good fit for the data. What we see is that if alpha was .05, the obtained frequencies do not fit the expected frequencies; observed data are different from expected. This means the model of equal frequencies (in this example) is NOT a good fit of the data because evidence suggests otherwise.

There are various effect size measures for $\chi$^2^ tests. Calculating an effect size can be done using `lsr::cramersV()` from the `lsr library` by passing the $\chi$^2^ object to the function. Cramer's V is a measure of association for nominal variables (think Pearson for nominal data). It ranges from 0 to 1.0, with larger V values representing stronger relationships between the data.  

- `lsr::cramersV(the chi-squared object)`

```{r}
#
Computer.v <- lsr::cramersV(Computers)

# take a look at the effect size
Computer.v
```

In this case, Cramer's V is about .46. If the frequencies for the obtained data were equivalent across all levels of the IV, V = 0 because obviously the frequencies would not be related to computer type. 


2.2. By default, the chi-squared test, `chisq.test()`, assumes equal probabilities of events (expected values) across levels of your IV. In some cases, when you know nothing about various events, a reasonable assumption is that they could be equal. However, you might do some research on market share and find the 8% of people own Macs, 91% own Windows PCs, and 1% own Linux PCs. Based on this information, you may wish to ask if your sample representing your campus sample differs from the entire market share based on the entire population. This could be useful if you were a computer vendor; knowing your target audience is useful when selling products. In this case, you would have to change your expected frequencies to something other than equivalent frequencies (the default setting).

To change the expected probabilities, add the "p" argument to `chist.test()`; p is a vector of probabilities of the same length as your levels of IV, in this case 3 category levels. The sum of the probabilities has to sum to 1.0.

```{r}
# expected probabilities for the 3 events based on data; Mac, PC, Linux
Expprob <- c(.08, .91, .01) # must sum to 1.0

# specify the chi-square model
Computer.chi <- chisq.test(Computers, p = Expprob)  
# or if you like everything together, computer.chi <- chisq.test(computers, p = c(.08, .91, .01))

# for curiosity sake, we can check the frequencies for what is expected based on market share data; more on this below when exp < 5
Computer.chi$expected

# and the observed (same as our vector)
Computer.chi$observed

# examine the Pearson chi-square statistic
Computer.chi
```

This comparison also reveals that they observed data do not fit what is expected from the market-share data. In fact, by comparing the observed and expected frequencies, there certainly appears to be more Mac users in our sample and fewer Windows users, but the Linux users seem to be about right.


2.3. Reporting the test statistic values

Based on the R output, you will see the variable for which you calculated, computers, the $\chi$^2^ value based on the calculation, the degrees of freedom, and the *p*-value corresponding to finding frequencies differ from expected under H0. 

When you calculate a Chi-squared test for independence and report the results of the statistic, you will need to specify the ($\chi$^2^ value and the degrees of freedom. The df for the test is the number of categories - 1 (e.g., 3 - 1 = 2).

To report the outcome of the test, the general form is:

  $\chi$^2^ (df, N = sample size) = chi-squared value, p < or > $\alpha$ 

By substitution and making sure to p, we can say:

  $\chi$^2^(2, N = 100) = 245.77, *p* < .05; Cramer's V = .46


If you used the *p*-value to make decisions about H0, you could interpret the data as evidence for the frequency counts from your samples not being the same as expected. In other words, you have evidence that you samples do not come from populations with equivalent ownership (example 1) across ownership of the computer types; your samples likely come from populations with unequal frequencies. 


2.4. There is an assumption of the $\chi$^2^ test, which was caught in the warning message. The warning that appeared when running the test indicated that the "chi-squared approximation might be incorrect". This warning will occur if the expected frequency is smaller than 5. In such cases, you may wish it increase your sample size such that the smallest expected frequency is greater than 5. 



##3.0. The *Fisher Exact test* and *Chi-squared test for independence*)

You may be interested in determining whether belongingness to one category depends on belongingness to another category. This test is referred to as the Fisher Exact *test for independence* and is based on the joint occurrence of events. Let's use the `DressColor` data for this test and ask whether the color of dress people saw differed by if they wore glasses. 

3.1. Prepping data for the test.
```{r}
# read in data frame. 
COLOR <- read.csv("DressColor.csv")


# create separate vectors from the vectors in the SURVEY data frame (there are other ways of doing this, but this approach is easy)
Glasses <- COLOR$Glasses 
Dress <- COLOR$Dress

# then add those vectors to a new data frame with a meaningful name using data.frame()
COLORG <- data.frame(Dress, Glasses)

# make a smaller and more manageable data frame to remove anyone with NA 
COLORG <- subset(COLORG, 
                    complete.cases(COLORG), 
                    select = c(Dress, Glasses))

# look at the joint occurrence of events in a 2 x 2 table/contingency table 
table(COLORG)
```

Notice that 7 out of 24 individuals who didn't wear glasses (7 + 17) and 17 out of 29 (17 + 12) who do wear glasses saw the white dress. Let's clean this up a little bit to make the labels easier to read. 

3.2. Making the data more readable if that matters to you.

```{r}
# in order to help understand what the categories are, make factors and label the levels
# factorize the IVs and add to the new data frame
COLORG$Dress.fact <- with(COLORG, factor(Dress, 
                              levels = c(1, 2), labels = c("White/Gold", "Blue/Black")))

# create Gender factor and add to the data frame, specify levels and labels
COLORG$Glasses.fact <- with(COLORG, factor(Glasses, 
                               levels = c(0, 1), labels = c("No", "Yes")))

# make the data set contain ONLY the vector variables relevant for the chi-square test and omit any missing values.
COLORG <- na.omit(COLORG[, c("Dress.fact", "Glasses.fact")])


# does it look right?
head(COLORG)


# assign the table to an object for the 2 x 2 table
COLORG.2x2 <- table(COLORG)
```

3.3. Specifying the model. 

Ok, now the goal of the Fisher Exact test of independence is to determine if the color of the dress is associated/related to wearing glasses. A significant outcome would indicate that the variables are not independent of one another, but rather are dependent. Using `fisher.test()`from the built-in stats library, simply pass the table object to the function.

- `fisher.test(contingency table)`
- `chisq.test(contingency table)`

```{r}
# pass the table to fisher.test()
COLORG.2x2.fish <- fisher.test(COLORG.2x2)

# examine the model
COLORG.2x2.fish
```

3.4. Interpreting the output.

We can see that the data fall slightly short of evidence for a relationship between glasses and dress color when tested against an alpha = .05. Fisher Exact test or $\chi$^2^ test can able be used used for contingency tables that are larger than this 2 x 2.

The $\chi$^2^ test can also be used if you have large sample sizes by simply using `chisq.test()` as before, but by passing the contingency table to the function.

```{r}
# pass the table to the function in order to create the object holding the chi-squared data
COLORG.2x2.chi <- chisq.test(COLORG.2x2)

# inspect
COLORG.2x2.chi
```

The Fisher Exact test or $\chi$^2^ test can able be used used for contingency tables that are larger than this 2 x 2.

WARNING: If you do use `chisq.test()` a warning message like "Chi-squared approximation may be incorrect" will appear if you have small cell values in the contingency table. In this case, you can use a Fisher exact test. The Yates' correction is simply a correction for the *p*-value not being exact for the $\chi$^2^ test.

3.5. Effect sizes.

** Check the phi coefficient - isn't appearing on the do it yourself - Why?**
When obtaining effect sizes for contingency tables (when you have more than one factor), there are different effect sizes you can use. Some people recommend the $\phi$ coefficient over Cramer's V. You can obtain that using `vcd::assocstats()` from the `vcd library`.

```{r}
# get measures of effect size
# 
vcd::assocstats(COLORG.2x2)
```

3.6. If you didn't have the data in a data frame, but you did have the frequencies, you could create a data frame using the frequencies to create rows and columns containing the frequencies.

```{r}
whitegoldglass <- c(7, 17)
blueblackglass <- c(17, 12)

# put into a data frame
DF <- data.frame(whitegoldglass, blueblackglass)

# convert the data frame into a matrix 
DF <- data.matrix(DF, rownames.force = NA) 
DF

# then pass the DF to the function, 
chisq.test(DF)
```

##4.0. Assumptions for *$\chi$ squared* tests

4.1. Assumptions for the $\chi$^2^ goodness-of-fit test

  a) Data must be frequencies
  b) Expected frequencies cannot be smaller than 5; if you have sample sized smaller than 10, use a Fisher's exact test fisher.test()
  c) Data must be independent (if you have repeated measures, use the McNemar test or the Cochran-Mantel-Haenszel test)


#Do it yourself!

1. **QUESTION:** Bring in the `GLOBAL` data frame to determine if there is a relationship between beliefs about global issues and being a teacher. Make sure to examine the variables of interest so that you can create appropriate labels for your variables. 

Teacher: 0=No, 1=Yes
Global Issues: 1=Climate Change, 2=Terrorism, 3=Poverty, 4=Education, 5=Food Scarcity, 6=Financial Collapse, 7=Other

*ANSWER*
```{r}
GLOBAL <- read.csv("Global.csv")

```

2. Grab the relevant variables and put them into their own vectors. Then assign them to a data frame. 

*ANSWER:*
```{r}
# get relevant variables for the 
Global <- GLOBAL$GlobalIssue
Teacher <- GLOBAL$Teacher

# assign to a small data frame
GT <- data.frame(Global, Teacher)
```


3. Take a look at the data frame using `str()`. If the variables are not factors then convert them and give them meaningful labels.  


```{r}
str(GT)

# create Gender factor and add to the data frame, specify levels and labels
GT$Teacher.f <- with(GT, factor(Teacher, levels = c(0, 1), labels = c("No", "Yes")))

# create Global factor and add to the data frame, specify levels and labels
# The levels aren't necessary for answering the question, but they are included below for practice.
GT$Global.f <- with(GT, factor(Global, 
                    levels = c(1, 2, 3, 4, 5, 6, 7), 
                    labels = c("Climate Change", "Terrorism", "Poverty", "Education",
                               "Food Scarcity", "Financial Collapse", "Other")))


```


4. Make the data set contain only the vector variables relevant for the chi-square test; dropping out any NA on any variables.

```{r}
GT <- na.omit(GT[, c("Teacher.f", "Global.f")])

```

5. Make a table (should only have your variables of interest) and run a chi-square test on the table using `chisq.test()`. 

```{r}
t <- table(GT)
GT.chi <- chisq.test(t)   # or GG.chi <- chisq.test(table(GG))
GT.chi
```

6.However, examine the observed and expected frequencies to see what's actually going on with the data.

```{r}
GT.chi$observed

# and examine the expected values to see if less than 5 
GT.chi$expected

```

7. Do Fisher exact because fe < 5 

```{r}
GG.fish <- fisher.test(t)
GG.fish  # this shows the same as the chi-squared test
```

8. For an effect size, pass the table object not the Chi-squared object to the effect-size function

```{r}
vcd::assocstats(t)
```


9. Interpret the results: 

*INTERPRETATION:* The df make sense, (r - 1)(c - 1), (1)(6) = 6 for this analysis. If using an alpha = .05 and *p*-value, there does not appear to be a relationship, or dependency, between being a teacher and beliefs about current global issues. In other words, the occupations measured here seem to have similar beliefs regarding global issues. However, this does not mean that using different groups of occupation or different global issues would not reveal a dependence.  

Examining the effect size rather than the *p*-value, the Cramer's V is .31, which could be interpreted as a weak relationship.


#DONE!#
Upload your knit HTML file that you completed with your partner. Make sure to include your names.
