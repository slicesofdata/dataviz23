---
title: "Homework 08: Comparing Multiple Sample Means Anova"
author: "partner names"
date: "add date"
output: html_document
---

##Part A##
#General Questions#

1. **QUESTION:** Describe the purpose of an ANOVA

*ANSWER:* 


2. **QUESTION:** What does an ANOVA test?

*ANSWER:* 

##Part B##

##Before you begin##

This homework exercise involves having you answer some questions, write some code, and create a nice HTML file with your results. When asked different questions, simply either type your coded or written responses after the ANSWER message. When asked to write code to complete sections, type your code in the empty code blocks that follow the ANSWER message (between the back ticks). After adding that code, you must make sure that it will execute. So remember to read the content and run each line of your code as you write it so that you know it executes correctly. If your code does not execute, then RMarkdown won't know what you are telling it to do and your HTML file will not be produced. Also, don't create your HTML file until you finish and know that all of your code works correctly.

##1. Installing and using libraries in RStudio##

1.1. Use the RStudio interface to install packages/libraries. Go to the Tools option and select Install Packages. Type the package name(s) correctly using the proper letter casing. Also, make sure that you *check the box to Install Dependencies*. Do not install with code.

Install the package:  plyr

1.2. Key functions used for this assignment: 

subset() for creating a subset of the data
- aov() for calculating an ANOVA
- bartlett.test() for viewing homogeneity of variance
- count() for viewing the frequency of each factor
- data.frame() for combining vectors into a data frame; built-in
- histogram() for histograms; lattice library
- levels for creating levels of a variable
- leveneTest() for viewing homogeneity of variance
- shapiro.test() for testing normality
- TukeyHSD() for analyzing the differences between groups



## 2. Loading libraries ##
If you know what libraries you will use for your code, you can load them now. Use the library() function to load the following libraries: lattice, plyr, and car. 

*ANSWER:* 
```{r, message = FALSE, warning = FALSE}
library(plyr)
library(car)
library(lattice)
```

##1. Overview of a between-groups ANOVA##

The statistical procedure for testing variation among the means of more than two groups 
is called the analysis of variance, abbreviated as *ANOVA*. The null hypothesis in an analysis of variance is that the several populations being compared all have the same mean. Hypothesis testing in analysis of variance is about whether the means of the samples differ more than you would expect if the null hypothesis were true. This question about means is answered, surprisingly, by analyzing variances (hence the name analysis of variance). Among other reasons, you focus on variances because when you want to know how several means differ, you are asking about the variation among those means

*Analysis of Variance* (ANOVA) is a commonly used statistical technique for investigating data by comparing the means of subsets of the data. The base case is the *one-way between-subjects ANOVA* which is an extension of *independent-samples t-test* for instances where comparisons are made between more than two groups.

Data for the *one-way between-subjects ANOVA* are sub-divided into groups based on a single classification factor. The standard terminology used to describe the set of factor levels is treatment even though this might not always have meaning for the particular application. There is variation in the measurements taken on the individual components of the data set and ANOVA investigates whether this variation can be explained by the grouping introduced by the classification factor.


For example: To test this we are going to bring in some variables from the survey data that we collected. In this case we are going to compare tipping habits across participants with different political orientations. 

First lets bring in the Survey Data:
```{r}
setwd("c:/users/gcook/desktop/Psyc109")
SURVEY <- read.csv("SurveyNames.csv")

SURVEY$Gender <- ordered(SURVEY$Gender, levels = c(0 , 1), labels = c("Male", "Female"))

t.test(SURVEY$HourSleep ~ SURVEY$Gender, paired = F)
```



##2.Examining Data and Assumptions of an ANOVA## 

2.1 Examining variables. Because there are so many variables in the SURVEY data, we can create a subset with the "ID', 'PolParty', and the 'Tip' variables:

```{r}
PARTYTIP <- subset(SURVEY, select = c(ID, PolParty, Tip))
```

If you examine the str() of the data frame, the variable you will notice that *PolParty* was brought in as an integer variable. R will read in a data file in a way that it thinks is appropriate. However, sometimes you will need to change the scaling of the variable so that you can perform certain test. Because PolParty refers to the political party preference of respondents, it is a categorical/nominal or factor variable so we need to modify it. Using the factor() function will convert it to a factor before we move on. 

```{r}
str(PARTYTIP)

PARTYTIP$PolParty <- factor(PARTYTIP$PolParty)
str(PARTYTIP)
levels(PARTYTIP$PolParty)
```

Now when we check the variable *PolParty* should be a factor

```{r}
str(PARTYTIP)
```


Also for the purpose of ease we are going to set levels for the PolParty variable for ease of interpreting results. These levels represent the different factors that constitute the variable PolParty 

```{r}
PARTYTIP$PolParty <- ordered(SURVEY$Gender, levels = c(0 , 1, 2, 3, 4, 5), 
                             labels = c("No Affiliation", "Democrat", "Independent", 
                                        "Libertarian", "Republican", "Green")
                             )
levels(PARTYTIP$PolParty)


levels(PARTYTIP$PolParty)[1] <- " 0 - No Affiliation"
levels(PARTYTIP$PolParty)[2] <- " 1 - Democrat"
levels(PARTYTIP$PolParty)[3] <- " 2 - Independent"
levels(PARTYTIP$PolParty)[4] <- " 3 - Libertarian"
levels(PARTYTIP$PolParty)[5] <- " 4 - Republican"
levels(PARTYTIP$PolParty)[6] <- " 5 - Green"
```

2.2 Assumptions of a between-subjects ANOVA

  1. DV is interval/ratio in measurement scale.
  
  2. The populations have the same variance; homogeneity of variance.
  
  3. The populations are distributed normally.
  
  4. Each value is sampled independently from each other value. This assumption requires that           
  each subject provide only one value. If an experimental unit provides two scores, then the values are 
  not independent. The analysis of data with two scores per subject is discusses along with 
  within-subjects ANOVA later.

These assumptions are the same as for a *t-test8 of differences between groups except that they apply to two or more groups, not just to two groups.


2.3 Testing the Assumptions

Lets check the assumptions before we run the ANOVA:

1.3.1 The Same Variance

In an *ANOVA*, one assumption is the *homogeneity of variance* (HOV) assumption. That is, in an *ANOVA* we assume that treatment variances are equal. Moderate deviations from the assumption of equal variances do not seriously affect the results in the *ANOVA*. Therefore, the *ANOVA* is robust to small deviations from the *HOV* assumption. We only
need to be concerned about large deviations from the HOV assumption. Bartlett's Test is a common test for the homogeneity of variances problem under the assumption that each treatment population is normally distributed.

Below we will perform Bartletts test to determine if we violate the homogeneity: 

```{r}
#First let us plot out the variables
plot(Tip ~ PolParty, data = PARTYTIP)

#Next let us run bartletts test. 
bartlett.test(Tip ~ PolParty, data = PARTYTIP)


```

Notice that Bartletts test of homogeneity indicates that we violate the homogeneity of variance. However, because some of our factors have a very low frequency, Bartletts test may be influenced by violations of normality. We are going to recommend Levene's Test in this case because the test is not as sensitive to departures from normality.

To run levenes test:
```{r}
#Run the function
leveneTest(PARTYTIP$Tip, PARTYTIP$PolParty)

```


Levenes test was not significant showing that we may not have violated homogeneity. Levene's Test is robust because it is not as sensitive to symmetric heavy-tailed distributions. It is important to always double check your assumption of homogeneity before continuing. 

2.3.2 Normal Distribution

To test the assumptions of normality we will use the shapiro.test() that we have previously used. 

```{r}
shapiro.test(PARTYTIP$Tip)
histogram(PARTYTIP$Tip)
```

Unfortunatly, you can not run shapiro.test() on a factor variable. To run the test of normality for an ANOVA we will need to run the shapiro.test() on the ANOVA we create. The R function aov() can be used for fitting ANOVA models.

Below we will create the ANOVA function and run shapiro.test() on it. 

```{r}
POLIANOVA <- aov(Tip ~ PolParty, data = PARTYTIP) # Runs an ANOVA examining tipping habits by Political Party
shapiro.test(residuals(POLIANOVA)) #Tests the Assumptions
```


Although it appears that we have violated the assumptions of normality for TIP data and for Political Party we will continue with our analysis. Violations of the factors may be due to the small sample size. In the future when running an ANOVA make sure to have an adequate sample size for each factor that you are using. For the purpose of the exercise we will continue on the with the analysis. 


## 3.Creating the ANOVA model

3.1 Creating the ANOVA

Having already created the ANOVA we will now examine it:

```{r}
summary(POLIANOVA)
print(model.tables(POLIANOVA,"means"),digits=3) 
boxplot(Tip ~ PolParty, data = PARTYTIP) 
```

The summary() function printed out the ANOVA model while the print function prints out the individual means of each group for tipping habits. It is important when running an ANOVA to always examine the means of each group. 

We are going to run through the model and interpret all the results. 

##4.Understanding the Variance

4.1 The Sum of Squares

Analysis of variance is a method for testing differences among means by analyzing variance. ANOVA partitions the variability among all the values into one component that is due to variability among group means (due to the treatment) and another component that is due to variability within the groups (also called residual variation). Variability within groups (within the columns) is quantified as the sum of squares of the differences between each value and its group mean. 

For Example:
In our ANOVA the Sum of Squares between groups, which examines the differences among the group means is: *64.4* with a DF of *4*

and

The Sum of Squares within-groups, which examines error variation, or variation of individual scores around each group mean. This is variation in the scores that is not due to the treatment (or independent variable) is *437.2* with a DF of *49*. 

We are going to store them below for future equations explaining the ANOVA. You can find these numbers under the second column of the ANOVA table under Sum Sq. 

```{r}

BGSS <- 64.4
WGSS <- 437.2
```

4.2 The Mean Square

Each sum-of-squares is associated with a certain number of degrees of freedom (df, computed from number of subjects and number of groups), and the mean square (MS) is computed by dividing the sum-of-squares by the appropriate number of degrees of freedom. These can be thought of as variances. The square root of the mean square residual can be thought of as the pooled standard deviation.

Below we will calculate the Mean Square from the Sum of squares that we obtained earlier.

```{r}
BGMS <- BGSS / 4 # Mean Squares in the table for between groups
BGMS
WGMS <- WGSS / 49  #Mean Squares in the table for Within Groups
WGMS

```

The mean square values obtained can be found in the ANOVA table that we previously ran earlier under the Mean Sq column. 

4.3 The F-value

The F ratio is the ratio of two mean square values. If the null hypothesis is true, you expect F to have a value close to 1.0 most of the time. A large F ratio means that the variation among group means is more than you'd expect to see by chance. You'll see a large F ratio both when the null hypothesis is wrong (the data are not sampled from populations with the same mean) and when random sampling happened to end up with large values in some groups and small values in others.

To calculate the F-value you divide the Between Groups Mean Square by the Within-Groups Mean Square:


```{r}
F <- BGMS / WGMS
F
```


The P value is determined from the F ratio and the two values for degrees of freedom shown in the ANOVA table. Luckily you do not need to look up any values in a table when running the analysis in R.

## 5.The Post HOC test

If you achieve significance in with your ANOVA you must run a POST-HOC test to determine which factors are significantly different and the direction.  Below, we show code for using the TukeyHSD (Tukey Honest Significant Differences) which will show which factors are significantly different. 

```{r}
TukeyHSD(POLIANOVA)
```

As you can see this compares each of the groups to each other and shows you the differences as well as if the difference is significant. In this case There are no significant difference between tipping habits for individuals of different political orientations. 


## 6.Effect Size

calculate an effect size after conducting an appropriate statistical test for significance. This post will look at effect size with ANOVA (Analysis of Variance), which is not the same as other tests (like a t-test). When using effect size with ANOVA, we use Eta squared, rather than Cohen's d with a t-test. 

Effect Size Guidelines:
Small: 0.01
Medium: 0.059
Large: 0.138

To Calculate eta squared you simply divide the between groups sum of squares by the total sum of squares. 
For our example we simply divide:

```{r}

TotalSS = BGSS + WGSS

EffectSize <- BGSS / TotalSS

```

Although significant results were not achieved we can still see that we have a fairly large effect when comparing the effect size obtained to the guidelines. 



## 7. Do it yourself!##

One question asked on the survey was electronic device used most and hours spent on social media. The question names in your Survey code are *Electronic* and *SocialMedia*.

*Question* First, create a subset that contains only those questions:

*Answer*:
```{r}


```


*Question* Check the variables to make sure that you have factors and integers. If Electronics was not brought in as a factor, convert it.:

*Answer*:
```{r}

```

*Question* Check the assumption of homogeneity of variance for the variables:

*Answer*:
```{r}

```



*Question* Check the Normality for the variables:

*Answer*:
```{r}

```





*Question* Create the ANOVA model using the aov() function:

*Answer*:
```{r}

```


*Question*: Was the ANOVA significant?
*Answer*:

*Question*: What is the Sum of Squares for Between-groups?
*Answer*:

*Question*: What is the Sum of Squares for Within-groups?
*Answer*:


*Question*: Run a POST-HOC test to determine which factors are significantly different and the direction. Use the TukeyHSD() function. 

*Answer*:
```{r}

```


*Question*: Calculate the Effect Size:

*Answer*
```{r}

```


*Question*: Describe the size of the effect?
*Answer*:
