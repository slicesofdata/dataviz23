---
title: "HW04 - Z-Scores and Correlation"
author: "replace with your names"
date: "replace with the date"
output: html_document
---


##Part A##
# 0. Before you begin 

In order to serve that goal, this homework exercise involves having you answer some questions, write some code, and create a nice HTML file with your results. When asked different questions, simply either type your coded or provide written responses after the ANSWER message. When asked to write code to complete sections, type your code in the empty code blocks that follow the **ANSWER** line. After adding your code, you must make sure that it will execute. So remember to run your code by highlighting it and pressing the RUN button or pressing PC: `CONTROL+ENTER`; Mac: `COMMAND+ENTER`. 

If your code does not execute, then `R Markdown` won't know what you are telling it to do and your HTML file will not be produced. Also, don't create your HTML file until you finish.

# 1. Downloading the source files 

1.1. Run the code below to download the some libraries needed for this assignment. By using `include=FALSE, cache=FALSE` in the r code header, we will make sure that any error messages do not appear in the HTML file you create.

```{r include=FALSE, cache=FALSE}
source("https://dl.dropboxusercontent.com/u/6036547/109_2016_f.txt?raw=1")
```


1.2. Key functions used for this assignment. Functions from libraries that are not built into the base `R` version will use specific calls which are preceded by the library name and two colons [e.g., `library::function()`]: 

- `getwd()` for getting the current working directy 
- `lattice::histogram()` for plotting nicer histograms
- `library()` for loading libraries that are not in the base version of `R`
- `mean()` for the mean of a variable
- `moments::kurtosis()` for the kurtosis of a variable 
- `moments::skewness()` for the skewness of a variable 
- `read.csv()` for reading a csv file into a data frame
- `str()` for examining the structure of a data frame
- `with()` for selecting the data frame on which to perform other operations 
- `summary()` for showing the mean, median, 25th and 75th quartile, min, and max
- `length()` for examining the length of a vector or factor
- `str()` for examining the structure of an 'R' object
- `lattice::xyplot()` for showing the data in a histogram
- `cov()` for computing the covariance of two variables
- `var()` for computing the variance of a variable
- `sqrt() for computing the square root
- 'cor()'` for computing the correlation between two variables
- `psych::corr.test()` for examining the correlations, sample sizes, and probabilities                                  
                       between elements of a data.frame
- `PerfomanceAnalytics::chart.Correlation()` for visualization of a correlation matrix
- `View()` for viewing the data.frame
- `subset()` for creating a data.frame that contains a subset from another data.frame


## 2. Loading libraries ##
Use the `library()` function to load the following libraries: moments, lattice, PerformanceAnalytics, psych

**ANSWER:** 
```{r, message = FALSE, warning = FALSE}
library("PerformanceAnalytics")
library("moments")
library("lattice")
library("psych")
```


## 3. Checking your working directory ##
Always make sure that your working directory points to Psyc109 on your desktop. 
```{r}
getwd()
setwd("C:/users/gcook/Desktop/Psyc109/")
```


##Part A##
##1. Correlations##
A correlation (*r*) describes the relationship between two variables. More precisely,
the usual measure of a correlation describes the relationship between two equal interval
numeric variables. This part will examine how to use the Pearson *r* correlation.

##2. Assumptions of bivariate linear correlation using Pearson r##

First, let's take a look at the assumptions of correlations the Pearson *r* correlation coefficient: 

  1. Our two variables should be measured at the interval or ratio level (i.e., they are continuous).

  2. There needs to be a linear relationship between the two variables.

  3. There should be no significant outliers. 

  4. Your variables should each be approximately normally distributed.


## 3. Reading in a data file ##
3.1. Read a data file and assign its contents to a data frame object named icecream. We could name this object any name we want (e.g., bananasundae, ketchup, or ice). In other words, the object name does not need to match the name of the file. In this case, however, icecream seems appropriate as a name.

**ANSWER:**
```{r, echo = FALSE, message = FALSE, warning = FALSE}
icecream <- read.csv("IceCream.csv")
icecream <- read.csv("C:/users/gcook/Desktop/Psyc109/IceCream.csv")
```

3.2. Examine the structure of your data frame and its contents as you have in earlier assignments. Pay attention to the names and spelling of the variables. Just for practice, use the `summary()` function from the psych library in order to examine the means of the two variables. If you run into any problems it might help to view your data by graphing a histogram using the `lattice::histogram()` function. 

**ANSWER:**
```{r}
#Check the variable names
str(icecream)

#Calculate the and median mean for both variables; compare them and ask yourself if their values inform you about skew. Remember you need to specify the data frame object in order to tell R to obtain some of the data frame's contents. 

with(icecream, summary(Temp))
with(icecream, summary(IceCreamSold))

```


3.3. As you can probably guess from the name, the data frame you just loaded contains information about temperature and amount of icecream sold. There is also an Id variable, which corresponds to the id number of the experimental unit. I advise that you always create an Id variable so that you can keep track of your participants and because that variable will be useful for identifying specific data points. Use the appropriate fucntions from the moments library to calculate the skewness and kurtosis of both variables. Consider whether the variables are distributed normally.

**ANSWER:**
```{r}
with(icecream, moments::skewness(Temp))
with(icecream, moments::skewness(IceCreamSold))

with(icecream, moments::kurtosis(Temp))
with(icecream, moments::kurtosis(IceCreamSold))
```

3.4. The assumptions of a correlation require that our variables be distributed normally. We can check this assumption both graphically and statistically. Use the histogram function from the lattice library to create a distribution for both variables. You can use the default distribution (a percentage breakdown) rather than one using the frequency counts. For practice, consider modifying the main and xlab arguments that you modified in the previous homework so that the labels are labeled clearly. You could also use the `qqplot()` and `qqline()` functions, but for simiplicity we won't do so here. 
 
**ANSWER:**
```{r}
library(lattice)
histogram(~icecream$Temp, 
          main="Histogram of Temperature", 
          xlab="Temperature in Celcius")

histogram(~icecream$IceCreamSold, 
          main="Histogram of Ice Cream Sales", 
          xlab="Sales in Thousands of Dollars")
```

##4. Scatterplots ## 
4.1. A scatterplot is one of the best ways to visualize the nature of a relationship between two variables. Scatterplots plot individual x and y variables along the x and y axes. When using the lattice library, you will need to specify the data frame object that contains your data as well as the variables and then write the formula for the plot: y ~ x ; y (weight) plotted as a function of x (height). 

Or you may want to plot x (height) as a function of y (weight) as: x ~ y 

Use the `xyplot()` function to plot IceCreamSold as a function of Temp from the icecream data frame. Also, make sure to add labels to your scatterplot that are specific. The temperature is in Celsius and sales are in thousand of dollars. To make the code legible, we will make each argument in the `xyplot()` function on its own line of code; each line after the first needs to be indented so that R knows all the lines of code go together. 

**ANSWER:**
```{r}
xyplot(IceCreamSold ~ Temp, 
       data = icecream,  
       main = "Ice Cream Sales as a function of Temperature", 
       xlab = "Temperature in Celcius", 
       ylab = "Sales in Thousands of Dollars")
```

Based on the scatterplot, do the data appear to follow a linear pattern? Curvilinear? 
**ANSWER:** Looks fairly linear; more linear than curvilinear

If you wanted to add a regression line, you could add the following argument to your `xyplot()` `function: type = c("p", "r")`

## 5. Evaluating the magnitude of the correlation coefficient ##
The graph provides useful information, but it does not provide the magnitude of the correlation. 

5.1. A correlation is a statistical measure the represents the linear relationship between two variables.  

5.2. You can calculate the Pearsons *r* correlation by dividing the covaraince of the two variables you are interesed in (In this case Temperature and Ice Cream sold) by the two standard deviations of the variables. For example:

```{r}
#Calculate the covariance of the two variables (or sum of cross products)
spxy <- cov(icecream$Temp, icecream$IceCreamSold)

#Then you need to calculate the standard deviations for both variables
xvar <- var(icecream$IceCreamSold)
yvar <- var(icecream$Temp) 

#You can then divide the covariance by square root of the variances:
rxy <- spxy/sqrt(xvar * yvar)
rxy
```

5.3. However, we can use the built-in `cor()` function to avoid all of those steps. We will calculate the magnitude of Pearson's *r* correlation coefficient using the `cor()` function and assign that value it to an obect named r. Then we will square it in order to obtain the coefficient of determination, which provides a measure of the variance in one variable that is determined by (explained by/accounted for by) another variable.  In this case, r-squared will provide the proportion of variance in IceCreamSold that can be accounted for by Temp (or the proportion of variance in Temp that can be accounted for by IceCreamSold).  

```{r}
# assign the r value to a an obect named r
r <- cor(icecream$Temp, icecream$IceCreamSold)

# square r and assign it to an obejct named r2
r2 <- r^2

# display both
r; r2 
```


5.4. Use the `cor.test()` function to obtain the r score and the exact *p*-value associated with obtaining a correlation of that size or larger if, under H0, the variables were not related and therefore the correlation was 0. When you need the exact p-value for your r values, you should use this test.
```{r}
cor.test(icecream$Temp, icecream$IceCreamSold)
```

5.5. You could write your own R function to combine histograms for x and y your variables, the scatterplot, and the correlation coefficient (*r*) to make that information appear any way you like to see them. However, we can use a function that does this already.  

The `chart.Correlation()` function from the `PerformanceAnalytics` library provides: **(a)** histograms for each variable along with a density plot overtop the histograms, **(b)** the scatterplot along with a line that fits the points, and **(c)** the correlation coefficient. You can apply the function on an entire data frame object if you wanted to examine all variables; in our case, we only have 3 variables so the visual is a simple 3 x 3 matix.  

When you use some functions, you might obtain errors. You should take a look a the errors that *R* reports in order to determine if they are important for your data. If you know they are not an issue and do not want them to print out in your RMarkdown file, you can choose not to display them by setting the warning argument to `FALSE` as well as not display messages by setting that to `FALSE` as well.

```{r, warning = FALSE, message = FALSE}
#Notice the header change to hide warnings and messages from appearing in you htlm file.
chart.Correlation(icecream)
```


5.6. One problem is that the ID variable isn't really useful here, so you would want to remove it. We can select subsets of our data frame and only examine the variables of interest. There are different ways we can do so, which are only a matter of personal preference. Here are two ways.


5.7. Because a data frame is composed of `[rows, columns]` , we can present all rows of data (all people), but remove Column 1 because that column is the Id variable. 
```{r, message = FALSE, warning = FALSE}
chart.Correlation(icecream[, -1]) 
```


5.8. If thinking about a data frame as rows and columns is confusing, you can pass all of the rows and then specify only the column names that you wish to examine. In this case, we have only two variables to examine. 
```{r, message = FALSE, warning = FALSE}
chart.Correlation(icecream[, c("Temp", "IceCreamSold")])
```


5.9. By NHST interpretation, a significant correlation would be one for which its value would be very unlikely under the null hypothesis that would assume *r* = 0. Larger correlations (especially those with with large sample sizes) would be rare to choose at random from a population of *r* values that follow a normal distribution having a mean = 0.    

Note that the scatterplot is shown in the bottom left corner, graphs of the variables are shown in the middle and the correlation is in the top right. 

The number of asterisks above the correlation value indicate whether the corresponoding *p*-value for the Pearson *r* coefficient is below an alpha of (0.05 = *, 0.01 = **, 0.001 = ***). 

The magnitude of the correlation indicates a measures of strength of a linear relationships between the variables. For example, we see that this is a positive linear correlation between temperatue and amount of ice cream sold, where high temperatures are assocaited with a high amount of ice cream sold. Like wise low temperatures are associated with low amount of ice cream being sold. This can be visually understood by looking at the scatterplot and seeing that dots that are low on temperature and also low on ice cream sold. 

##6. Identifying outliers##
6.1. The reason why this chart is usefull is that it can allow for you graph the variables, potentially seeing something that might not show up when only viewing the descriptive statistics. 

6.2. For example, load in the next dataset and calculate the mean and skewness of the variables. 
**ANSWER:** 
```{r}
#hide skewed <- read.csv("skewedicecream.csv")
#
skewed <- read.csv("c:/users/gcook/desktop/Psyc109/skewedicecream.csv")
str(skewed)
#Calculate the mean of both variables
mean(skewed$Temp); mean(skewed$IceCreamSold); 
#Calculate the skewness and kurtosis of both variables
skewness(skewed$Temp); kurtosis(skewed$Temp)
skewness(skewed$IceCreamSold); kurtosis(skewed$IceCreamSold)  
```

6.3. After loading in the new dataset, use `chart.Correlation()` to chart the correlations of variables in the skewed data frame.
**ANSWER:**
```{r, message = FALSE, warning = FALSE}
#hide
chart.Correlation(skewed)
```

6.4. As before, the `chart.Correlation()` function will create a chart with all of the variables in the data frame unless you exclude some. 
```{r, message = FALSE, warning = FALSE}
chart.Correlation(skewed[ , -1])
```

6.5. As you can see the magnitude of the correlation has decreased and the relationship between the variables has been altered greatly when the one score that is very different from the others is part of the data set. You may also notice that the graph for IceCreamSold is not as clean as it was before. This is what happens when you do not have a lot of data points and when one of those data points is an outlier. Outliers can greatly influence the relationship between two variables, as well as the distribution of one of your variables. In this case not a lot of ice-cream was sold on a very hot day. When running a study it would be very possible to collect data on a day like that. The next few steps will go over how to determine if the data point is an outlier. 


##7. Removing Outliers: Something important to consider##
7.1 Now we will determine if the score might be considered and outlier by graphing out the correlation and determine if a specific data point is very different from the others. In the previous graph there is a data point in the far left corner that is high on temperature and low on ice cream sold. It appears that this data value is greatly influencing the correlation, bringing it down to a .40. First we view the data set and attempt to determine which data point is the outlier.

```{r}
#Let's view the skewed dataset
View(skewed)
```

7.2. Luckily, because of the small dataset, data point 13 can be identified as the outlier. If we determine a score in an outlier, we can simply make the score a missing value, or not available (NA). In order to do this, we can modify the Temp values to replace the scores with NA by telling R to take the skewed$Temp variable and check to see if the Id value is equal to (==). The double equal evaluates the Id number to see if it matches the value. In the example below, we use a vector c(13) rather than simply using a specfic of 13 because the vector would allow us to evaluate matches for more than one Id number (e.g., c(1,2,13)).

To remove this variable we will create an outlier object which can be used to remove all the outliers that we identify. 

```{r}
skewed$Temp[skewed$Id == c(13)] <- NA # or more than one Id number if you have more than 1 outlier 
View(skewed) # notice how the value is not NA for participant 13
```

7.3. Now that you have made some values NA, you can use a function that selects variables that has only the x and y data pairs. The `complete.cases()` function will allow you to select only the complete x and y cases (not any row with NA). We will also need to create a small data frame that contains this subset of data so that we can run the correlation; the `subset()` function makes this easier to so. `The subset()` function also allows you to specify the variables you wish to select for your subset. 

7.4. The following code is using subset() to create a subset of data. The first argument is the original data that you want to subset; in this case skewed. The second argument tells R to use only the compete cases of the skewed data frame, and the third argument selects your variable(s) of interest; the c() is used to combine the variables of interest. The new data frame is assigned to an object named notskewed.

Run the function on the skewed data frame and assign the new data frame to a new object (e.g., notskewed). If you look at the contents of your new data frame, you will see that you have a smaller data frame now. 

```{r}
notskewed <- subset(skewed, 
                    complete.cases(skewed), 
                    select = c(Temp, IceCreamSold))

# Take a look at the new data frame
notskewed
```

7.5. Now, in order to calculate the correlation and see the chart as before, we need to apply the functions to the new data frame. 

```{r, message = FALSE, warning = FALSE}
notskewedr <- cor(notskewed$Temp, notskewed$IceCreamSold)
notskewedr

chart.Correlation(notskewed)
```

7.5. As you can now see, removing the data point (outlier) has greatly increased the correlation and reveals the strong relationship between Temperature and Ice Cream sold. If there was more than out outlier, both can be placed in the Outlier object. 

__________________________________________________________________________________









##Part B#











5.4. Use the cor.test() function to obtain the r score and the exact *p*-value associated with obtaining a correlation of that size or larger if, under H0, the variables were not related and therefore the correlation was 0. When you need the exact p-value for your r values, you should use this test.
```{r}
cor.test(icecream$Temp, icecream$IceCreamSold)
```

5.5. You could write your own R function to combine histograms for x and y your variables, the scatterplot, and the correlation coefficient (r) to make that information appear any way you like to see them. However, we can use a function that does this already.  

The chart.Correlation() function from the PerformanceAnalytics library provides: **(a)** histograms for each variable along with a density plot overtop the histograms, **(b)** the scatterplot along with a line that fits the points, and **(c)** the correlation coefficient. You can apply the function on an entire data frame object if you wanted to examine all variables; in our case, we only have 3 variables so the visual is a simple 3 x 3 matix.  

When you use some functions, you might obtain errors. You should take a look a the errors that R reports in order to determine if they are important for your data. If you know they are not an issue and do not want them to print out in your RMarkdown file, you can choose not to display them by setting the warning argument to FALSE as well as not display messages by setting that to FALSE as well.

```{r, warning = FALSE, message = FALSE}
#Notice the header change to hide warnings and messages from appearing in you htlm file.
chart.Correlation(icecream)
```


5.6. One problem is that the ID variable isn't really useful here, so you would want to remove it. We can select subsets of our data frame and only examine the variables of interest. There are different ways we can do so, which are only a matter of personal preference. Here are two ways.


5.7. Because a data frame is composed of [rows, columns] , we can present all rows of data (all people), but remove Column 1 because that column is the Id variable. 
```{r, message = FALSE, warning = FALSE}
chart.Correlation(icecream[, -1]) 
```


5.8. If thinking about a data frame as rows and columns is confusing, you can pass all of the rows and then specify only the column names that you wish to examine. In this case, we have only two variables to examine. 
```{r, message = FALSE, warning = FALSE}
chart.Correlation(icecream[, c("Temp", "IceCreamSold")])
```


5.9. By NHST interpretation, a significant correlation would be one for which its value would be very unlikely under the null hypothesis that would assume *r* = 0. Larger correlations (especially those with with large sample sizes) would be rare to choose at random from a population of *r* values that follow a normal distribution having a mean = 0.    

Note that the scatterplot is shown in the bottom left corner, graphs of the variables are shown in the middle and the correlation is in the top right. 

The number of asterisks above the correlation value indicate whether the corresponoding *p*-value for the Pearson *r* coefficient is below an alpha of (0.05 = *, 0.01 = **, 0.001 = ***). 

The magnitude of the correlation indicates a measures of strength of a linear relationships between the variables. For example, we see that this is a positive linear correlation between temperatue and amount of ice cream sold, where high temperatures are assocaited with a high amount of ice cream sold. Like wise low temperatures are associated with low amount of ice cream being sold. This can be visually understood by looking at the scatterplot and seeing that dots that are low on temperature and also low on ice cream sold. 

##6. Identifying outliers##
6.1. The reason why this chart is usefull is that it can allow for you graph the variables, potentially seeing something that might not show up when only viewing the descriptive statistics. 

6.2. For example, load in the next dataset and calculate the mean and skewness of the variables. 
**ANSWER:** 
```{r}
#hide skewed <- read.csv("skewedicecream.csv")
#
skewed <- read.csv("c:/users/gcook/desktop/Psyc109/skewedicecream.csv")
str(skewed)
#Calculate the mean of both variables
mean(skewed$Temp); mean(skewed$IceCreamSold); 
#Calculate the skewness and kurtosis of both variables
skewness(skewed$Temp); kurtosis(skewed$Temp)
skewness(skewed$IceCreamSold); kurtosis(skewed$IceCreamSold)  
```

6.3. After loading in the new dataset, use chart.Correlation() to chart the correlations of variables in the skewed data frame.
**ANSWER:**
```{r, message = FALSE, warning = FALSE}
#hide
chart.Correlation(skewed)
```

6.4. As before, the chart.Correlation() function will create a chart with all of the variables in the data frame unless you exclude some. 
```{r, message = FALSE, warning = FALSE}
chart.Correlation(skewed[ , -1])
```

6.5. As you can see the magnitude of the correlation has decreased and the relationship between the variables has been altered greatly when the one score that is very different from the others is part of the data set. You may also notice that the graph for IceCreamSold is not as clean as it was before. This is what happens when you do not have a lot of data points and when one of those data points is an outlier. Outliers can greatly influence the relationship between two variables, as well as the distribution of one of your variables. In this case not a lot of ice-cream was sold on a very hot day. When running a study it would be very possible to collect data on a day like that. The next few steps will go over how to determine if the data point is an outlier. 


##7. Removing Outliers: Something important to consider##
7.1 Now we will determine if the score might be considered and outlier by graphing out the correlation and determine if a specific data point is very different from the others. In the previous graph there is a data point in the far left corner that is high on temperature and low on ice cream sold. It appears that this data value is greatly influencing the correlation, bringing it down to a .40. First we view the data set and attempt to determine which data point is the outlier.

```{r}
#Let's view the skewed dataset
View(skewed)
```

7.2. Luckily, because of the small dataset, data point 13 can be identified as the outlier. If we determine a score in an outlier, we can simply make the score a missing value, or not available (NA). In order to do this, we can modify the Temp values to replace the scores with NA by telling R to take the skewed$Temp variable and check to see if the Id value is equal to (==). The double equal evaluates the Id number to see if it matches the value. In the example below, we use a vector c(13) rather than simply using a specfic of 13 because the vector would allow us to evaluate matches for more than one Id number (e.g., c(1,2,13)).

To remove this variable we will create an outlier object which can be used to remove all the outliers that we identify. 

```{r}
skewed$Temp[skewed$Id == c(13)] <- NA # or more than one Id number if you have more than 1 outlier 
View(skewed) # notice how the value is not NA for participant 13
```

7.3. Now that you have made some values NA, you can use a function that selects variables that has only the x and y data pairs. The complete.cases() function will allow you to select only the complete x and y cases (not any row with NA). We will also need to create a small data frame that contains this subset of data so that we can run the correlation; the subset() function makes this easier to so. The subest() function also allows you to specifty the variables you wish to select for your subset. 

7.4. The following code is using subset() to create a subset of data. The first argument is the original data that you want to subset; in this case skewed. The second argument tells R to use only the compete cases of the skewed data frame, and the third argument selects your variable(s) of interest; the c() is used to combine the variables of interest. The new data frame is assigned to an object named notskewed.

Run the function on the skewed data frame and assign the new data frame to a new object (e.g., notskewed). If you look at the contents of your new data frame, you will see that you have a smaller data frame now. 

```{r}
notskewed <- subset(skewed, 
                    complete.cases(skewed), 
                    select = c(Temp, IceCreamSold))

# Take a look at the new data frame
notskewed
```

7.5. Now, in order to calculate the correlation and see the chart as before, we need to apply the functions to the new data frame. 

```{r, message = FALSE, warning = FALSE}
notskewedr <- cor(notskewed$Temp, notskewed$IceCreamSold)
notskewedr

chart.Correlation(notskewed)
```

7.5. As you can now see, removing the data point (outlier) has greatly increased the correlation and reveals the strong relationship between Temperature and Ice Cream sold. If there was more than out outlier, both can be placed in the Outlier object. 


##8. Try it yourself!##
8.1. Load the "Book.csv" from your working directory into an object named Book and look at its structure.
**ANSWER:**
``` {r}
# Load the csv
#hide Book <- read.csv("Book.csv")
Book <- read.csv("C:/users/gcook/desktop/Psyc109/Book.csv")
```


8.2. Now examine the names of the variables, mean, median, skewness, and kurtosis of the relevant variables. 
**ANSWER:**
```{r}
# Names 
str(Book); names(Book)
# Mean
mean(Book$BOOKS); mean(Book$ATTEND); mean(Book$GRADE)
# Median
median(Book$BOOKS); median(Book$ATTEND); median(Book$GRADE)
# Skewness
skewness(Book$BOOKS); skewness(Book$ATTEND); skewness(Book$GRADE)
# kurtosis
kurtosis(Book$BOOKS); kurtosis(Book$ATTEND); kurtosis(Book$GRADE)
```


8.3. Plot out a correlation using xyplot() to see if there appears to be a linear relationship between the two variables of your choice
**ANSWER:**
```{r}
xyplot(GRADE ~ ATTEND, 
       data = Book,  
       main = "Grades as a Function of Attendance", 
       xlab = "Attendance", 
       ylab = "Grades")
```


8.4. Now using the chart.Correlation() function, chart out the data set.
**ANSWER:**
```{r, message = FALSE, warning = FALSE}
#hide 
chart.Correlation(Book)
```
Book


8.5. Now using the chart.Correlation() function, chart out the data set, by removing the ID variable. Make sure you spell the variable name correctly; use str() to help you if necessary.
**ANSWER:**
```{r, message = FALSE, warning = FALSE}
#hide
chart.Correlation(Book[, -1])
```


8.6. Get a subset of the data that does not include the Id variable. 
**ANSWER:**
```{r}
mysubset <- subset(Book, 
                   complete.cases(Book), 
                   select = c(BOOKS, ATTEND, GRADE))
# take a look
mysubset
```

8.7. For your correlation, get the exact *p*-value.
**ANSWER:** The p-value for GRADE and ATTEND is 0.001; r=.48
```{r}
cor.test(Book$GRADE, Book$ATTEND)
```

8.8. How many of the correlations are significant (test at alpha = .05) and how did you make that decision? 
**ANSWER:** 3 are. The asterisks indicate a significant relationship for all bivariate correlations at the .01 level.



8.9. Describe how strong the correlations are.
**ANSWER:** The correlation between books and attendance is .44; between books and grades is .49; and between attendance and grades is .48. All three correlations are moderate in strenth. The coefficients of determination are all approximately .2 (20%)
