---
title: "Regression - Part 1"
author: "replace with partner names"
date: "replace with date"
output: html_document
---

##Part A##
#General Questions#

1. **QUESTION:** Describe the purpose of using bivariate-linear regression. Include in your answer the type of data needed for linear regression analyses.

**ANSWER:**




2. **QUESTION:** Identify the two coefficient values that are needed for a regression analysis and describe what they are.

**ANSWER:**




3. **QUESTION:** The regression model is compared to another model, a simpler one. Identify what that model is and describe what the error in that model represents.  

**ANSWER:**



4. **QUESTION:** If the SStotal is 2000 and the SSresidual are 1000. What is the proportion of variance in the criterion is accounted for by the predictor? 

**ANSWER:**




##Part B##

##Before you begin##

This homework exercise involves having you answer some questions, write some code, and create a nice HTML file with your results. When asked different questions, simply either type your coded or written responses after the ANSWER message. When asked to write code to complete sections, type your code in the empty code blocks that follow the ANSWER message (between the back ticks). After adding that code, you must make sure that it will execute. So remember to read the content and run each line of your code as you write it so that you know it executes correctly. If your code does not execute, then RMarkdown won't know what you are telling it to do and your HTML file will not be produced. Also, don't create your HTML file until you finish and know that all of your code works correctly.

Besides lecture notes, some videos help with understanding R output. I recommend watching:
- https://www.youtube.com/watch?v=eTZ4VUZHzxw
- https://www.youtube.com/watch?v=q1RD5ECsSB0

##1. Installing and using libraries in RStudio##

1.1. Use the RStudio interface to install packages/libraries. Go to the Tools option and select Install Packages. Type the package name(s) correctly using the proper letter casing. Also, make sure that you check the box to Install Dependencies. Do not install with code.

- car
- lattice
- QuantPsyc
- Mac users, most of you should already have XQuartz installed from the first R class, but please make sure that you have it downloaded and installed in your Applications folder. See the original download instruction file if you need help.  
- Do you still have problems with the ncvTest() or durbinWatsonTest() functions from the car library? If so, the errors did not resolve themselves by installing a new version of R, do the following. Remove the # below and execute the following code. After doing so, add back the #. Please let me know if this solved your error. 

```{r, echo=FALSE}
#install.packages("nlme", repos="http://cran.r-project.org")
```



1.2. New functions used for this assignment: 

- durbinWatsonTest() for testing assumptions; car library 
- lm() for evaluating linear models; built-in library
- lm.beta() for calculating standarized regression coefficients; QuantPsyc library
- ncvTest() for testing assumptions; car library 
- par() for changing graphing parameters; built-in library
- plot() for examining lm regression plots; built-in library
- round() for rounding values; built-in library
- summary() for obtaining a summary of the model components
- shapiro.test() for testing normality; built-in stats library
- sqrt() for transforming data; built-in library


## 2. Loading libraries ##
Use the library() function to load the following libraries: car, lattice, QuantPsyc

**ANSWER:** 
```{r, message = FALSE, warning = FALSE}

```

## 3. Checking your working directory ##
Always make sure that your working directory points to Psyc109 on your desktop. 
```{r}
#getwd()
#setwd("C:/users/gcook/desktop/Psyc109/")

```

##Part B##
##1. General Linear Regression##

1.1. Linear regression is an approach for modeling the linear relationship between a criterion y and one or more predictor/explanatory variables (or independent variables). 

1.2. There are two common forms of *linear regression*:

1.2.1. *Simple-linear regression* is used when the goal is to use one predictor variable in the linear regression equation to predict a criterion variable (Ex: formula: criterion  ~ predictor). This will produce a linear model of your criterion as-a-function-of your predictor. 

1.2.2. *Multiple-linear regression* is used to indicate there is more than one predictor in the linear regression equation (Ex: formula: criterion ~ predictor1 + predictor2 + predictor3).

Both of these forms of regression analysis have assumptions that should be met in order to interpret the data. The assumptions are addressed in sections below.


##2. Simple (bivariate) Linear Regression##

2.1. Understanding Simple Bivariate Linear Regression.

For a simple-linear regression there is:

  - One predictor in the equation
  
  - An unstandardized regression coefficient that represents 
    the marginal relationship between the criterion and the 
    predictor variables.
    
  - The standardized regression coefficient equals is the correlation value.
    
2.2. Reading in the data

One of the first steps in completing a simple linear regression is to plot your data on a scatter plot. For that we are going to need to bring some data into our workspace. We will read in a data file named "Album Sales.csv" and assign it to a data frame object named ALBUM. Examine the structure once you are done. 

```{r}
ALBUM <- read.csv("Album Sales.csv") # Read in csv file and assign as data frame object
str(ALBUM) 
```


As usual, the first thing to do with any new data set is to view the data frame. You will notice that the str() of the data frame indicates that there are 200 observations of data (rows) and 3 variables (columns). The varibles/columns are:

- *Id* represents the unique Id for a rock band album 
- *Sales* represents album sales (in thousands of pounds)
- *Ads* represents the amount of money spent on the advertising budget (in thousands of pounds)
- *Airplay* represents the number of times an album is played on the radio during the week prior to release
- *Attract* represents the attractiveness of the band members [on a scale from 0 (not attractive at all to 10 (most attractive as possible)]


```{r}
View(ALBUM)
```



##3. Producing a correlation matrix## 

3.1. As we have done before, you can specify all of the variable pairs you want to correlate, but if there are multiple pairs, there is an easier way than doing this repeatedly for each x-y pair. You can simply use the cor() function on the ALBUM object itself (e.g., cor(ALBUM)) rather than specify the variable names as you have done before (e.g., cor(ALBUM$sales, ALBUM$Ads)).

```{r}
cor(ALBUM)
```

3.2. However, the decimal points in the correlation matrix are a little busy. We can use the built-in round() function to round the results by specifying the number of decimals to round to. With R, you can nest functions inside functions, so we will nest cor() inside round() in order to round the correlation values for all the variable pairs. Remember, cor() using Pearson's r by default.

```{r}
round(cor(ALBUM), 3)  # rounding makes it easier to look at

round(cor(ALBUM[, -1]), 3)  # because ID is meaningless for our correlations, we can remove column 3 of ALBUM because ID is the third column in the ALBUM data frame
```


```{r}
pairs(ALBUM[, -1]) # plot the pairs of data  
```

##4. Define a linear model##

4.1. We will define a linear model using the built-in lm() function by specifying arguments of the function. The general form of the model is as follows:

mymodel <- lm(formula = criterion ~ predictor, 
              data = mydataframe, 
              na.action = some action)

- *mymodel* is an object that contains information about the linear model; summary() will be useful for inspecting the model
- *criterion* is the predicted variable 
- *predictor(s)* are the variable used to predict scores on the criterion
- *data* is the name of the data frame object
- *na.action* (optional) allows you to specify a complete data set if you wish to drop out anyone with missing values; this in an alternative to subetting when using regression models

4.2. You can specify the predictor and the criterion, "y as a function of x", as well as the data frame. 

```{r}
# Using the dataframe$variable approach
ALBUM.model <- lm(formula = ALBUM$Sales ~ ALBUM$Ads)

# Specifying the data object makes things easier
ALBUM.model <- lm(formula = Sales ~ Ads, data = ALBUM)

# If there are missing values, we can remove them
ALBUM.model <- lm(formula = Sales ~ Ads, data = ALBUM, na.action = na.exclude)
```


##5. Examining and interpreting the simple bivariate linear model##

5.1. If you only wanted to examine the y-intercept and the regression coefficient, you can simply ask to see them by calling the model object.

```{r}
ALBUM.model  # Displays only the coefficients from the model
```

Or you can summarize other details of the linear model by using the built-in summary() function. 

```{r}
summary(ALBUM.model)
```

5.2. Interpreting Overall Model Fit

5.2.1. The *Multiple R-squared* represents the proportion of variance in the criterion that is accounter for by the predictor. The square root of this is Pearson's correlation coefficient. Because there is only one predictor, this multiple R-squared is really just r squared. Because the estimates are based on samples, they are not perfect for making inferences about the populations you really care about. The adjusted R-squared is adjusted for shrinkage, or loss of predictive power that occurs when using sample data to predict population data; the more error in the regression model (e.g., residuals), the more the adjusted R-squared will differ from the unadjusted R-squared. 

5.2.2. The *F-statistic* represents the ANOVA test value of the ratio test of the model. A statistically significant result indicates that the regression model explains the data better than model based on the mean of album sales (mean-based model). If the *p*-value is less than your alpha level, then the regression model fits the data well. This ANOVA test only tells you whether the overall model fits the data; it does not examine the components of the model.

5.3. Interpreting Specific Model Parameters (coefficents part of the output table)

The coeffiencts report is displayed in 2 rows and 4 columns. 

5.3.1. The *Estimate* column displays the two estimated coeffient values y-intercept (b0) and the regression coefficient (b1). The *Std. Error* column displays the error in estimating the coefficients. This error indicates the amount of error in coefficients; small error is good and would indicate that b0 and b1 would not vary as much from sample to sample. The *t* value column displays the value of the *t*-statistic, which simply tests whether the coeffienct differs from 0, which can be inferred also from the Pr(>|t|) column, which is the *p*-value for the t-test.

   5.3.1.1 The first row corresponds to the y-intercept (b0). The y-intercept predicts the 
   number of sales when no money is spent on advertisements. A *p*-value less than alpha 
   means the y-intercept differs from 0. Mathematically, this can be less than 0. 

   5.3.1.2. The second row corresponds to the regression coefficient/slope (b1). This 
   represents the increase or decrease (depending on whether the correlation is positive or 
   negative) in the number of album sales for each unit change in money spent on 
   advertising. A *p*-value less than alpha means the slope differs from 0. When there are 
   multiple predictor variables, there will be slopes corresponding the each predictor and 
   the criterion. 
   

##6. Examine Model Assumptions##

6.1. Model assumptions can and should be inspected both visually and statistically. Passing the model to the built-in plot() function will return a set of plots for inspecting the model assumptions. Other specialize functions from other libraries can also help inspect a model.

*Assumptions and things to look for:*

6.1.1. The predictor(s) must be either quantitative or categorical (2 categories); the criterion must be quantitative and continuous. The criterion should also be unbounded or unconstrained; for example if a scale that ranges from 1 to 9 is used to measure the criterion and responses only fall between say 3 and 7, the data are constrained, or bounded. This can only be checke by evaluating the range using the built-in range() function. 

```{r}
range(ALBUM$Sales)
```


6.1.2. Predictors should not be restricted and not have variances that are near 0. Range restriction in general is always problematic with regression. We can check variance. It's often good to determine if your data range in ways similar to  previous research. If you have a much smaller range, you should investigate reasons why. Otherwise, your correlations and regression coefficients may not map on well with previous research.

```{r}
mean(ALBUM$Ads, na.rm = TRUE)
range(ALBUM$Ads)
var(ALBUM$Ads, na.rm = TRUE) # does not look near 0
sd(ALBUM$Ads, na.rm = TRUE)  # does not look near 0, so we have variability

library(lattice)  # we need this for histogram()
histogram(~ ALBUM$Ads)  # notice that the predictor is skewed positively
```


The predictor (advertisements) appears to be skewed positively. We can test whether it is statistically different from a normal distribution by using the Shapiro-Wilk test. The shapiro.test() function is part of the built-in stats library so you don't need to load anything special. The D'Agostino test will test for skewness only, not normality, but is part of the moments library so you would need to load it if you wanted to test for skewness. The Shapiro-Wilk test value is W (kind of like the z-test value is Z). In order to determine if the distribution is not normal in shape, compare the *p*-value that corresponds to Shapiro-Wilk test value (W) to your desired alpha. In this example, we can see that our distribution of money spent on advertisements is not normal if alpha = .05. 

```{r}
shapiro.test(ALBUM$Ads)
```

When you have skewness, you should deal address that problem. You can mathematically transform your data from raw values to another metric. However, you need to be careful when interpreting statistics using transformed data because you are no longer dealing with actual raw data. Although we will not deal with transformations much in this class, you can find some discussions online, for example at https://en.wikipedia.org/wiki/Power_transform and http://pareonline.net/getvn.asp?v=8&n=6

For example, we can create a new variable in our data frame that represents the square root of the Ads variable. We will use the built-in sqrt() function. There are other transformations that can fix your data if this one doesn't work. We won't get into all of them here.

```{r}
ALBUM$Ads.sqrt <- sqrt(ALBUM$Ads)
str(ALBUM)  # check the structure to make sure the variable is there.

histogram(~ ALBUM$Ads.sqrt)  # looks better than before
qqnorm(ALBUM$Ads.sqrt); qqline(ALBUM$Ads.sqrt)  # the tails of the distribution don't hug the identity line; could be a problem.

shapiro.test(ALBUM$Ads.sqrt) # but these transformed data still have a distribution different from normal, see p-value
```


6.1.3. Using the plot() function on the model object that you created will produce model plots for examining the assumptions of the model. You can view each plot separately or produce a 2x2 chart that displays them together. However, grouping the plots will reduce their size for visual inspection. 

```{r}
par(mfrow = c(2,2))  # adjust the plots in order to plot the 4 graphs on one chart with 2 rows, 2 columns.
plot(ALBUM.model)
par(mfrow = c(1,1))  # return to 1 row, 1 column
```


6.1.4. The relationship between the criterion and the predictor(s) is linear rather than curvilinear. Plot #1 is the residual plot for which errors (y-axis)) plotted as a function of predicted/fitted values (x-axis); a straight horizontal line on this plot would indicate that the linearity assumption is met; ff a curvilinear model is better than a linear model, then the errors would not be distributed normally, but instead be greater in some parts of the plot than in others. 


6.1.5 Homoscedasticity of the residuals. The variance in the residuals, or error in prediction, should be the same across the values of the prectors. Homoscedasticity is also referred to as constant variance. Non-constant variance in the residuals is referred to as heteroscedasticity. We can examine homoscedasticity visually and statistically.

Plot #1 can also provide information about the variance in the residuals across levels of the predictor. Based on this example, you can see that there is more variability of the residuals (y-axis) on the left side of the plot than at the right side; thus the variability in residuals does not seem to be constant. If the variance is not constant, then we may have issues with residuals not being distributed normally (see next assumption).

If we want to statisically test for constant variance, we can use the ncvTest() function from the car library (install if you haven't yet). The ncv stands for non-constant variance. We want constant variance. The ncvTest() function provides a Chi-Square test value and a corresponding *p*-value for the test. If p is less than or equal to alpha, you have evidence that that you have non-constant variance, or heteroscedasticity. Unfortunately, we have heteroscedasticity. 

```{r}
library(car)
ncvTest(ALBUM.model) # the p-value is much lower than .05
```


6.1.6. Residuals (prediction errors) are distributed normally. If errors are random and the size of the prediction error does not depend on whether the score on the predictor is low or high, then those errors should be distributed normally. Plot #2 will provide detail about the normality of the residuals. Plot #2 is a quantile-quantile plot used to determine normality of errors; if points fall along the identity line, the errors are normally distributed. In this example, you can see that points on the ends of the distribution do not quite fall along the identify line. We could also test this assumption by using the sharipo.test() function on the residuals themselves. 

We can take a look at a histogram or because the residuals are actually stored as part of the information returned from running the lm() function, we can extract those residuals from the model object. Lukily, they are named appropriately as residuals. Remember, the model was ALBUM.model; the residuals do not depart from a normal distribution. 

```{r}
histogram(~ ALBUM.model$residuals) # take a look; looks fairly normal to the eye

shapiro.test(ALBUM.model$residuals) # test for normality; p > alpha so the residuals do not depart far enough from normality to say we have violated this assumption.
```


6.1.7. No multicolinearity; predictors should not be strongly correlated with each other. When you have more than one predictor, you have to examine for multicolinearity. collinearity will mask the true relationship among variables. Having multicolinearity makes the beta values (y-intercept and slope) unreliable and untrustworthy when making inferences from samples to populations. Generally speaking, there will be more error in beta values for greater amounts of multicolinearity. There are other influences too, but we won't address them here. You should just know to determine if you meet this assumption. Because we only have one predictor in this example, we do not have to test for this assumption. 

If you had multiple predictors, you would want to test for multicollinearity. One easy way to test this is to examine Variance Inflation Factors (VIFs) using the vif() function from the car library. The function returns a VIF value for each predictor. If the square root of the VIF is greater than 2, this predictor would be eliminated from your model. 

An example if you have multiple predictors:
```{r}
# vif(mymodel) # variance inflation factors for the model
# sqrt(vif(mymodel)) # the square root of them
```

6.1.7. Independence of errors. For any two obervations in the data, the errors are not related. This is usually not a problem as long as your sample is selected at random and you don't allow people to participate in your study, allow them to talk to other participants, etc. in ways that affect how two people (or objects) respond. This independence assumption can be tested using the Durbin-Watson test using the durbinWatsonTest() function in the car library. The Durbin-Watson test will produce a D-W statistic value that will be large if independence is violated (and you have dependence). Compare p to alpha. In this example, we do not see violations of independence, which is good.

```{r}
durbinWatsonTest(ALBUM.model)
```



##7. Determining Outliers##

When building a linear model, it is always important to determine if there are any outliers in your data. Any interpretation of your regression model is useful to the extent that you don't have any outliers in your data set that inlfuence your correlation or the predictive power either by making it stronger than it actually is or weakening it. 

7.1. Cook's Distance measure is easy to use and actually tests how each data point affect your regression model by providing a Cook's distance value. It combines the information of leverage (how a point changes your slope) and residual of the observation (the prediction error). If a data point influences your model, it will let you know. We can flag data points as outliers (which would would want to remove) if they have large Cook's D values. 

7.2. We will be using Cook's distance to determine the influence that a data point has on your linear model. Cook and Weisberg (1982) suggest that Cook's D values > 1 are outliers. The code for doing so is a little complicated, but it will plot Cook's D values for the model as a function of rows in the data frame and it flag the cases/rows with the highest Cook's D values. Just because they are flagged, this does not mean you need to remove these rows from the data frame.  


```{r}
plot(ALBUM.model, which = 4, cook.levels = cutoff) # check to see if D values are greater than 1
```


7.3. BONUS: The influencePlot() function from the car library can be useful to flag individual influence points. The plot displays *hat values* or *leverage* points. The size of the circle in the plot is related to the amount that the data point influences (or has leverage on your model). If you specify the id.method argument as "noteworthy" you can find out which row in your data frame is of concern because the row number will appear by a circle. You do not want to include data points that influence your model. You can remove them from your data set by creating a subset data frame to manipulate.    

Leverage or hat values near 0 indicate little or no influence on your model, whereas those near 1 indicate complete influence. Belsley et al. (1980) suggest that values greater than 2 times the average are typically problematic. 

```{r}
influencePlot(ALBUM.model, 
              id.method = "noteworthy", 
              main = "Influence Plot", 
              sub = "Circle size is proportial to Cook's Distance")
```


In this graph the x-axis refers to the data point while the y-axis refers to cook's distance, telling us how influential each data point is. It may appear data points 6 and 7 are the most influential but the y-axis is very small with the highest data point being slightly above .6 cooks distance. We can determine that our original data set did not have any extremely influence single data points. If any data points are above a 1 on Cook's distance, it is up to you to remove them using previous methods of removing outliers. 


##8. Multiple Linear Regression##

Multiple regression focuses on using more than one predictor to predict a criterion. Because predictors need to be correlated with the criterion to be useful, examining the correlations of the variables is useful. 

```{r}
round(cor(ALBUM[, -1]), 3)  # remove column 1 of ALBUM because ID is the first column in the data frame
```

8.1. **QUESTION:** What is the correlation value between Sales and Airplay?

*ANSWER:* 


8.2. **QUESTION:** Use the cor.test() function to test whether the correlation between Sales and Airplay is statistically different from 0; use alpha = .05. Provide the *p*-value in your answer. 

*ANSWER:* 


8.3. **QUESTION:** Use the cor.test() function to examine the correlations between the 3 predictors. Are any pairs of predictors correlated significantly?; use alpha = .05?

*ANSWER:*

```{r}
 
```

You might have notice that two predictors are correlated significantly. In order to determine if a predictor is problematic to your regression model, you would want to check for multicollinearity. 

##9. Define a multiple-regression model##

9.1. There are different types of models to create, but the focus will be on simple additive models. Taking a *hierarchical regression* approach, create the model by entering predictors in the order of importance or theoretical contribution. 

mymodel <- lm(formula = criterion ~ predictor1 + predictor3 + predictor3, 
              data = mydataframe, 
              na.action = some action)
              

9.2. Specify the multiple-regression model using multiple predictors. Use the lm() function to build a linear model to predict album sales from Ads, Airplay, and Attraction of band members and name the regression model ALBUM.model2 so that you can distinguish this from the bivariate model.

```{r}

```

9.3. **QUESTION:** Calculate Cook's D for the model to determine if there are any outliers that will influence model. Replace mymodel in the code below with the model object you just created. 

```{r}
plot(mymodel, which = 4, cook.levels = cutoff) 
```


9.4. Obtain a summary of the model using the summary() function.

```{r}
 
```

**QUESTION:** Use the F-statisic and the *p*-value to help determine whether the model is statistically better than a mean-based model. Be careful with reading the scientific notation.

*ANSWER:*  



**QUESTION:** What proportion of the variance in Sales does the model account for? Is this greater than was accounted for by the simple model?

*ANSWER:*  



**QUESTION:** Are the regression coefficients (bs, slopes) for the 3 predictors significantly different from a slope of 0?

*ANSWER:*  



**QUESTION:** Which predictor has the steepest standardized slope value? 

*ANSWER:*  



9.5. Standardized beta coefficients adjust the slope estimate as a function of the standard error of the estimate. This is one way to compare the regression coefficients of different predictors. The lm.beta() function from the QuantPsyc library will standarize the values in order to compare them easily. Here is an example:

```{r}
library(QuantPsyc)
ALBUM.model2.betas <- lm.beta(ALBUM.model2)  # assign the standardized betas to an object 
ALBUM.model2.betas # take a look at the object to compare the betas
```

**QUESTION:** Now that the slopes have been standarized, you can examine the change in the criterion for each standard deviation change in each predictor. Which predictor has the steepest standardized slope value (biggest bang for your buck)? 

*ANSWER:*  


9.6. Use the plot() function to generate the diagnostic plots for examining the model. 

```{r}

```


**QUESTION:** Based on that plot, does the model appear to be a linear model? Explain how you arrived at that decision. 




9.7. **QUESTION:**  Use the appropriate function to test for the homoscedasticity/constant variance assumption. Is the assumption met? Why or why not?

```{r}
#hide ncvTest(ALBUM.model2) 
```


9.8. **QUESTION:** Use the appropriate function to test to determine whether the residuals of the model are distributed normally.

```{r}
#
```

9.9. **QUESTION:**  Use the appropriate function to test for independence of errors. Is the assumption met? Why or why not?

```{r}
# hide
durbinWatsonTest(ALBUM.model)
```






##10. Comparing Models##

Assuming that you have met regression assumptions for different models, how can you determine which is better than another? You can examine the R-squared values to see the proportion of variance in the criterion that accounted for one model versus another. Obviously, the greater that R-squared value, the better the predictive ability for that model. One way to compare models is to compare them with an ANOVA test using the anova() function. In the example below, if you comp

```{r}
anova(ALBUM.model, ALBUM.model2)
```









skip Standardized residuals are used to compare slopes of multiple predictors. Standardization adjusts each slope according to the standard  

Multicolinearity also makes identifying the unique predictive power for each predictor very complicated. Whereas you might be able to determine overall model fit, you will not be able to accurately determine individual predictor contribution because its will be shared another predictor. 

##END##



You will notice that the variable names are X1, X2, X3, and X4.  Below is a description of the variables for later interpretation. 

  Antelope Data Set:

    ID = The ID standings for the year number that the data was collected
    X1 = spring fawn count/100
    X2 = size of adult antelope population/100
    X3 = annual precipitation (inches)
    X4 = winter severity index (1=mild,5=severe)
    
Again check the structure of your data frame and its contents. Pay attention to the names and spelling of the variables. As in the last assignment use the summary() function from the psych library to examine the means of the two variables. For the first part of the homework you only need to examine the first two variables. 

**ANSWER:**
```{r}
#Check the variable names
str(Antelope)

#Calculate the and median mean for both variables; compare them and ask yourself if their values inform you about skew. Remember you need to specify the data frame object in order to tell R to obtain some of the data frame's contents. 

summary(Antelope$X1)
summary(Antelope$X2)



```



Use the appropriate functions from the moments library to calculate the skewness and kurtosis of both variables. Consider whether the variables are distributed normally.

**ANSWER:**
```{r}

skewness(Antelope$X1)
skewness(Antelope$X2)

kurtosis(Antelope$X1)
kurtosis(Antelope$X2)
```

?shapiro.test
library(nortest)
??nortest

##3. Building a linear model##

After examining our data we are going to want to determine the line that "best fits" a seemingly linear relationship between our two variables. The basic way of writing formulas in R is dependent ~ independent. The tilde can be interpreted as "regressed on" or "predicted by". The second most important component for computing basic regression in R is the actual function you need for it: lm(...), which stands for "linear model".


```{r}
## Builds a linear model called regression
cor.test(Antelope$X1, Antelope$X2)
Antelope
regression <- lm(formula = X1 ~ X2, data = Antelope)

## shows us the intercept and slope for the line that best fits the relationship
regression

summary(regression)
plot(regression)

durbinWatsonTest(regression)

ncvTest(regression)
spreadLevelPlot(regression)

multreg <- lm(formula = X1 ~ X2 + X3, data = Antelope)
cloud(X1 ~ X2 * X3, main="3D Scatterplot by Cylinders", data = Antelope)

vif(multreg)
summary(multreg)

plot(multreg)

durbinWatsonTest(multreg)



library(gvlma)
cor.test(Antelope$X2, Antelope$X3)
gvmodel <- gvlma(regression) 
summary(gvmodel)



library("car")
outlierTest(regression)


summary(regression)
plot(regression$residuals)

plot(resid(regression))
```

In this case the slope of the line that best fits the relationship of X1 and X2 has a slope of.4975 and an intercept of -1.6791. It would be nice to view the scatterplot of the data and this line together on the same graph. 

One might use the commands:

```{r}
## This creates a plot of the relationship and places the linear relationship line on the plot
plot(Antelope$X1 ~ Antelope$X2)
abline(regression)

library("lattice")
xyplot(X1 ~ X2, data = Antelope)


```


What this tells us is that for a year with an adult antelope population(X2) of 0, a -1.6791 score on fawns is predicted in the spring(X1). This is often also called a conditional expectation because it is the value you expect for the dependent variable under the condition that the independent variable is 0. Put a bit more formally: E(Y|X=0) = -1.6791. The regression weight is the predicted difference between the years that differ in adult population by 1 point in the data.

##4. Determining Outliers 

4.1. Cooks Distance

When building a linear model, it is always important to determine if there are any outliers in your data. We will be using Cook's distance to determine the influence that a data point has on your linear model. For this linear model we will consider any score with a Cook's distance value greater than 1 an outlier. 

Below we will examine the cooks distance for our linear model:

```{r}
#Let's view the Cook's distance for the dataset Antelope

plot(regression, which = 4:4, bin = 1)

```

In this graph the x-axis refers to the data point while the y-axis refers to cook's distance, telling us how influential each data point is. It may appear data points 6 and 7 are the most influential but the y-axis is very small with the highest data point being slightly above .6 cooks distance. We can determine that our original data set did not have any extremely influence single data points. If any data points are above a 1 on Cook's distance, it is up to you to remove them using previous methods of removing outliers. 


##5. The assumptions of a linear model

After removing outliers from our data set it is important that we test the assumptions of a linear model. While we may already know that some of the assumptions have been satisfied, for the purpose of this test we are going to check them all again in this section

5.1. Linear relationship.    

The best way to determine if there is a linear relationship between the variables is to graph it out on a scatterplot. 

Graph out the linear relationship below:

**Answer** 

```{r}



```


**Answer**
Is there a linear relationship between the variables? 
Answer:


5.2. Multivariate normality.

Secondly, the linear-regression analysis requires all variables to be multivariate normal.  This assumption can best be checked with a histogram and a fitted normal curve or a Q-Q-Plot. A Q-Q plot is a scatterplot created by plotting two sets of quantiles against one another. Quantiles as cut points that divide the data into equal-sized groups; quartiles have 3 cut points which split the data into 4 quarters. If both sets of quantiles came from the same distribution, we should see the points forming a line that's roughly straight. If you have roughly a straight line on a Q-Q plot then most will argue that the assumption of multivariate normality has been met, although this is not a full proof method

Below we will plot out a Q-Q plot:

```{r}

## This will plot out a 4 different charts - The top right one is the Q-Q plot. 
par(mfrow = c(2, 2))
plot(regression)
par(mfrow = c(1, 1)) # return the parameters to 1 row, 1 column or all graphs will be grouped

```

** Answers**
Is the line on the Q-Q plot a relatively straight line?
Answer:

**Answers**
Have we satisfied the assumption of Multivariate normality?
Answer: 

5.3. -No or little multicollinearity.

Multicollinearity exists when two or more of the predictors in a regression model are moderately or highly correlated. Unfortunately, when it exists, it can wreak havoc on our analysis and thereby limit the research conclusions we can draw. Luckily bivariate regression only has one predictor which means multicollinearity cannot be a problem. Problems in multicollinearty will arise when we move on to multiple linear regression. 


5.4. Homoscedasticity.

Homoscedasticity refers to the assumption that that the dependent variable exhibits similar amounts of variance across the range of values for an independent variable. Homoscedacity means that when you plot the individual prediction error against the predicted value, the variance of the error predicted value should be constant. To check the assumptions of homoscedacity we will examine the "Residuals vs Fitted" graph that we previously plotted. 


The plot of residuals versus fitted values is useful for checking the assumption of linearity and homoscedasticity. If the model does not meet the linear model assumption, we would expect to see residuals that are very large (big positive value or big negative value). To assess the assumption of linearity we want to ensure that the residuals are not too far away from 0 (standardized values less than -2 or greater than 2 are deemed problematic). To assess if the homoscedasticity assumption is met we look to make sure that there is no pattern in the residuals and that they are equally spread around the y = 0 line. Luckily for us it appears that our data is spread around the line, with no points over .3.

**Answers**
Have we violated the assumptions of Homoscedacity?
Answer:


##6. A Summary of the linear model

6.1. Linear Model Summary / Significant Predictors 

Like for most R objects, the summary-function shows the most important information in a nice table.


```{r}
## This will show a summary of the linear model
summary(regression)

```


What is the most important information in this table? Most probably would say the coefficients-section, which contains the parameter estimates and their corresponding t-tests. This shows us that X1(adult antelope population) is significantly related to X2(spring fawn count). This can be seen in the low p-value. If a predictor is not shown to be significant in this table than we cannot predict scores based off of the non-significant variable. 


6.2. The R-Squared 

The second most important line is the one containing the Multiple R-squared which represents the amount of variance accounted for in the dependent variable by the predictors. In this case over 88% of the variability in variable X1 is shared with the variability in the variable X2(spring fawn count). Due to the fact that the R^2 is the squared multiple correlation between the dependent and predictor we should be able to gain the R^2 by squaring the correlation between the two variables. To check this, we can use

```{r}

summary(regression)$r.squared
cor(Antelope$X1, Antelope$X2)^2

```



6.3. Predicting Scores

It is also important to point out that based off of the F-statistic and p-value at the bottom of the summary table we can determine that our model is significant and that X2 significantly predicts the score on X1. These predictions allow us to infer what value of 2 on the X2 variable would result in on X1. The coefficient of X2 is interpreted as the different in the predicted value in Y (in this case X1) for each one-unit difference in X2. However, if X2 were a categorical variable coded as 0 or 1, a one unit difference represents switching from one category to the other, then the coefficient of X2 would then be the average difference in Y(X1) between the category for which X2 = 0 (the reference group) and the category for which X2 = 1 (the comparison group). 

Now let us show this in R:

```{r}
# This is the coefficient for variable X2 and can be found in the coefficients table
X2Coeff <- .49752

#This is the intercept for the linear model
Intercept <- -1.67914

#Lets test our linear model when the variable X2 is 3
X2 <- 3

## Place it in our linear model (Y = X2 * Coefficient + Intercept)
PredictedScore <- X2 * X2Coeff + Intercept 

## Predicted Score
PredictedScore

```


** Answer **
The predicted score for X1 when X2 is 3 is:

It is important to mention that when more variables are added to the linear model that the coefficients will change. You will not have to worry about this until multiple regression but it is something to keep in mind. 


##7.  Do it yourself!

7.1. Load the "Alcohol.csv" from your working directory into an object named Alcohol and look at its structure.
**ANSWER:**
``` {r}
# Load the csv
#Hide
Alcohol <- read.csv("Alcohol.csv")
```


7.2. Now examine the names of the variables, mean, median, skewness, and kurtosis of the relevant variables. 
**ANSWER:**
```{r}
# Names 
str(Alcohol); names(Alcohol)
# Mean

# Median

# Skewness

# kurtosis

```


7.3. Make a linear model where the variable Sips predicts Tempo
**ANSWER:**
```{r}


```


7.4. Next plot out the relationship and place your linear model line on it. 
**ANSWER:**
```{r, message = FALSE, warning = FALSE}
#hide 

```
Book


7.5. Are any of the data points outliers?
**ANSWER:**
```{r, message = FALSE, warning = FALSE}


```


7.6 Are any of the assumptions violated?
**Answers:**
```{r}


```


7.7. Now view the summary of the model
**ANSWER:**
```{r}

```


7.8. Is sips a significant predictor of tempo?
**ANSWER:** 



7.9. How much variance is accounted for in tempo by sips?
**ANSWER:** 


7.10 Using only the cor() function, show how the r-square is achieved
```{r}



```




