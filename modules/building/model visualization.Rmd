---
title: "Model Visualization"
#author: "gcook"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
rm(list = ls(all.names = TRUE))      # remove objects in R

source("https://pastebin.com/raw/8mXH02yg")   # run and comment out before knitting
source("https://pastebin.com/raw/97NNTTzu")   # run to include in function definitions

# set the paths for project, script, and data dirs
proj_dir <- gsub("GCDS.*.Rmd", "GCDS", get_ActivePath())
proj_name = ""
r_dir    <- paste(proj_dir, "r", sep = "/")    # r subdir
data_dir <- paste(proj_dir, "data", sep = "/") # data subdir
if ( proj_name != "" & !dir.exists(paste(proj_dir, proj_name, sep = "/")) ) {
  # create project dir
  suppressWarnings(dir.create(paste(proj_dir, proj_name, sep = "/")))
  r_dir <- gsub("/r", paste0("/", proj_name, "/r"), r_dir)
  data_dir <- gsub("/data", paste0("/", proj_name, "/data"), data_dir)
  # create sub directories
  suppressWarnings(dir.create(r_dir))
  suppressWarnings(dir.create(data_dir)) }
```

```{r}
#https://easystats.github.io/modelbased/
```


```{r}
multi_models <- function(data, formula, nest) {

  m = data %>%
  nest_by(!!!nest) %>% # nest_by returns a list with column "data"
  mutate(mod = list(lm(formula, data = data))) %>%
  summarize(tidy(mod)) #%>% view()
  names(m)[1] = nest
  m %>%
    mutate(., across(where(is.numeric), round, digits = 4)) %>%
  return(m)
}

a = multi_models(data = mtcars, 
             formula = formula(mpg ~ drat + wt),
             nest = "cyl"
             )

view(a)

mydata <- data.frame( 
  hour     = factor(rep(1:24, each = 21)),
  price    = runif(504, min = -10, max = 125),
  wind     = runif(504, min = 0, max = 2500),
  temp     = runif(504, min = - 10, max = 25)
  )

# see https://stackoverflow.com/questions/22713325/fitting-several-regression-models-with-dplyr
mods = mtcars %>%
  dplyr::nest_by(cyl) %>%
  dplyr::mutate(
    mod = list(lm(mpg ~ drat + wt, data = data))) %>%
  dplyr::summarise(
    mod.out     = list(broom::tidy(mod)),
    glance.out  = list(broom::glance(mod)),
    augment.out = list(broom::augment(mod))
    ) %>%
  dplyr::ungroup()

```


# **Libraries**

```{r}
library(dplyr)
library(ggplot2)
library(parameters)
library(performance)
library(modelbased)
library(see)
```


# **Data**

From Slack #data, save "STROOP_agg.csv" to "GCDS/data". If you are not saving data files to the data directory, you likely have files of all sorts saved in different places. Disorganization will at some point get the best of you. When you submit your project code, you'll need to make sure files are organized nicely. 

```{r}
suppressMessages(source(paste(r_dir, "aggregate_STROOP.R", sep =  "/")))

STROOP_agg <- readr::read_csv(paste(data_dir, "STROOP_agg.csv", sep = "/"))

head(STROOP_agg)
```

# **Pivot Data Frame to Wide**

We will use `tidyr::pivot_wider()` to pivot the data frame. Select only your needed columns, then pivot by selecting `names_from` to pivot on and create variable names and to take `values_from` a different variable to insert in the cells of the created variables. If you have other variables in the data frame, you may need to specify the `id_cols` for how to pivot.

```{r}
STROOP_WIDE <- STROOP_agg %>%
  select(., id, trialtype, rt) %>%
  tidyr::pivot_wider(., id_cols = NULL, # the default
    names_from  = trialtype,
    values_from = rt
  ) 
view(head(STROOP_WIDE))
```


If you have other columns in the data frame, specify columns.

```{r}
STROOP_WIDE <- STROOP_agg %>%
  tidyr::pivot_wider(., id_cols = id,  # override the default; 
                        names_from  = trialtype,
                        values_from = rt
  ) 
```

## *Standardize the Variables*

Add a some new variables to make ids odd/even using modular arithmetic and center variables (standardize) using `scale()`.

```{r}
STROOP_WIDE <- STROOP_WIDE %>%
  mutate(., odd = ifelse(id %% 2, 1, 0),
         across(.cols = c(congruent, incongruent), 
                .fns = ~as.numeric(scale(.x)), 
                .names = "{.col}_z")
         )

view(head(STROOP_WIDE))
```


# **Fitting a Linear Model Visually**

## *Using `"loess"` Default*

```{r}
STROOP_WIDE %>%
  ggplot(data = ., aes(x = congruent, y = incongruent)) +
  geom_point() +
  geom_smooth() +
  see::theme_modern()
```


## *Using `"lm"`*

```{r}
STROOP_WIDE %>%
  ggplot(data = ., aes(x = congruent, y = incongruent)) +
  geom_point() +
  geom_smooth(method = "lm") +
  see::theme_modern()
```


```{r}
STROOP_WIDE %>%
  ggplot(data = ., aes(x = congruent_z, y = incongruent_z)) +
  geom_point() +
  geom_smooth(method = "lm") +
  see::theme_modern()
```

## *Correlation Matrix*

If the function looks linear, obtain the correlation (or a matrix) to determine *Pearson's r*, the default for `cor()` as indicated by `method = "peason"`. This provides details of the strength of the relationship. Squaring `r` will provide the coefficient of determination, or explained variance.

```{r}
stroop_cor <- STROOP_WIDE %>%
  select(., -id) %>%
  cor(., method = "pearson")

stroop_cor
```


# **Fitting a Linear Model Statistically**

1. We will define a linear model using the base R `lm()` function by specifying the formula for fitting a line through x and y points. The general form for the regression model is as follows:

Regression model: 

- predicted `y` value = `intercept` + (`slope` * value of `X`)

Or in R, this is:

`mymodel <- lm(formula = outcome ~ predictor, data = mydataframe, na.action = some action)`

Where:
- `mymodel` is an object that contains information about the linear model; summary() will be useful for inspecting the model
- `criterion` is the predicted variable
- `predictor(s)` are the variables used to predict scores on the criterion outcome
- `data` is the name of the data frame object
- `na.action` (optional) allows you to specify a complete data set if you wish to drop out anyone with missing values; this in an alternative to subsetting when using regression models

`mymodel <- lm(formula = y ~ x, data = DATA, na.action = na.exclude)`



## *Model 1: Simple/Bivariate Linear Regression*

Predicting y from x1 (continuous) predictor:

`lm(y ~ x1, data = data frame)`

```{r}
mod_stroop <- STROOP_WIDE %>%
  lm(incongruent_z ~ congruent_z, data = .)
```

### *Interpreting the Overall Model Fit*

For model details, pass model to `summary()`

```{r}
summary(mod_stroop)

mod_stroop$coefficients

mod_stroop$residuals

mod_stroop$fitted.values
```

#### *R-squared*

Multiple R-squared represents the proportion of variance in the criterion that is accounted for by the predictor(s). The square root of this is Pearson’s correlation coefficient. Because there is only one predictor, this multiple R-squared is really just r squared. Because the estimates are based on samples, they are not perfect for making inferences about the populations you really care about. The adjusted R-squared is adjusted for shrinkage, or loss of predictive power that occurs when using sample data to predict population data; the more error in the regression model (e.g., residuals), the more the adjusted R-squared will differ from the unadjusted R-squared.

#### *F-statistic*

The F-statistic represents the ANOVA test value (F value) for the ratio test of the model. A statistically significant result indicates that the regression model explains the data better than a model based on the mean of album sales (mean-based model). People often compare the p-value to alpha to decide if the regression model fits the data better than the mean-based model. This ANOVA test only tells you whether the overall model fits the data; it does not examine the components of the model (e.g., b0, b1, etc.).

#### *Model Parameters/Coefficients*

Interpreting Specific Model Parameters (coefficients part of the output table)

The coefficients report is displayed in 2 rows and 4 columns. The `Estimate` column displays the two estimated coefficient values y-intercept (b0) and the regression coefficient (b1). The `Std. Error` column displays the error in estimating the coefficients. This error indicates the amount of error in coefficients; small error is good and would indicate that b0 and b1 would not vary as much from sample to sample. The `t-value` column displays the value of the t-statistic, which simply tests whether the coefficient differs from 0 (H0: t = 0), which can be inferred also from examining the `Pr(>|t|)` column which provides the p-value for the t-test.

The *first row* corresponds to the **intercept* (b0). The y-intercept predicts the value of `y` when `X = 0` (e.g., X = 0). A p-value less than or equal to alpha may indicate that the y-intercept differs from 0. Mathematically, b0 can be less than 0. This is the response time on incongruent trials with congruent trials are 0. Of course, you cannot have response times of 0 but also because the response times are centered with a mean of 0, this represents the value of value of y when x = 0. 

The *second row* corresponds to the **regression coefficient/slope** (b1). This represents the increase or decrease (depending on whether the correlation is positive or negative) in response times to incongruent trials for each unit change in response times for the congruent trials. A p-value equal to or less than alpha means the slope differs from 0. When there are multiple predictor variables, there will be slopes corresponding the each predictor and the criterion.


Model parameters can also be checked using the `parameters::model_parameters()` function, which will provide them in a cleaner table and adjust for scientific notation as well as provide confidence intervals bounded around the estimates, which `summary()` does not provide.

```{r}
parameters::model_parameters(mod_stroop)
```

Additional Metrics:

Using `r2()` and `model_performance()`, you can see the model performance metrics, including AIC and BIC metrics.

*AIC (Akaike’s Information Criterion)* is a metric for model accuracy. The lower the AIC values, the better the model (useful when comparing models). AICc is a version of AIC correcting for small sample sizes. *BIC (Bayesian Information Criterion)* is a variant of AIC with a stronger penalty for including additional variables to the model.

*Root Mean Squared Error (RMSE)* represents the average error performed by the model in predicting the outcome for an observation. RMSE is the square root of the mean squared error (MSE). The lower the RMSE, the better the model.

```{r}
performance::r2(mod_stroop)
performance::model_performance(mod_stroop)
```


### *Examining Model Assumptions*

You should generally examine model assumptions prior to interpreting the model because if the assumptions are violated, the model is invalid. In other words, you don't want to use a model to predict data when the model is not working in the ways it was designed.

#### *Linear Relationship between x and y*

The predictor(s) must be either quantitative or categorical (2 categories); the outcome variable must be quantitative and continuous. It should also be unbounded or unconstrained; for example if a scale that ranges from 1 to 9 is used to measure the criterion and if responses only fall between say 3 and 7, the data are constrained (bounded). An obvious solution is to check the range for the criterion using the built-in `range()` function.

Variance Concerns. Predictors should not be restricted and not have variances that are near 0. Range restriction in general is often problematic with regression (leading to attenuated correlations and reduce predictive ability). If you have a much smaller range, you should investigate reasons why. 

Passing the model to the `plot()` function will produce model plots for examining the assumptions of the model. You can view each plot separately or produce a 2x2 chart that displays them together. Because there will be 4 plots, you will need to the some graphical parameters first by making a 2 columns and 2 rows using `par(mfrow = c(2,2))`. However, grouping the plots will reduce their size for visual inspection. 


```{r}
par(mfrow = c(2,2))
plot(mod_stroop)
```

Similarly, `performance::check_model()` will produce a similar plot set without using additional code. This function will also adjust the plots to include depending on your model so that you don't always need to specify them. However, to check the assumptions individually, you will specify them with their unique functions. 

- `check_heteroscedasticity()`
- `check_normality()`
- `check_outliers()`
- `check_collinearity()`

```{r}
performance::check_model(mod_stroop)

performance::check_normality(mod_stroop)
performance::check_heteroscedasticity(mod_stroop)
performance::check_outliers(mod_stroop)
#performance::check_collinearity(mod_stroop)
```

#### *Linear Relationship between x and y*

The relationship between the criterion and the predictor(s) is *linear* rather than *nonlinear*. Plot #1 is the residual plot for which errors (y-axis)) plotted as a function of predicted/fitted values (x-axis); a straight horizontal line on this plot would indicate that the linearity assumption is met; a curvilinear model is better than a linear model, then the errors would not be distributed normally, but instead be greater in some parts of the plot than in others.

#### *Homoscedasticity of the Residuals*

The variance in the residuals, or error in prediction, should be the same across the values of the predictors. Homoscedasticity is also referred to as constant variance. Non-constant variance in the residuals is referred to as heteroscedasticity. We can examine homoscedasticity visually and statistically. If your variance in the residuals is across the range of the predictor, this means your model is predicting `y` better or worse depending on the value of `x`. Keep in mind that the model is a general fit and used to predict all y values from x, so if your model predicts `y` well on the lower end of the range of `x` but poorly on the higher end of `x`, your general model is going to be erroneous for some prediction. Maybe you need a different model.


Plot #1 can also provide information about the variance in the residuals across levels of the predictor. Based on this example, you can see that there is more variability of the residuals (y-axis) on the left side of the plot than at the right side; thus the variability in residuals does not seem to be constant. If the variance is not constant, then we may have issues with residuals not being distributed normally (see next assumption).

If we want to statistically test for constant variance, we can pass the model to the `car::ncvTest()` function (install `car` if not installed). The ncv stands for non-constant variance. We want constant variance. The `ncvTest()` function provides a Chi-Square test value and a corresponding p-value for the test. If p is less than or equal to alpha, you have evidence that that you have non-constant variance, or heteroscedasticity. 


```{r}
car::ncvTest(mod_stroop)

performance::check_heteroscedasticity(mod_stroop)
```

#### *Residuals (prediction errors) are distributed normally*

If model errors are random and the size of the prediction error does not depend on whether the score on the predictor is low or high, then those errors should be distributed normally. Plot #2 will provide detail about the *normality of the residuals*. Plot #2 is a *quantile-quantile plot* used to determine normality of errors; if points fall along the identity line, the errors are distributed normally. We could also test this assumption by using the `sharipo.test()` function on the model residuals themselves. The residuals are stored in a vector of the model (e.g., `mymodel$res`), which you can pass to `shapiro.test()`.

```{r}
shapiro.test(mod_stroop$residuals)

performance::check_normality(mod_stroop) # note, uses a different metric (check docs)
```


#### *No multicolinearity*

Predictors should not be strongly correlated with each other. After all, predictors are added to a model in order to predict y based on independent rather than dependent information. When you have more than one predictor, you have to examine for multicolinearity. Why? Collinearity will mask the true relationship among variables. Having multicolinearity makes the beta values (y-intercept and slope coefficients) unreliable and untrustworthy when making inferences from samples to populations. Generally speaking, there will be more error in beta values for greater amounts of multicollinearity. There are other influences too, but we won’t address them here. You should just know to determine if you meet this assumption. Because we only have one predictor in this example, we do not have to test for this assumption.

If you had multiple predictors, you would want to test for multicollinearity. One easy way to test this is to examine Variance Inflation Factors (VIFs) using the `car::vif()` function from the car library. The function returns a VIF value for each predictor. If the square root of the VIF is greater than 2, this predictor would be eliminated from your model. There are other rules of thumb that you can examine.

If you have multiple predictors:

```{r}
# car::vif(mymodel) # variance inflation factors for the model
# sqrt(car::vif(mymodel)) # the square root of them
```


#### *Independence of Errors*

For any two observations in the data, the errors should not be related. This is usually not a problem as long as your sample is selected at random and you don’t allow people to participate in your study, allow them to talk to other participants, etc. in ways that affect how two people (or objects) respond. This independence assumption can be tested using the Durbin-Watson test using the `car::durbinWatsonTest()`.  The Durbin-Watson test will produce a D-W statistic value that will be large if independence is violated (and you have dependence). Compare *p* to alpha. 

```{r}
car::durbinWatsonTest(mod_stroop)
```

## *Model : Multiple Linear Regression*

Predicting y from x1 (continuous) and x2 (categorical) predictors: `lm(y ~ x1 + x2, data = data frame)`

```{r}
mod_stroop2 <- lm(incongruent_z ~ congruent_z + odd, data = STROOP_WIDE)

performance::check_model(mod_stroop2)
performance::model_performance(mod_stroop2)
```


## *Linear Model Using `lm()`*

- `rt ~ trialtype as congruent or incongruent`

```{r}
head(STROOP_agg)

lm_stroop <- STROOP_agg %>%
  lm(rt ~ trialtype, data = .)

#performance::r2(lm_stroop)
parameters::model_parameters(lm_stroop)
performance::model_performance(lm_stroop)
```


## *Analysis of Variance (ANOVA) Model Using `aov()`*

- `rt ~ trialtype as a factor`

```{r}
STROOP_agg %>%
  ggplot(data = ., aes(trialtype, rt)) +
  geom_boxplot(fill = "grey") +
  geom_jitter(aes(color = trialtype), width = .15) + 
  see::theme_modern() + stat_summary(shape = 1) 
```

```{r}
aov_stroop <- STROOP_agg %>%
  mutate(., trialtype = factor(trialtype)) %>%
  lm(rt ~ trialtype, data = .)
```

Using `aov()`:

```{r}
aov_trialtype <- STROOP_agg %>%
  mutate(., trialtype = factor(trialtype)) %>%
  aov(rt ~ trialtype, data = .)

car::Anova(aov_trialtype)
performance::check_model(aov_trialtype)
parameters::model_parameters(aov_trialtype)
performance::model_performance(aov_trialtype)
```


Using `lm()` but congruent as baseline.

- `rt ~ incongruent` 0 = baseline, 1 = incongruent

```{r}
lm_incongruent <- STROOP_agg %>%
  lm(rt ~ incongruent, data = .)

performance::check_model(lm_incongruent)
car::Anova(lm_incongruent)
parameters::model_parameters(lm_incongruent)
performance::model_performance(lm_incongruent)
```

```{r}
aov_incongruent <- STROOP_agg %>%
  aov(rt ~ incongruent, data = .)

performance::check_model(aov_incongruent)
car::Anova(aov_incongruent)
parameters::model_parameters(aov_incongruent)
performance::model_performance(aov_incongruent)
#coefficients(aov_incongruent)
```




## *Checking the Model Diagnostics*

```{r}
# Comprehensive visualization of model checks:
performance::check_model(mod_stroop)

# separately
performance::check_normality(mod_stroop)
plot(performance::check_normality(mod_stroop) )


performance::check_heteroscedasticity(mod_stroop)
plot(performance::check_heteroscedasticity(mod_stroop))

performance::check_autocorrelation(mod_stroop)

#performance::check_collinearity()
#plot(performance::check_collinearity())
```

## *Assessing Model Fit and Performance*

```{r}
parameters::parameters(mod_stroop)

# Summarizing Model Performance
performance::model_performance(mod_stroop)

```



# **Comparing Models**

## *Model 1*

## *Model 2*

```{r}
anova(mod_stroop, mod_stroop2)

performance::test_performance(mod_stroop, mod_stroop2)

performance::compare_performance(mod_stroop, mod_stroop2)

plot(performance::compare_performance(mod_stroop, mod_stroop2, rank = TRUE))
```


# **Model Predictions**

```{r}
mod_stroop_pred <- mod_stroop %>% modelbased::estimate_expectation(.)
#head(mod_stroop_pred)
mod_stroop_pred2 <- mod_stroop2 %>% modelbased::estimate_expectation(.)

# add the actual data to predicted
# model 1
mod_stroop_pred <- mod_stroop_pred %>%
  mutate(., incongruent_z = STROOP_WIDE$incongruent_z)

# model 2
mod_stroop_pred2 <- mod_stroop_pred2 %>%
  mutate(., incongruent_z = STROOP_WIDE$incongruent_z)

mod_stroop_pred %>%
  ggplot(aes(x = incongruent_z, 
             y = Predicted)) +
  geom_line(aes(x = incongruent_z, 
                y = incongruent_z), 
            linetype = "dashed") +
  geom_point() +
  ylab("incongruent_z (predicted)") +
  theme_modern()


# plot both models predictions
ggplot(data = mod_stroop_pred, 
       aes(x = incongruent_z, y = Predicted)) +
  # with identity line (diagonal) representing perfect predictions
  geom_abline(linetype = "dashed") +
  # Add the actual predicted points of the models
  geom_point(aes(color = "Model 1")) +
  geom_point(data = mod_stroop_pred2, 
             aes(color = "Model 2")) +
  # Aesthetics changes
  labs(y = "incongruent_z (predicted)", color = NULL) +
  scale_color_manual(values = c("Model 1" = "blue", "Model 2" = "red")) +
  see::theme_modern()


```


# **Refittng for Standardized Model** 

```{r}

mod_stroop <- STROOP_WIDE %>%
  lm(incongruent ~ congruent, data = .)

# actual model parameters
parameters::model_parameters(mod_stroop)

# if not run on zs (same parameters as if passing z scores)
parameters::model_parameters(mod_stroop, standardize = "refit")


```

# *Linear Model on Aggregated Data*

```{r}
lm_agg <- STROOP %>%
    filter(., rt < 7000) %>%
    group_by(., id, trialtype) %>%
    summarise(., rt = mean(rt)) %>%
    # then pass to lm
    lm(rt ~ trialtype, data = .)

performance::r2(lm_agg)
performance::model_performance(lm_agg)

parameters::model_parameters(lm_agg, effects = "all")
#parameters::model_parameters(lm_agg, ci_method = "wald")
```


# **Linear Mixed Model**

## *Random Intercepts Model Using `lme4`*



```{r}
library(lme4)
view(STROOP)

#mod_mixed1 <- nlme::lme(fixed = rt~1,
#                        random = ~1|id, 
#                        data = STROOP)

lm_mixed1 <- 
  STROOP %>%
  filter(., rt < 7000) %>%
  lme4::lmer(rt ~ trialtype + (1|id), data = ., REML = T)

lm_mixed1

# Intraclass Correlation Coefficient
performance::icc(lm_mixed1)
performance::r2(lm_mixed1)
performance::model_performance(lm_mixed1)

performance::check_normality(lm_mixed1)
performance::check_heteroscedasticity(lm_mixed1)
#performance::check_collinearity(lm_mixed1)
performance::check_model(lm_mixed1)

# model parameters with CI, df and p-values 
# based on Wald approximation
parameters::model_parameters(lm_mixed1, effects = "all")


# model parameters with CI, df and p-values 
# based on Kenward-Roger approximation
#parameters::model_parameters(lm_mixed1, ci_method = "kenward")
```

## *Compare the Models*

```{r}
#anova(lm_agg, lm_mixed1)

parameters::compare_models(lm_agg, lm_mixed1)

plot(parameters::compare_models(lm_agg, lm_mixed1))
```


# *Random Slopes Model Using `lme4`*

As with the random-intercepts model, the random effects are defined in parentheses as a linear combination of effects, e.g., `(trialtype|id)`.

```{r}
#library(coda)
#library(languageR)

lm_mixed2 <- STROOP %>%
    select(., id, rt, trialtype) %>%
    filter(., rt < 7000) %>% 
    lme4::lmer(rt ~ trialtype + (trialtype|id), 
               data = ., REML = T)

# Intraclass Correlation Coefficient
performance::icc(lm_mixed2)

performance::r2(lm_mixed2)

lmerTest::ranova(lm_mixed2)
#lmerTest::lmer(rt ~ trialtype + (1|id), data = STROOP)
#
#

performance::model_performance(lm_mixed2)
#check_overdispersion(lm_mixed2)
performance::check_normality(lm_mixed2)
performance::check_heteroscedasticity(lm_mixed2)
#performance::check_collinearity(lm_mixed2)
performance::check_model(lm_mixed2)


parameters::model_parameters(lm_mixed2, effects = "all")
#parameters::model_parameters(lm_mixed2, ci_method = "kenward")
```

#  

```{r eval=FALSE, include=FALSE}
library(rstanarm)

model_stan_glmer <- STROOP %>%
    select(., id, rt, trialtype) %>%
    filter(., rt < 7000) %>% 
    rstanarm::stan_glmer(rt ~ trialtype + (1|trialtype), 
                              data = ., cores = 4)


```

## *Compare Models*

```{r}
performance::compare_performance(lm_agg, lm_mixed1, lm_mixed2,
                                 rank = T)
plot(performance::compare_performance(lm_agg, 
                                      lm_mixed1, lm_mixed2,
                                 rank = T))

plot(performance::compare_performance(lm_mixed1, lm_mixed2,
                                 rank = T))

parameters::compare_models(lm_agg, lm_mixed1, lm_mixed2)

plot(parameters::compare_models(lm_agg, lm_mixed1, lm_mixed2))
```



# **Choosing Influential Model Parameters**

`parameters::select_parameters()` will select the best variables automatically and update the model. 


## *Adding all Variables as Predictors*

Pass `.` as the predictor rather than specify all variables separately

```{r}
diamond <- ggplot2::diamonds %>% select(., -c(x, y, z))

lm(price ~ ., data = diamond) %>%
  parameters::select_parameters(.) %>%
  summary(.)
  
```


## *Adding all Interaction Terms*

Pass `.*.` as the predictor...

```{r}
mod <- lm(price ~ . * ., data = diamond)

mod %>% summary(.)

#mod %>% parameters::select_parameters(.) %>% parameters::model_parameters(.)
```




# **Updating Models**

```{r}
# int00.lm.3 <- update(int00.lm.2, .~. - featureSize, data=int00.dat)

#https://github.com/faridcher/ml-course


#Nest by
#https://github.com/keithmcnulty/peopleanalytics-regression-book/blob/master/r/10-tidy_modeling.Rmd

```

# 

```{r}



```


```{r}
#conruent is baseline (0), so incongruent is the comparison group (1)

mutate(., incongruent = case_when(
  congruent == "congruent" ~ 0,
  congruent == "incongruent" ~ 1
))

```

# Simple model

T-test vs. linear regression



```{r}
diamonds <- ggplot2::diamonds

CUT_CARAT <- diamonds %>%
  # select from the data frame only cut and carat
  select(., c("cut", "carat")) %>%
  # filter by any two cut qualities
  filter(., cut %in% c("Ideal", "Premium"))

# check the structure of the data frame. Does it look right?
str(CUT_CARAT)
```

Notice there are still 5 levels. Fix so that there are 2 levels ordered in terms of quality.


```{r}
CUT_CARAT <- diamonds %>%
  # select from the data frame only cut and carat
  select(., c("cut", "carat")) %>%
  # filter by any two cut qualities
  filter(., cut %in% c("Ideal", "Premium")) %>%
  mutate(., cut = factor(cut, levels = c("Ideal", "Premium")))

?rstatix::reorder_levels
CUT_CARAT <- CUT_CARAT %>%
  rstatix::reorder_levels(., "cut", order = c("Ideal", "Premium"))

# check structure again
str(CUT_CARAT)

view(CUT_CARAT)
# assign to a data frame object

# run a t test on the independent groups by passing a formula
# outcome variable as a function for predictor
# outcome ~ predictor 
# make sure to pass data = . or
CUT_CARAT %>%
  t.test(carat ~ cut, paired = FALSE, data = .)


# This can be messy, so pipe the t-test to broom::tidy() and assign
CUT_CARAT_t_model <- CUT_CARAT %>%
  t.test(carat ~ cut, paired = FALSE, data = .) %>% 
  broom::tidy(.)


CUT_CARAT_t <- CUT_CARAT %>%
  t.test(carat ~ cut, paired = FALSE, data = .) %>% 
  #parameters::select_parameters(.) %>%
  parameters::model_parameters(.) %>% 
  mutate(., across(where(is.numeric), round, digits = 2)) %>%
  #broom::tidy(.) #%>% 
  htmlTable::htmlTableWidget()
  #view(., filter = "none")

CUT_CARAT_lm_model <- CUT_CARAT %>%
  lm(carat ~ cut, data = .) %>%
  broom::tidy(.)

CUT_CARAT_t_model
CUT_CARAT_lm_model
  parameters::model_parameters(.) %>%
  mutate(., across(where(is.numeric), round, digits = 2)) %>%
  htmlTable::htmlTableWidget()


view(CUT_CARAT_t_model)

car::Anova(fit.1, type = "III")

rstatix::get_anova_table(res.aov, correction = "auto")
# planned contrasts
contrasts.factora <- matrix(c(2,-1,-1, 0,1,-1), ncol=2)
contrasts(hays$factora) <- contrasts.factora
contrasts(hays$factora)



  

# regression model
model1 <- lm(Sepal.Width ~ Petal.Length * Species + Petal.Width,
            data = iris)
parameters::model_parameters(model1)

parameters::model_parameters(model1) #, eta_squared = "partial")


# Mixed Models using library(lme4)
model2 <- lme4::lmer(Sepal.Width ~ 
                       Petal.Length + (1 | Species),
                    data = iris)

model2 %>% 
  model_parameters() %>% 
  plot()

#(model1, model2)
#
#
```





```{r}
DAT <- gapminder %>%
  filter(continent == "Europe")

DAT %>%
  ggplot(., aes(lifeExp)) +
  geom_histogram(color = "#000000", fill = "#0099F8") +
  labs(
    title = "Histogram of Life Expectancy in Europe",
    subtitle = "",
    caption = "Source: PSYC131 [Games, Cognition, and Data Science]",
    x = "Life expectancy",
    y = "Count"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(color = "#0099F8", size = 16, face = "bold"),
    plot.subtitle = element_text(size = 10, face = "bold"),
    plot.caption = element_text(face = "italic")
  )
```


Example with `see` library.

```{r}
# Correlation Matrix
mtcars %>%
  #dplyr::select(mpg, cyl, disp, wt, drat) %>%
  correlation::correlation(., 
                           method = "pearson",
                           winsorize = T
                           ) %>%
  summary(., redundant = F) %>%
  plot() +            # plot 
  ggtitle("") +       # add your own title
  see::theme_modern() # clean up the plot and put in a modern theme
  
# # Gaussian Graphical Models
mtcars %>%
  dplyr::select(mpg, cyl, disp, wt, hp, vs) %>%
  correlation::correlation(., 
                           method = "pearson",
                           winsorize = T,
                           partial = T
                           ) %>%
  plot() +
  ggtitle("")        # add your own title
```


There are some great tips and examples for creating
[http://sthda.com](http://sthda.com/english/wiki/ggplot2-scatter-plots-quick-start-guide-r-software-and-data-visualization)


For examples of how individual cases could influence models and predictions, check out some examples of these [outliers and influential cases](https://tillbe.github.io/outlier-influence-identification.html).

put this someplace
You can use the `purrr` library from the `tidyverse` ecosystem to improve the functionality of `base R`. 

`purrr::map()` will apply a function to each element of a list or atomic vector

```{r}
?purrr::map

library(purrr)

library(magrittr)

mtcars %>%
  #split(.$cyl) %>%                          # split is from base R
  #split(.$am) %>%                          # split is from base R
  purrr::map(~ lm(mpg ~ wt, data = .)) %>%  # run model on each split
  purrr::map(summary) %>%                   # summarize each model
  purrr::map_dbl("r.squared") %>%           # extract rsquared from each model
  round(., 2) %>%                           # round the returned rsquared
  htmlTable::htmlTableWidget()              # output as an html

#>         4         6         8 
#> 0.5086326 0.4645102 0.4229655
```




Atomic vectors represent the "atoms" of R. They are the simplest building blocks upon which the rest of R is built. Four types of atomic vector important for data analysis:

- **double** vectors (`<dbl>`) contain real numbers.

`double <- c(1.5, 2.8, pi)`

- **integer** vectors (`<int>`) contain integers.

`integer <- c(1L, 2L, 3L)` 

Without the L following the value, `R` will create a vector of doubles.

`not_integer <- c(1, 2, 3)`

Check the type `typeof(not_integer)` and you will see "double" is returned.

- **character** vectors (`<chr>`) contain strings made with "".
`character <- c("Jim", "Jane", "Boris", "Tanya")`

- **logical** vectors (`<lgl>`) contain `TRUE` or `FALSE`.

`logical <- c(TRUE, FALSE, FALSE)`


Vectors can also have names 

```{r}
v_named <- c(guava = 2, pineapple = 4, dragonfruit = 1)

v_named
```








http://www.sthda.com/english/wiki/ggplot2-colors-how-to-change-colors-automatically-and-manually








```{r}
library(magrittr)
```

Add to end
`ggplot` includes a useful function for saving plots to your local computer. Using `ggplot2::ggsave()`, you can save files in a variety of file formats.

Warning: By default, `ggplot2::ggsave()` saves the most recent plot. If you wish to save a plot that is currently displayed in the RStudio 'plots' tab:

`ggplot2::ggsave()` take a few important arguments:

- `filename`
- `plot = last_plot()`
- `units` `c("in", "cm", "mm", "px")`
- `dpi`

# Saving the last plot (default):

```{r}
ggplot2::ggsave(filename = 'my_ggplot.pdf', 
                width = 5, height = 4
                )
```

# Saving a specific plot (add the `plot` argument):


If you save the plot as an object (here named `myplot`), you can pass the object by specifying it in the `plot` argument.

```{r}
myplot <- mpg %>% ggplot2::ggplot(ggplot2::aes(
  x = displ, 
  y = cty)) + 
  ggplot2::geom_point(ggplot2::aes(colour = class))

ggplot2::ggsave(filename = 'my_ggplot.pdf', 
                plot = myplot,
                width = 5, 
                height = 4
                )
```






# Making Attactive Plots with `ggplot`







## Getting Data

```{r}
#install.packages("gcookbook")
cabbage <- gcookbook::cabbage_exp

str(cabbage)
```


```{r}
?ggplot2::geom_col
cabbage %>%
  ggplot2::ggplot(ggplot2::aes(Date, Weight)) +
  ggplot2::geom_col(
    ggplot2::aes(fill = Cultivar),
    position = ggplot2::position_dodge(),
#    position = ggplot2::position_dodge(0.8),
#    color = "black",
#    width = .7
  )

```


## Boxplots

```{r}
# Formula syntax
boxplot(len ~ supp, data = ToothGrowth)
```

By adding the `+` operator in the formula call, the outcome variable will be displayed as a function of two predictors rather than one. 

```{r}
boxplot(len ~ supp + dose, data = ToothGrowth)
```

With `ggplot2`, you can create a similar plot using `geom_boxplot()` and setting the `aes()` x and y axes.

```{r}
ggplot2::ggplot(ToothGrowth, ggplot2::aes(x = supp, y = len)) +
  ggplot2::geom_boxplot()
```


# Building and adding to `ggplot` objects: Histograms, Facetting, etc.  

```{r}
# first, the core xy plot specs
hbase <- mtcars %>% 
  ggplot2::ggplot(., ggplot2::aes(x = mpg))
#hbase <- ggplot2::ggplot(hays, ggplot2::aes(x = dv))
# now the base plot
hbase1 <- hbase + 
  ggplot2::geom_histogram(
    ggplot2::aes(y = ..density..), # change y axis to density
    bins = 8, colour = "black", fill = "white") +
  ggplot2::geom_density()

hbase1

# now add the specification that creates separate panels for aach group
hbase2 <- hbase1 + ggplot2::facet_grid(am ~ .)

hbase2




mtcars
pbox <- ''
p <- mtcars %>%
  ggplot2::ggplot(., ggplot2::aes(x = as.factor(cyl), y = mpg)) +
  ggplot2::geom_boxplot() #+

p_stats <- p + ggplot2::stat_summary(fun = mean, 
                        geom = "point", 
                        shape = 16, 
                        size = 3.5, 
                        color = "blue")
p_stats

p_stats_labs <- p_stats + 
  ggplot2::xlab("Number of Cylinders") +
  ggplot2::ylab("Miles Per Gallon (mpg)") +
  ggplot2::ggtitle(" Boxplot of MPG by Number of Cylinders\n Means are added with blue point")

p_final <- p_stats_labs + 
  ggplot2::theme_minimal() + 
  ggplot2::theme(text = ggplot2::element_text(size=12))

# the three plots
p
p_stats
p_final
```


An alternative to `dplyr:summarise()` paired with `dplyr::groupby()` is `describeBy()` from the `psych` library. An advantage of this function is how it summarizes data out-of-the-box, without . A disadvantage is with the returned output, which is not in the form of a data frame but instead in a `list`. Although you can convert lists to data frames, unfortunately, the structure does not cleanly

`function_name <- function(arg1, arg2, ... ) {`
        `# Code`
`}`


```{r}
l2 <- function(list) {
  return(do.call(rbind.data.frame, list))
}

psych::describeBy(mpg ~ cyl + am, data = mtcars, mat = TRUE)

psych::describeBy(mpg, mpg$cyl, data = mtcars, mat = TRUE)


htmlTable::htmlTable(format(estatistica, digits = 2)) 

htmlTable::htmlTableWidget(format(estatistica, digits = 2)) 
 
?psych::describeBy
psych::describeBy(mtcars[,c("mpg", "am")], mtcars$am, mat = TRUE)

#estatistica <- describeBy(pag,list(pag$Jogo))

estatistica2 <- do.call("rbind",estatistica)


?describe
sjPlot::tab_df(psych::describe(iris),
       digits = 2,
  title = "Descriptive statistics", #give your tables title
  file = "Descriptive_statistics.doc")

my_describe <- function(data, group = NULL, form = NULL, fast = TRUE){
  
  if(deparse(substitute(group)) == 'NULL')
    psych::describe(data)
  else  psych::describeBy(as.formula(substitute(data~group)))
}

my_describe <- function(data, group = NULL, form = NULL, fast = TRUE){
  
  f = 
  if(deparse(substitute(group)) == 'NULL')
    psych::describe(data)
  else psych::describeBy(as.formula(substitute(data~group)))
}


my_describe(data = mtcars, group = am)
```

```{r}

getMode <- function(df) {
  ux <- na.omit(unique(df))
  ux[which.max(tabulate(match(df, ux)))]
}

describeBy <- function(dataframe) {

  getMode <- function(dataframe) {
    ux <- na.omit(unique(dataframe))
    ux[which.max(tabulate(match(dataframe, ux)))]
  }
  # dplyr for summarizing
  dataframe %>%
    dplyr::summarise_all(.funs = list(
      mean = function(x) ifelse(is.numeric(x), sprintf("%.3f", mean(x)), as.character(getMode(x))), 
      sd   = function(x) ifelse(is.numeric(x), sd(x), sd(as.numeric(x))), 
      min  = function(x) ifelse(is.numeric(x), sprintf("%.3f", min(x)), levels(x)[1]), 
      max  = function(x) ifelse(is.numeric(x), sprintf("%.3f", max(x)), levels(x)[length(levels(x))]), 
      n    = function(x) sum(!is.na(x))
    )) %>% 
    # tidyr for cleaning up
    tidyr::pivot_longer(
      dplyr::everything(),
          names_to = c("set", ".value"),
          names_pattern = "(.+)_(.+)"
    )
}
describeBy()

iris %>% 
  dplyr::summarise_all(.funs = list(
    mean = function(x) ifelse(is.numeric(x), sprintf("%.3f", mean(x)), as.character(getMode(x))), 
    sd   = function(x) ifelse(is.numeric(x), sd(x), sd(as.numeric(x))), 
    min  = function(x) ifelse(is.numeric(x), sprintf("%.3f", min(x)), levels(x)[1]), 
    max  = function(x) ifelse(is.numeric(x), sprintf("%.3f", max(x)), levels(x)[length(levels(x))]), 
    n    = function(x) sum(!is.na(x))
  )) %>% 
  tidyr::pivot_longer(
    dplyr::everything(),
        names_to = c("set", ".value"),
        names_pattern = "(.+)_(.+)"
    )

list.as.data.frame
colnames(psych::describeBy(mpg ~ cyl + am, data = mtcars))


d = list.as.data.frame(psych::describeBy(mpg ~ cyl + am, data = mtcars))

colnames(d)

colnames(psych::describeBy(mpg ~ cyl + am, data = mtcars))
rownames(psych::describeBy(mpg ~ cyl + am, data = mtcars))


psych::describeBy(, group = factora, type = 2)


ggplot2::ggplot(ToothGrowth, ggplot2::aes(x = interaction(supp, dose), y = len)) +
  ggplot2::geom_boxplot()


ggplot2::ggplot(ToothGrowth, ggplot2::aes(x = supp, y = len)) +
  ggplot2::geom_boxplot(fill)

```


## Bar Plot

```{r}
my_plot <- ggplot2::ggplot(cabbage, ggplot2::aes(
  x = Date, 
  y = Weight, 
  fill = Cultivar)) +
  ggplot2::geom_col(position = "dodge") +
  ggplot2::geom_text(
    ggplot2::aes(label = Weight),
    colour = "white", size = 3,
    vjust = 1.5, position = ggplot2::position_dodge(.9)
  ) 

my_plot

my_plot + ggplot2::theme_classic()

```



```{r}

```

 

## Stacked Bar Plot

```{r}

# Sort by the Date and Cultivar columns
ce <- cabbage %>%
  dplyr::arrange(Date, rev(Cultivar))

# Get the cumulative sum
ce <- ce %>%
  dplyr::group_by(Date) %>%
  dplyr::mutate(label_y = cumsum(Weight))


ggplot2::ggplot(ce, ggplot2::aes(
  x = Date, 
  y = Weight, 
  fill = Cultivar)) +
  ggplot2::geom_col() +
  ggplot2::geom_text(
    ggplot2::aes(y = label_y, label = Weight), 
    vjust = 1.5, 
    colour = "white")

```

Adding labels to stacked bars

```{r}


```



