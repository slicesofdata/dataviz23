---
title: 'GCDS Homework #06'
author: "KEY"
date: "Due: 10-13-2022"
output:
  #word_document:
  #  toc: yes
  #  number_sections: yes
  html_document:
    toc: yes
    number_sections: yes
    code_folding: show # vs. hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
########################################################################
# Don't forget to run this
######################################################################
rm(list = ls(all.names = TRUE))      # remove objects in R

source("https://pastebin.com/raw/8mXH02yg")   # run and comment out after knitting
source("https://pastebin.com/raw/97NNTTzu")   # run to include in function definitions

# set the paths for project, script, and data dirs
proj_dir <- gsub("GCDS.*.Rmd", "GCDS", get_ActivePath())
proj_name = ""
r_dir    <- paste(proj_dir, "r", sep = "/")    # r subdir
data_dir <- paste(proj_dir, "data", sep = "/") # data subdir
if ( proj_name != "" & !dir.exists(paste(proj_dir, proj_name, sep = "/")) ) {
  # create project dir
  suppressWarnings(dir.create(paste(proj_dir, proj_name, sep = "/")))
  r_dir <- gsub("/r", paste0("/", proj_name, "/r"), r_dir)
  data_dir <- gsub("/data", paste0("/", proj_name, "/data"), data_dir)
  # create sub directories
  suppressWarnings(dir.create(r_dir))
  suppressWarnings(dir.create(data_dir)) }
```

# **Overview**

For this homework, you will save the `R Markdown` homework document and save it to "GCDS/r" on your computer. Rename as **"GCDS_HW_07_yourname"**. When you open it, add your name to the author part of the YAML code. The homework will need to be submitted as a `knit` HTML file. Feel free to work with a partner but understand that the work submitted has to be your own.


*NOTE: The plots may take a moment to render as there is a lot of data.*


# **Getting Data**

Take the `diamonds` data from `ggplot2` and select out the variables `x`, `y`, and `z` and assign the data frame to `diamond`.

```{r}
diamond <- diamonds %>% select(., -c(x,y,z))
```


# **Summarize the Data**

```{r}
# grand mean
diamond %>%
  summarise(., price = mean(price))

# mean by groups
diamond %>%
  mutate(., carat = factor(carat)) %>%
  group_by(carat) %>%
  summarise(., price = mean(price))
```


# **Visualize a Model**

Use `ggplot2` to plot `price` as a function of `carat`. 

```{r}
diamond %>%
  ggplot(., aes(carat, price)) +
  geom_jitter(alpha = .1) + 
  see::theme_modern()


diamond %>%
  ggplot(., aes(carat, price)) +
  stat_summary(geom = "pointrange") + 
  see::theme_modern()


```

# **Correlation Matrix**

Select the numeric variables in `diamond` and pass that to `cor()` in order to view the correlation matrix.

```{r}
diamond %>%
  select(., where(is.numeric)) %>%
  cor(.) 
```


# **Simple Linear Regression**

Build a simple linear model that predicts `price` from `carat` size. 

```{r}
lm_carat <- diamond %>%
  lm(price ~ carat, data = .) 
```

# **Check the Model Assumptions**

Use `check_collinearity()`, `check_normality()`, `check_heteroscedasticity()`, `check_outliers()` from the `performance` library to check any necessary assumptions. 

```{r}
check_normality(lm_carat)
check_heteroscedasticity(lm_carat)
check_outliers(lm_carat)
```

*Summary*: Describe what do you find.





# **Check the Model Parameters and Performance Metrics**

Using `summary()`, `model_parameters()`, or `model_performance()` to assess the model and then complete the following statements by entering the relevant values.


```{r}
summary(lm_carat)

model_parameters(lm_carat)
model_performance(lm_carat)
```

*Summary*: The percentage of variance in `price` that is explained by variation in `carat` size is _____. Although not realistic, based on the model the price for a diamond of `carat` size = 0 would be ____. For each unit increases/decreases in `carat` size, `price` of the diamond increases by ___ dollars. 



# **Multiple Predictors**

Now add a diamond's `table` or `depth` characteristic to the model. You can specify the predictors in a new model. 

Alternatively, you can use `update()` to take an existing model and update it. If the outcome is the same, a `.` would take the place of the original outcome variable to the left of the `~`. To keep the first predictor the same, use `.` in place of it but then add the other predictors. 

Example: `update(old model, . ~ . + new predictor, data = diamond)`

```{r}
lm_carat_table <- update(lm_carat, . ~ . + 
                                 table, data = diamond)

#lm_carat_table <- diamond %>%  lm(price ~ carat + table + depth, data = .) 
```


# **Check the Model Assumptions**

```{r}
check_collinearity(lm_carat_table)
check_normality(lm_carat_table)
check_heteroscedasticity(lm_carat_table)
check_outliers(lm_carat_table)
```

# **Check the Model Parameters and Performance Metrics**

Using `summary()`, `model_parameters()`, or `model_performance()` to assess the model and then complete the following statements by entering the relevant values.


```{r}
summary(lm_carat_table)
model_parameters(lm_carat_table)
model_performance(lm_carat_table)
```

*Summary*: The percentage of variance in `price` that is explained by the predictors is _____. Although not realistic, based on the model the price for a diamond of `carat` size = 0 would be ____. For each unit increase in diamond `carat` size, `price` of the diamond increases/decreases by ___ dollars when controlling for or holding constant `table` size. Also, for each unit increase in a diamond's `table`, it's price increases/decreases by ___ dollars when controlling for or holding constant `carat` size.


*Comment*: 

Does the estimate associated with `carat` change across the two models? If so, how? 

Does holding constant the role of `carat` size affect relationship between `table` and `price`? Hint: Look at the correlation matrix?




# **Compare the Models**

Use `compare_performance()` to compare your two models.

```{r}
compare_performance(lm_carat, lm_carat_table, rank = T)
```

Does one have a higher R-squared value?; a lower AIC or BIC value?; a lower RMSE value? If so, is one a better model? 










# **Interpreting Mean-Centered Models**

Take the diamonds data and mutate z scores for each numeric variable. The use `carat_z` to predict `price_z`.

```{r}
z_carat <- diamond %>%
  mutate(., across(.cols = where(is.numeric), 
                   .fns = ~as.numeric(scale(.x)), 
                   .names = "{.col}_z")) %>%
  lm(price_z ~ carat_z, data = .) 
```


Using `summary()`, `model_parameters()`, or `model_performance()`, complete the following statements by entering the relevant values. 

```{r}
options(scipen = 999)

summary(z_carat)
model_parameters(z_carat)
model_performance(z_carat)

summary(lm_carat)
model_parameters(lm_carat)
model_performance(lm_carat)

```

The percentage of variance in `price_Z` that is explained by variation in `carat_z` size is _____. Although not realistic, based on the model the price for a diamond of `carat_z` size = 0 would be ____. For each unit increase in `carat_z` size, `price_z` of the diamond increases by ___ dollars. 

```{r}

```



```{r}
ggplot(data = diamond) +
  geom_bar(aes(cut))

ggplot(data = diamond) +
  stat_count(aes(clarity))

diamond %>%
  group_by(cut, clarity) %>%
  summarise(., n = n()) %>%
  ggplot(data = .) +
  geom_col(aes(clarity, n, fill = cut), position = "dodge") +
  #see::theme_modern() +
  see::theme_blackboard()


#diamond %>%
#  group_by(cut, clarity) %>%
#  summarise(., n = n()) %>%
#  ggplot(data = .) +
#  geom_point(aes(clarity, n, color = cut), position = "jitter") +
#  see::theme_blackboard()
```

# **Using `GGally::ggpairs()`**

```{r}
 diamond %>% select(., -depth) %>%
  GGally::ggpairs(.)
```


 

```{r}
diamond %>% 
  select(., price, carat, table) %>%
  GGally::ggpairs(.)
```




```{r}

diamond %>%
  ggplot(., aes(carat, price, color = carat), 
         alpha = .05) +
  geom_point() + 
  #geom_smooth(method = "lm", color = "red") + 
  ggpubr::stat_regline_equation(
    label.x = 2, label.y = 310) +
  see::theme_modern()


# 3d plots
library(plotly)
diamond
p <- diamond %>%
  plotly::plot_ly(., x = ~carat, 
                  y = ~depth, 
                  z = ~price, 
                  type = "scatter3d",
                  mode = "markers", color = ~carat)

p


  add_surface() #z = ~plane, x = ~carat, y = ~depth)
?add_surface()
library(scatterplot3d)
scatterplot3d(x = diamond$carat,
                             z = diamond$depth, 
                             y = diamond$price)
?scatterplot3d
model_parameters(lm_carat_table)
model_performance(lm_carat_table)

lm_carat_x_table <- diamond %>%
  lm(price ~ carat * table, data = .) 

model_parameters(lm_carat_x_table)
model_performance(lm_carat_x_table)

head(diamond)
diamond %>%
  lm(price ~ carat + depth, data = .) %>% summary()


compare_performance(lm_carat, 
                    lm_carat_table,
                    lm_carat_x_table,
                    rank = T)

```


# **Model Interpretation**

## *Coefficients*

Intercept and slope coefficient estimates can be found the the `summary()` table, extracted using `coef()`, . Remember that when `x` is 0, the outcome variable, `y` is `b0`, the intercept. And that `y`, changes (increases or decreases) by `b1`, the estimate for predictor variable `x1`. When you have multiple predictors, you'll have coefficient estimates for each predictor in the model (e.g., `x1`, `x2`, `x3`, etc.)

Because these coefficients are only estimates of the model, the standard error corresponding to the estimates provides so uncertainty around those point estimates. You can use the standard error to determine the range of values within which you would would be confident that the real estimate is. Using `parameters::model_parameters()`, you can also see the confidence interval. 

Taking the error into consideration, the t-tests in the `summary()` table will provide information about whether the estimates differ from 0 (e.g., the intercept is not 0 and the slope is not 0 but positive or negative). You can calculate these same tests by hand because `t-test = estimate / std.error`. In other words, the t-test tells you how many standard deviations away from 0 the estimate is.  

```{r}
coef()

model_parameters()

sigma()
```


```{r}
confint(model, level = .95)
```

Using the 95% confidence interval to help interpret the model, you can be 95% confident that `y` will increase/decrease somewhere between the corresponding 2.5% value on the low end to the 95.5% value on the high engh. for each unit change in `x1`. If your mode is 

```{r}

```



# **Multiple Linear Regresssion**


```{r}

```



# **Data**

```{r}
library(performance)
library(see)

mtcars %>%
  mutate(., cyl = factor(cyl)) %>%
  ggplot(data = ., mapping = aes(mpg, wt)) +
  geom_point(aes(color = cyl)) +
  #geom_smooth(method = "lm", se = T) +
  geom_smooth(method = "lm", se = F) +
  see::theme_modern()

mtcars %>%
  #mutate(., cyl = factor(cyl)) %>%
  ggplot(data = ., mapping = aes(cyl, mpg)) +
  geom_point(aes(color = factor(cyl))) +
  geom_smooth(method = "lm", se = F, color = "black", linetype = "solid") +
  see::theme_modern()

# the grand mean
mtcars %>% summarise(., mpg = mean(mpg))

# mean by groups
mtcars %>%
  mutate(., cyl = factor(cyl)) %>%
  group_by(cyl) %>%
  summarise(., mpg = mean(mpg))

# if the cyl predictor was numeric
mtcars %>%
  group_by(cyl) %>%
  lm(mpg ~ cyl, data = .) %>% 
  summary(.)

# do an ANOVA on the numeric
mtcars %>%
  group_by(cyl) %>%
  lm(mpg ~ cyl, data = .) %>% 
  anova(.)


# if the cyl predictor was a factor
mod_mtcars <- mtcars %>%
  mutate(., cyl = factor(cyl)) %>%
  lm(mpg ~ cyl, data = .) 

mod_mtcars %>% summary(.)
  #anova(.)
  #model_parameters(.) #%>% as.data.frame()
mod_mtcars %>% model_performance(.)

car::Anova(mod_mtcars, type = "III")



# all cars
lm_0 <- mtcars %>% lm(mpg ~ wt, data = .)
summary(lm_0)
model_performance(lm_0)

mtcars %>%
  ggplot(., aes(mpg, wt)) +
  xlim(10, 40) +
  geom_point() + 
  geom_smooth(method = "lm", se = F) +
  facet_wrap(~cyl) + see::theme_modern()

lm_4 <- mtcars %>% filter(., cyl == 4) %>% lm(mpg ~ wt, data = .)
model_performance(lm_4)
lm_6 <- mtcars %>% filter(., cyl == 6) %>% lm(mpg ~ wt, data = .)
model_performance(lm_6)
lm_8 <- mtcars %>% filter(., cyl == 8) %>% lm(mpg ~ wt, data = .)
model_performance(lm_8)


multi_mod <- lm(mpg ~ ., data = mtcars); 

wt_ <- lm(mpg ~ wt, data = mtcars)
model_performance(wt_)

cyl_ <- lm(mpg ~ cyl, data = mtcars)
model_performance(cyl_)

cyl_hp <- lm(mpg ~ cyl + hp, data = mtcars)
model_performance(cyl_hp)


cyl_wt <- lm(mpg ~ cyl + wt, data = mtcars)
model_performance(cyl_wt)

wt_cyl <- lm(mpg ~ wt + cyl, data = mtcars)
model_performance(wt_cyl)

compare_performance(wt_cyl, cyl_wt, rank = T)

compare_performance(cyl_, wt_, cyl_wt, wt_cyl, rank = T)

str(diamonds)
unique(diamonds$cut)
unique(diamonds$clarity)


 
 
 
diamond_full_mod <- diamond %>%
  lm(price ~ ., data = .)
model_parameters(diamond_full_mod)
model_performance(diamond_full_mod)

par(mfrow = c(1,1))
corrplot::corrplot(cor(mtcars), F)
?corrplot::corrplot
GGally::ggpairs(mtcars)

str(mtcars)
model_performance(lm(mpg ~ cyl, data = mtcars))
model_performance(lm(mpg ~ wt + cyl, data = mtcars))
model_performance(lm(mpg ~ wt * cyl, data = mtcars))

summary(multi_mod)
check_collinearity(multi_mod)
plot(check_collinearity(multi_mod))

check_heteroscedasticity(multi_mod)
plot(check_heteroscedasticity(multi_mod))

check_normality(multi_mod)
plot(check_normality(multi_mod))

check_autocorrelation(multi_mod)
car::durbinWatsonTest(multi_mod)

check_distribution(multi_mod)
plot(check_distribution(multi_mod))


check_model(lm_0)
model_performance(lm_0)
?compare_performance
compare_performance(lm_0, lm_4, rank = T)
compare_performance(lm_0, lm_6, rank = T)
compare_performance(lm_0, lm_8, rank = T)
car::Anova(lm_0, lm_4)
anova(lm_0, lm_4, test = "Chi.sq")

# comparing multiple
m1 <- lm(y ~ x, data = )
m2 <- lm(y ~ x + z, data = )
#m3 <- lm(y ~ x + z, data = )

# Comparing Models
performance::compare_performance(m1, m2, ...)
plot(performance::compare_performance(m1, m2, ..., rank = TRUE))

```


# **Using geom_smooth() to Fit a Line Through Data**

## *Raw Data Plot*

Use the `diamonds` data set from `ggplot2` to plot a geom_point of `y = price`, `x = carat`.

```{r}
diamond <- ggplot2::diamonds

diamond %>%
  ggplot(data = ., aes(x = price, y = carat)) +
  geom_point()
```

## *`log`Transformed Data Plot*

Convert your variables to `log()` and plot the same point geom with the log data. Tip, if you mutate the change as a pipe, your variable names will look cleaner.

```{r}
diamond$carat
diamond %>%
  mutate(., 
         carat = log(carat),
         price = log(price)
         ) %>%
  ggplot(data = ., aes(x = carat, y = price)) +
  geom_point()
```

# **Using geom_smooth() to Fit a Line Through Data**

`ggplot2`'s `geom_smooth()` is a function used to help understand patterns in the data. Check the `method` parameter in the help section to see how you can pass a linear, `"lm"`, or a `"loess"` function.

Take either the untransformed or transformed data and plot a linear function through the points using `"lm"` by adding a `geom_smooth()` layer to the plot.

```{r}
diamond %>%
  mutate(., 
         carat = log(carat),
         price = log(price)
         ) %>%
  ggplot(data = ., aes(x = carat, y = price)) +
  geom_point(alpha = .15) +
  geom_smooth(method = "lm")
```

# **Facet the Plot**

Use `facet_wrap()` to pass separate plots by and ordered factor in the data frame.

```{r}
diamond %>%
  ggplot(data = ., aes(x = carat, y = price)) +
  geom_point(alpha = .2) +
  geom_smooth(method = "lm") +
  facet_wrap(~cut)
```



**Upload Submission**: Make sure you have named your file correctly, knit it to html (click icon in RStudio) and upload file [here](https://claremontmckenna.app.box.com/f/80863fd8e0f44ec7a95882d9dda8bb91).

```{r}
message(paste0("knit by ", usr_time))
```





`nchar()` is a function for counting the number of characters in a  

The R documentationa states that "`nchar` takes a character vector as an argument and returns a vector whose elements contain the sizes of the corresponding elements of `x`."

```{r}
nchar("Bill")

nchar("100000000")
## [1] 9
you get “9” back, no problem. But if you do:

nchar(100000000)
```

# **Data Frame Format**

When working with data, R is typically friendly with long-format data frames and less so with wide-format data frames. So what's the difference between wide and long format?

Let's create a data frame to work with. 

```{r}
mydata_long <- data.frame(
  Day   = c("Monday", "Monday", 
            "Friday", "Friday"),
  Spent = c(300, 365, 298, 310)
)  

mydata_wide1 <- data.frame(
  Monday_Spent = c(300, 365, NA, NA),
  Friday_Spent = c(NA, NA, 298, 310)
)

mydata_wide2 <- data.frame(
  Monday_Spent   = c(300, 365),
  Friday_Spent   = c(298, 310)
)
```

Looking at the long format, you see there are different days  appearing on different rows and also different amounts `Spent`  corresponding to them. As such, `Day` repeats in the data frame.

```{r}
mydata_long
```

Looking at the first wide format, you see that there are separate column variables for amounts `spent for given day`. Across the rows, the columns show amounts `Spent` on specific Days, otherwise `NA`s.  

```{r}
mydata_wide1
```

Looking at the second version of the wide format, but you can see there are two column variables representing the days and amounts `Spent`. Again, amount `Spent` is represented in multiple columns. 

```{r}
mydata_wide2
```

# **Advantages and Disadvantages**

Looking at the three data frames, you cannot tell whether the data represent *different* people going shopping on *different* days or whether there are the same people shopping on different days. An Id variable would be helpful here. 

Let's add them.

```{r}
mydata_long <- mydata_long %>% 
  mutate(., Id = c(1, 2, 1, 2)) %>% print()

mydata_wide1 <- mydata_wide1 %>% 
  mutate(., Id = c(1, 2, 1, 2)) %>% print()

mydata_wide2 <- mydata_wide2 %>% 
  mutate(., Id = c(1, 2)) %>% print()

```

You can see now that the data frames are more clear as to how data were collected. People shopped on both Monday and Friday, spending different amounts on each day for all the data frames. But how might you find out whether they are spending different amounnts on different days? In other words, what this the difference between the two days?

R loves long-format. Libraries like `dplyr` and `ggplot2` love long format. In fact, you can create `ggplot` plots using wide-format data but you'll want to pull out your hair. Similarly, `dplyr` has specific functions to deal with the long format, like `group_by()`. 

A disadvantage of a long format is that calculating differences between variables across rows can be both difficult and confusing. You can look into the `lag()` function if you want. Also, sometimes you cannot simply lag a trial back to perform a computation. Also, `lead()` is similar in the other direction.

To show you how `lag` works, we can create a new variable that assigns the `Spent` amount on a row by and index of one.

```{r}
mydata_long %>%
  mutate(., lag = lag(x = Spent, n = 2))
```


So the value on one row is pushed the next row. 


## *Using Long Format Version*

If you want to calculate the difference then, say Friday - Monday spending, we can take the difference between the current day and the spent about from the previous trial. For this example, Friday's `298` - Monday's `300` = `2`.

```{r}
mydata_long %>%
  mutate(., Diff = Spent - lag(x = Spent, n = 2))
```

But you also have the difference across `Id` which is super confusing. We would need a new computation for each `Id`.


## *Using Wide Format Version 1*

But notice that you cannot just `lag()` in the same column but rather lag different columns to compare the two days. Also and we would need to arrange the data by `Id` first because the data are alternating from row to row. 

```{r}
mydata_wide1 %>%
  arrange(., Id) %>%
  mutate(., Diff = Friday_Spent - lag(x = Monday_Spent, n = 1))
```

Sure, you could just filter rows for which `Diff` is not `NA` but then you've lost data too. Such computations and mental gymnastics may not be helpful, however, when dealing with long-format data. Wide-format data makes 


# **Pivoting Data Frames**

## *Long to Wide*

An alternative is to take a long-format data frame and pivot into a wide-format data frame in order to perform a simple computation. So, taking `mydata_long`, we can use `tidyr::pivot_wider()` and select how to move that rows into columns. Look a the data frame first.

```{r}
mydata_long
```

You want to moved the `Day` column that represents rows such that you have only two rows and now a column variable for each `Day`.  One way to do this is to pass an argument to the `names_from` parameter. `names_from` is used to take the names of the data frame from which you want to pivot the data. But you will also need to tell `pivot_wider()` where to get the values from. This is of course the `Spent` variable so you would pass that as an argument to the `values_from` parameter.

```{r}
mydata_long %>%
  tidyr::pivot_wider(., 
  names_from  = Day,
  values_from = Spent
) 
```

You now have a wider data frame. If you may forget the columns represent spent dollars, you can also add a prefix to the column names by passing a string prefix to `names_prefix` .


```{r}
mydata_long %>%
  tidyr::pivot_wider(., 
  names_from  = Day,
  values_from = Spent,
  names_prefix = "Spent_",
) 
```

And then just mutate a variable that takes the difference between the two days.

```{r}
mydata_long_new <- mydata_long %>%
  tidyr::pivot_wider(., 
  names_from  = Day,
  values_from = Spent,
#  names_prefix = "Spent_",
) %>% 
  mutate(., Diff = Friday - Monday) 
```


## *Wide to Long*

You will have to specify your `cols` argument for pivoting to long format. This might not work as you expect. If you want to pivot the data frame back to `Id`


```{r}
mydata_long_new
mydata_long_new %>%
  tidyr::pivot_longer(., 
                      cols = !Id,
                      names_to = "Day",
                      values_to = "Spent" 
  )
```

```{r}
mydata_long %>%
  tidyr::pivot_longer(., 
                      cols = Id,
                      names_to = "Day",
                      values_to = "Spent" 
  )
```




# may need if alternating NAs %>% group_by(., ) %>% dplyr::summarise_all(purrr::discard, is.na) %>% distinct()

You see that there are 3 column variables in the data frame. One variable is a factor and the other two are numeric values representing the medians of `wind` and `temp` for each `hour`. You can also see that `hour` has 24 unique values. `unique()` will provide the unique elements of a vector, whether character, factor, or numeric. 

```{r}
unique(mydata$hour)
```

Because `hour` is a factor vector, you can find this out with `levels()` also. However, `levels()` will return `NULL` if you aren't working with factors.

```{r}
levels(mydata$hour)
```

So is this data frame in long or wide format? Well, `wind` and `temp` have repeated measurements for each `hour`, which you see as rows. The same data could be represented as multiple columns.

Let's start with a simpler data frame by subsetting to only include `hour` and `temp`

```{r}
mydata %>%
  select(., hour, temp) %>%
  tidyr::pivot_wider(., 
    names_from  = hour,
    values_from = temp,
    names_prefix = "temp_hour_"
  ) %>% view(.) # may need if alternating NAs %>% group_by(., ) %>% dplyr::summarise_all(purrr::discard, is.na) %>% distinct()
```



```{r}
mydata %>%
  tidyr::pivot_wider(., 
    names_from  = hour,
    values_from = c(wind, temp)
  ) %>% view(.) # may need if alternating NAs %>% group_by(., ) %>% dplyr::summarise_all(purrr::discard, is.na) %>% distinct()
```



# **Difference Scores**

With some data, you may want to or be asked to (at your behest) to calculate a difference between two values. 



```{r}

```



```{r}
mydata <- data.frame( 
  hour     = factor(rep(1:24, each = 21)),
  price    = runif(504, min = -10, max = 125),
  wind     = runif(504, min = 0, max = 2500),
  temp     = runif(504, min = - 10, max = 25)
  )
mydata
bio_dat %>% 
  group_by()

# creating a data frame
data_frame <- data.frame(col1 = sample(6:9, 9 , replace = TRUE),
						col2 = letters[1:3],
						col3 = c(1,4,5,1,9,11,2,7,2))

print ("Original DataFrame")
print (data_frame)

# computing difference of each group
data_frame
data_frame %>%
  mutate(., diff = ave(col3, factor(col2),
					FUN = function(x) comean(x))
  ) %>% arrange(., col2)
						
print("Modified DataFrame")
print(data_frame)

```


# 




I held out as long as I could before posting this homework, hoping that more responses would be recorded so that the homework questions would make your your final project easier. Unfortunately, since the posting on 9/16 only about half of the class/teams yet to complete the general survey so these data are incomplete and another file will need to be downloaded and retrieved at some point the future. You will have to keep this in mind so that your final project uses the full data set. 

*What does the data limitation mean for this homework assignment?* You will see that the exact game categories, design, mechanics, etc. are not reflected by their variable names but only vaguely by their grouping as exported from the survey software. As part of data science is cleaning up data; even surveys need to be cleaned up. I planned to do this as a courtesy but the low response rate even from enrolled students suggests this isn't a priority. Unfortunately, answering this week's questions come with a fair amount of ambiguity but if you are curious, you can clean up your variables by reviewing pdf of the survey available in Slack #data named "GCDS22_Qualtrics_Survey.pdf". There is also a file in #data named "General_Survey_var_names.csv" that will help you understand the variables and questions.

**Questions:**

If you collaborated with any peers, add their names here: 


# **Load Libraries**

Load the necessary libraries to read in, manipulate, and plot data.

```{r}
library(readr)
library(dplyr)
library(ggplot2)
```


# **Read Data**

The current responses to the General Survey are in a data file named "General_Survey.csv" located in Slack #data. You should read in the file and save as a data frame object named `GENERAL`.

Completed: 1001 1002 1003 1004 1101 1102 1103 1104 1501 1502 1503 1504 1703 1705 1706 1901 1902 1903 1904 1905 2001 2002 2003

```{r}
dat_file_path <- paste(data_dir, "General_Survey.csv", sep = "/")

GENERAL <- readr::read_csv(dat_file_path)
```


# **Cleaning up your 99s**

Of course, you might be curious who did not answer certain questions, for which a `99` would be present in the data. However, for certain plots involving any sums, means, etc., the `99` will influence your plot to an extent it doesn't make sense. Keep in mind, you will not typically be tipped off to fix such problems as the data scientist needs to look at the data to understand it and ask questions if they are uncertain.

At some point you might need to make all `99`s into `NA`s before moving forward. 

```{r}
GENERAL <- GENERAL %>%
  mutate(., across(everything(), ~na_if(.x, 99)))
```

  
# **Histogram or Density Plot**

## *Histogram 1: One variable, One color*

Create either a `geom_histogram()` or `geom_density()` for a numeric variable of interest.

```{r}
GENERAL %>% ggplot(.) +
  geom_density(aes(Cat_43))
```


## *Histogram 2: One variable, n colors*

Create either a `geom_histogram()` or `geom_density()` for a numeric variable that visualizes a separate histogram for the `Gender` variable on the same plot.

```{r}
GENERAL %>% ggplot(.) +
  geom_density(mapping = aes(x = Cat_43, 
                             fill = as.factor(Gender)),
               alpha = .5)

```




# **Bar Plot**

## *Bar Plot 1: One variable*

Create a `geom_bar()` to visualize the variable in the data set that represented the most pleasurable reason to play board games. 

```{r}
GENERAL %>%
  ggplot() +
  geom_bar(data = GENERAL, 
           mapping = aes(x = Pleasuremost))

```

## *Bar Plot 2: One variable, One color*

Do the same for board game play `Frequency` but this time plot so that the counts are on the x axis. 

```{r include=FALSE}
#Coding and Response Options:

#1 = "Never"
#2 = "About once or twice a year"
#3 = "About once every two months"
#4 = "About once a month"
#5 = "About once every two weeks"
#6 = "About once a week"
#7 = "More than once a week"

GENERAL <- GENERAL %>%
  mutate(., Frequency = factor(
    case_when(
      Frequency == 1 ~ "Never",
      Frequency == 2 ~ "About once or twice a year",
      Frequency == 3 ~ "About once every two months",
      Frequency == 4 ~ "About once a month",
      Frequency == 5 ~ "About once every two weeks",
      Frequency == 6 ~ "About once a week",
      Frequency == 7 ~ "More than once a week"),
    ordered = T
    )) 
```

```{r}
ggplot() +
  geom_bar(data = GENERAL, 
           mapping = aes(x = Frequency)
           ) +
  coord_flip()

```

## *Bar Plot 3: Bars colors by gender*

Create the same bar plot but modify your aesthetics to fill the bar with colors for `Gender` groups.

1 = Male
2 = Female
3 = Non-binary
4 = Other
5 = Prefer not to say
NA = Didn't answer

Remember that data need to take a position in visual space and by default, the position of data was their 'identity'. Because all genders answered the same question about play frequency, by default the bars for both groups will take the same position on the plot for each frequency response option. As a result, you will end up with a *stacked bar plot*. Sometimes, you want stacked bars but for this instance, you want to quickly determine whether the counts vary as a function of gender. 

Pro tip, to make the bars not take the same position, add `position = "dodge"` to your function so that they bars will take a dodge position. 

```{r}
ggplot() +
  geom_bar(data = GENERAL, 
           mapping = aes(x = Frequency, 
                         fill = factor(Gender)), 
           position = "dodge") #+
#  coord_flip()
```



# **Column Plot**

## *Column Plot 1: Single Plot*

Chose one of your game categories (e.g,. CAT_) or mechanics (e.g., Mech_) to pair with Gender for a column plot. Determine whether you find differences in ratings. 

```{r}
GENERAL %>%
  ggplot(., 
         mapping = aes(x = Gender, 
                       y = Mech_1)) +
  geom_col()
```


## *Column Plot 2: Plot With Aesthetics*

Take the same variables but change the bar color to vary with Gender 

```{r}
GENERAL %>%
  ggplot(., 
         mapping = aes(x = Gender, 
                       y = Mech_1,
                       fill = as.factor(Gender))) +
  geom_col()
```


# **Point Plot**

Take two column variables that have the fewest `NA`s and create a point-plot geom. You can practice adding aesthetics if you want some practice for your projects.

```{r}
GENERAL %>%
  ggplot(.) +
  geom_point(aes(x = Breadth, 
                 y = Relationships))
```


# **Box Plot**

Create a box-plot geom  (e.g., `box_plot()`) to plot `Pleasure__1` on the x axis and `Competition` on the y axis.  

```{r}
GENERAL %>%
  ggplot(.) +
  geom_boxplot(aes(x = Pleasure__1, 
                   y = Competition))
```

# **BONUS CHALLENGE**

Recode all variable names in order to make sense of the data or complete the general survey and cognitive task. 


